{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone models.\n",
    "# !pip install git+https://github.com/rcmalli/keras-vggface.git\n",
    "# !pip install keras_vggface\n",
    "# !pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-15 16:33:51.405345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VGGFace compatible with TensorFlow2.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from utils.models import SiameseNetwork, BackboneNetwork\n",
    "from utils.funcs import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BASE_MODEL = 'vgg16'\n",
    "INPUT_SHAPE = (224, 224,)\n",
    "FINE_TUNE = True\n",
    "EPOCHS = 15\n",
    "DIST = 'default'\n",
    "LEARNING_RATE = 0.00003\n",
    "PATIENCE = 5\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Modify paths as per your method of saving them\n",
    "BASE_PATH = \"/root/KinshipRecognition\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_PATH}/data/aug_train_ds.csv\"\n",
    "TRAIN_FOLDERS_PATH = f\"{BASE_PATH}/data/train/train-faces/\"\n",
    "\n",
    "# Output file\n",
    "MODEL_NAME = f\"ensemble_{BASE_MODEL}_{DIST}_finetune{FINE_TUNE}\"\n",
    "\n",
    "# All images belonging to families F0X** will be used to create the validation set while training the model\n",
    "val_families_list = [\"F00\", \"F01\", \"F02\", \"F03\", \"F04\", \"F05\", \"F06\", \"F07\", \"F08\", \"F09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 0: Validation on F00\n",
      "##############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-15 16:34:15.669924: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-15 16:34:15.671328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-15 16:34:15.790286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-08-15 16:34:15.790319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-15 16:34:15.791615: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-15 16:34:15.791673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-08-15 16:34:15.792869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-15 16:34:15.793090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-15 16:34:15.794269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-15 16:34:15.794895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-15 16:34:15.798051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-15 16:34:15.800733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-15 16:34:15.801560: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-15 16:34:15.813886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-08-15 16:34:15.813953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-15 16:34:15.813999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-15 16:34:15.814028: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-08-15 16:34:15.814057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-15 16:34:15.814085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-15 16:34:15.814113: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-15 16:34:15.814141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-15 16:34:15.814169: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-15 16:34:15.822800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-15 16:34:15.822861: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-15 16:34:16.348814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-15 16:34:16.348845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-08-15 16:34:16.348850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-08-15 16:34:16.352764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30130 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)\n",
      "2021-08-15 16:34:16.353158: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 512)          0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 512)          0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 512)          0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 512)          0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 512)          0           layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "rbf (RBF)                       (None, 512)          512         layer_normalization[0][0]        \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 512)          0           subtract[0][0]                   \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 512)          0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 512)          0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           rbf[0][0]                        \n",
      "                                                                 multiply[0][0]                   \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           65568       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          4224        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-15 16:34:17.280778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-08-15 16:34:17.298976: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-15 16:34:18.303890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-15 16:34:18.509677: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 53s 165ms/step - loss: 0.7035 - acc: 0.6069 - val_loss: 0.6291 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67969, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_0.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.5463 - acc: 0.7393 - val_loss: 0.6084 - val_acc: 0.6908\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67969 to 0.69078, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_0.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.4874 - acc: 0.7804 - val_loss: 0.6555 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69078\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.4384 - acc: 0.8086 - val_loss: 0.6997 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69078\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.4246 - acc: 0.8152 - val_loss: 0.6437 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69078\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3894 - acc: 0.8400 - val_loss: 0.6474 - val_acc: 0.6859\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69078\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3589 - acc: 0.8515 - val_loss: 0.6994 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69078\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3380 - acc: 0.8693 - val_loss: 0.6814 - val_acc: 0.6908\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69078\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 46s 152ms/step - loss: 0.3181 - acc: 0.8728 - val_loss: 0.6501 - val_acc: 0.7036\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.69078 to 0.70359, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_0.h5\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3028 - acc: 0.8846 - val_loss: 0.6721 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70359\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.2999 - acc: 0.8836 - val_loss: 0.7471 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.70359\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.2893 - acc: 0.8887 - val_loss: 0.7830 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.70359\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.2944 - acc: 0.8913 - val_loss: 0.6876 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.70359\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.2681 - acc: 0.9023 - val_loss: 0.7344 - val_acc: 0.6822\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.70359\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.2871 - acc: 0.8942 - val_loss: 0.8189 - val_acc: 0.6580\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.70359\n",
      "##############################\n",
      "Iteration 1: Validation on F01\n",
      "##############################\n",
      "input_6 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 512)          0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 512)          0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 512)          0           layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 512)          0           layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 512)          0           layer_normalization_3[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "rbf_1 (RBF)                     (None, 512)          512         layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 512)          0           subtract_2[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 512)          0           multiply_5[0][0]                 \n",
      "                                                                 multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 512)          0           layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2048)         0           rbf_1[0][0]                      \n",
      "                                                                 multiply_4[0][0]                 \n",
      "                                                                 subtract_3[0][0]                 \n",
      "                                                                 multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           65568       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          4224        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           4128        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 52s 169ms/step - loss: 0.7232 - acc: 0.5640 - val_loss: 0.6777 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60906, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_1.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5450 - acc: 0.7416 - val_loss: 0.6125 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.60906 to 0.67125, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_1.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4792 - acc: 0.7822 - val_loss: 0.6188 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67125 to 0.67359, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_1.h5\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.4349 - acc: 0.8167 - val_loss: 0.6661 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.67359\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.4090 - acc: 0.8295 - val_loss: 0.6725 - val_acc: 0.6727\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.67359\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3826 - acc: 0.8470 - val_loss: 0.6815 - val_acc: 0.6614\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67359\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3343 - acc: 0.8612 - val_loss: 0.7133 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.67359\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3409 - acc: 0.8618 - val_loss: 0.6908 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.67359 to 0.68531, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_1.h5\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.2844 - acc: 0.8880 - val_loss: 0.6755 - val_acc: 0.6947\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.68531 to 0.69469, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_1.h5\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.2940 - acc: 0.8897 - val_loss: 0.7301 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69469\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.2712 - acc: 0.8968 - val_loss: 0.7601 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69469\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.2731 - acc: 0.8982 - val_loss: 0.8255 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69469\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.2637 - acc: 0.9018 - val_loss: 0.7586 - val_acc: 0.6723\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69469\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.2581 - acc: 0.9025 - val_loss: 0.8684 - val_acc: 0.6612\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69469\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.2435 - acc: 0.9125 - val_loss: 0.7903 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.69469\n",
      "##############################\n",
      "Iteration 2: Validation on F02\n",
      "##############################\n",
      "input_9 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 512)          0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 512)          0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 512)          0           layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 512)          0           layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 512)          0           layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "rbf_2 (RBF)                     (None, 512)          512         layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 512)          0           subtract_4[0][0]                 \n",
      "                                                                 subtract_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 512)          0           multiply_9[0][0]                 \n",
      "                                                                 multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 512)          0           layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2048)         0           rbf_2[0][0]                      \n",
      "                                                                 multiply_8[0][0]                 \n",
      "                                                                 subtract_5[0][0]                 \n",
      "                                                                 multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           65568       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          4224        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           4128        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            33          dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 51s 166ms/step - loss: 0.7520 - acc: 0.5601 - val_loss: 0.6364 - val_acc: 0.6572\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65719, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_2.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.5725 - acc: 0.7195 - val_loss: 0.5954 - val_acc: 0.6984\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65719 to 0.69844, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_2.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.5076 - acc: 0.7672 - val_loss: 0.5761 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.69844 to 0.70750, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_2.h5\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.4655 - acc: 0.7906 - val_loss: 0.5780 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.70750\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.4167 - acc: 0.8202 - val_loss: 0.5489 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70750 to 0.73359, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_2.h5\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3667 - acc: 0.8535 - val_loss: 0.5882 - val_acc: 0.7159\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.73359\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.3545 - acc: 0.8569 - val_loss: 0.6059 - val_acc: 0.6963\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.73359\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3215 - acc: 0.8747 - val_loss: 0.5732 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.73359 to 0.74063, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_2.h5\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.3456 - acc: 0.8611 - val_loss: 0.5714 - val_acc: 0.7383\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.74063\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3083 - acc: 0.8823 - val_loss: 0.6063 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.74063\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.2834 - acc: 0.8939 - val_loss: 0.6254 - val_acc: 0.7227\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.74063\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.2881 - acc: 0.8929 - val_loss: 0.5988 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.74063\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.2960 - acc: 0.8865 - val_loss: 0.6189 - val_acc: 0.7381\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.74063\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.2770 - acc: 0.8881 - val_loss: 0.6567 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.74063\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.2563 - acc: 0.9042 - val_loss: 0.6222 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.74063 to 0.75172, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_2.h5\n",
      "##############################\n",
      "Iteration 3: Validation on F03\n",
      "##############################\n",
      "input_12 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 512)          0           global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 512)          0           global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract_6 (Subtract)           (None, 512)          0           layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 512)          0           layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 512)          0           layer_normalization_7[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "rbf_3 (RBF)                     (None, 512)          512         layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 512)          0           subtract_6[0][0]                 \n",
      "                                                                 subtract_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 512)          0           multiply_13[0][0]                \n",
      "                                                                 multiply_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 512)          0           layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2048)         0           rbf_3[0][0]                      \n",
      "                                                                 multiply_12[0][0]                \n",
      "                                                                 subtract_7[0][0]                 \n",
      "                                                                 multiply_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 32)           65568       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32)           0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          4224        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           4128        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            33          dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 54s 176ms/step - loss: 0.7107 - acc: 0.5961 - val_loss: 0.6339 - val_acc: 0.6517\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65172, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_3.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 50s 168ms/step - loss: 0.5556 - acc: 0.7328 - val_loss: 0.5411 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65172 to 0.73750, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_3.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.4915 - acc: 0.7773 - val_loss: 0.5629 - val_acc: 0.7161\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.73750\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.4200 - acc: 0.8245 - val_loss: 0.5862 - val_acc: 0.7066\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.73750\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.3972 - acc: 0.8386 - val_loss: 0.5794 - val_acc: 0.7186\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.73750\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3682 - acc: 0.8477 - val_loss: 0.5652 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.73750\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.3537 - acc: 0.8562 - val_loss: 0.5786 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.73750\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3267 - acc: 0.8746 - val_loss: 0.5653 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.73750\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.2953 - acc: 0.8839 - val_loss: 0.6303 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.73750\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.3162 - acc: 0.8782 - val_loss: 0.6392 - val_acc: 0.6984\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.73750\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.2894 - acc: 0.8919 - val_loss: 0.6569 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.73750\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.2684 - acc: 0.8946 - val_loss: 0.7321 - val_acc: 0.6670\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.73750\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 47s 159ms/step - loss: 0.2508 - acc: 0.9055 - val_loss: 0.6712 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.73750\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.2529 - acc: 0.9056 - val_loss: 0.6576 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.73750\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.2735 - acc: 0.8977 - val_loss: 0.6562 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.73750\n",
      "##############################\n",
      "Iteration 4: Validation on F04\n",
      "##############################\n",
      "input_15 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_9 (Glo (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 512)          0           global_average_pooling2d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 512)          0           global_average_pooling2d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract_8 (Subtract)           (None, 512)          0           layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 512)          0           layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 512)          0           layer_normalization_9[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "rbf_4 (RBF)                     (None, 512)          512         layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 512)          0           subtract_8[0][0]                 \n",
      "                                                                 subtract_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_9 (Subtract)           (None, 512)          0           multiply_17[0][0]                \n",
      "                                                                 multiply_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 512)          0           layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 2048)         0           rbf_4[0][0]                      \n",
      "                                                                 multiply_16[0][0]                \n",
      "                                                                 subtract_9[0][0]                 \n",
      "                                                                 multiply_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 32)           65568       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          4224        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 32)           4128        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32)           0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            33          dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 49s 162ms/step - loss: 0.7114 - acc: 0.5969 - val_loss: 0.7106 - val_acc: 0.6089\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60891, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_4.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.5395 - acc: 0.7362 - val_loss: 0.8902 - val_acc: 0.4655\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.60891\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.4677 - acc: 0.7912 - val_loss: 0.9461 - val_acc: 0.5095\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.60891\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.4195 - acc: 0.8207 - val_loss: 0.9622 - val_acc: 0.5163\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.60891\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3980 - acc: 0.8362 - val_loss: 1.0152 - val_acc: 0.5108\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.60891\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3463 - acc: 0.8601 - val_loss: 0.9713 - val_acc: 0.5319\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.60891\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.3355 - acc: 0.8643 - val_loss: 1.1681 - val_acc: 0.4892\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.60891\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3115 - acc: 0.8773 - val_loss: 1.1533 - val_acc: 0.5020\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60891\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.2848 - acc: 0.8880 - val_loss: 1.1761 - val_acc: 0.5050\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60891\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.2895 - acc: 0.8852 - val_loss: 1.2032 - val_acc: 0.4883\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.60891\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.2943 - acc: 0.8859 - val_loss: 1.1767 - val_acc: 0.5086\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60891\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.2636 - acc: 0.8997 - val_loss: 1.1473 - val_acc: 0.5153\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.60891\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.2599 - acc: 0.9036 - val_loss: 1.1682 - val_acc: 0.5119\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.60891\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.2463 - acc: 0.9031 - val_loss: 1.2492 - val_acc: 0.4938\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.60891\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.2437 - acc: 0.9078 - val_loss: 1.3471 - val_acc: 0.4833\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.60891\n",
      "##############################\n",
      "Iteration 5: Validation on F05\n",
      "##############################\n",
      "input_18 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_10 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_11 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 512)          0           global_average_pooling2d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 512)          0           global_average_pooling2d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_10 (Subtract)          (None, 512)          0           layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 512)          0           layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, 512)          0           layer_normalization_11[0][0]     \n",
      "                                                                 layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_5 (RBF)                     (None, 512)          512         layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 512)          0           subtract_10[0][0]                \n",
      "                                                                 subtract_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_11 (Subtract)          (None, 512)          0           multiply_21[0][0]                \n",
      "                                                                 multiply_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, 512)          0           layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2048)         0           rbf_5[0][0]                      \n",
      "                                                                 multiply_20[0][0]                \n",
      "                                                                 subtract_11[0][0]                \n",
      "                                                                 multiply_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 32)           65568       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32)           0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          4224        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           4128        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32)           0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            33          dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 50s 163ms/step - loss: 0.7652 - acc: 0.5754 - val_loss: 0.6966 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61000, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_5.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5588 - acc: 0.7330 - val_loss: 0.6497 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61000 to 0.65625, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_5.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4834 - acc: 0.7815 - val_loss: 0.6630 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.65625\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4389 - acc: 0.8110 - val_loss: 0.6597 - val_acc: 0.6714\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.65625 to 0.67141, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_5.h5\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3849 - acc: 0.8383 - val_loss: 0.6888 - val_acc: 0.6589\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.67141\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3651 - acc: 0.8489 - val_loss: 0.6872 - val_acc: 0.6695\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67141\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3480 - acc: 0.8582 - val_loss: 0.7081 - val_acc: 0.6683\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.67141\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3336 - acc: 0.8694 - val_loss: 0.6875 - val_acc: 0.6747\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.67141 to 0.67469, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_5.h5\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3213 - acc: 0.8676 - val_loss: 0.7662 - val_acc: 0.6492\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.67469\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3039 - acc: 0.8814 - val_loss: 0.7532 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.67469 to 0.67797, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_5.h5\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.2795 - acc: 0.8941 - val_loss: 0.7127 - val_acc: 0.6798\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.67797 to 0.67984, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_5.h5\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.2725 - acc: 0.8958 - val_loss: 0.7694 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.67984 to 0.68141, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_5.h5\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.2797 - acc: 0.8961 - val_loss: 0.7485 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.68141\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.2620 - acc: 0.9011 - val_loss: 0.8190 - val_acc: 0.6695\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.68141\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.2478 - acc: 0.9118 - val_loss: 0.8144 - val_acc: 0.6602\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.68141\n",
      "##############################\n",
      "Iteration 6: Validation on F06\n",
      "##############################\n",
      "input_21 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_12 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_13 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 512)          0           global_average_pooling2d_12[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 512)          0           global_average_pooling2d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_12 (Subtract)          (None, 512)          0           layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_25 (Multiply)          (None, 512)          0           layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_26 (Multiply)          (None, 512)          0           layer_normalization_13[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_6 (RBF)                     (None, 512)          512         layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_24 (Multiply)          (None, 512)          0           subtract_12[0][0]                \n",
      "                                                                 subtract_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_13 (Subtract)          (None, 512)          0           multiply_25[0][0]                \n",
      "                                                                 multiply_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_27 (Multiply)          (None, 512)          0           layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 2048)         0           rbf_6[0][0]                      \n",
      "                                                                 multiply_24[0][0]                \n",
      "                                                                 subtract_13[0][0]                \n",
      "                                                                 multiply_27[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 32)           65568       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 32)           0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128)          4224        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 128)          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 32)           4128        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            33          dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 53s 175ms/step - loss: 0.7237 - acc: 0.5793 - val_loss: 0.6365 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.66031, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_6.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5379 - acc: 0.7525 - val_loss: 0.6015 - val_acc: 0.6817\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.66031 to 0.68172, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_6.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.4718 - acc: 0.7925 - val_loss: 0.6183 - val_acc: 0.6897\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.68172 to 0.68969, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_6.h5\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.4371 - acc: 0.8052 - val_loss: 0.6029 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.68969 to 0.70859, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_6.h5\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3878 - acc: 0.8437 - val_loss: 0.6494 - val_acc: 0.6956\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.70859\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3717 - acc: 0.8471 - val_loss: 0.6290 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.70859\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3327 - acc: 0.8694 - val_loss: 0.6677 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.70859\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3401 - acc: 0.8545 - val_loss: 0.6890 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.70859\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3310 - acc: 0.8663 - val_loss: 0.7589 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.70859\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.2843 - acc: 0.8952 - val_loss: 0.7143 - val_acc: 0.6837\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.70859\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.2797 - acc: 0.8921 - val_loss: 0.7334 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.70859\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.2825 - acc: 0.8859 - val_loss: 0.7281 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.70859\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.2610 - acc: 0.9036 - val_loss: 0.7182 - val_acc: 0.6880\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.70859\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.2410 - acc: 0.9120 - val_loss: 0.8010 - val_acc: 0.6680\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.70859\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.2355 - acc: 0.9136 - val_loss: 0.7466 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.70859\n",
      "##############################\n",
      "Iteration 7: Validation on F07\n",
      "##############################\n",
      "input_24 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_14 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_15 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 512)          0           global_average_pooling2d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 512)          0           global_average_pooling2d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_14 (Subtract)          (None, 512)          0           layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_29 (Multiply)          (None, 512)          0           layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_30 (Multiply)          (None, 512)          0           layer_normalization_15[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_7 (RBF)                     (None, 512)          512         layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_28 (Multiply)          (None, 512)          0           subtract_14[0][0]                \n",
      "                                                                 subtract_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_15 (Subtract)          (None, 512)          0           multiply_29[0][0]                \n",
      "                                                                 multiply_30[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_31 (Multiply)          (None, 512)          0           layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 2048)         0           rbf_7[0][0]                      \n",
      "                                                                 multiply_28[0][0]                \n",
      "                                                                 subtract_15[0][0]                \n",
      "                                                                 multiply_31[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 32)           65568       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          4224        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 128)          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 32)           4128        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32)           0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1)            33          dropout_23[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 49s 161ms/step - loss: 0.7052 - acc: 0.5725 - val_loss: 0.6980 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60984, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_7.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 111s 372ms/step - loss: 0.5508 - acc: 0.7400 - val_loss: 0.6154 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.60984 to 0.67969, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_7.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 251s 839ms/step - loss: 0.4830 - acc: 0.7842 - val_loss: 0.6194 - val_acc: 0.6784\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.67969\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 54s 179ms/step - loss: 0.4377 - acc: 0.8052 - val_loss: 0.7362 - val_acc: 0.6145\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.67969\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 104s 347ms/step - loss: 0.3950 - acc: 0.8344 - val_loss: 0.7700 - val_acc: 0.6206\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.67969\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.3911 - acc: 0.8307 - val_loss: 0.7021 - val_acc: 0.6748\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67969\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 101s 338ms/step - loss: 0.3518 - acc: 0.8531 - val_loss: 0.6993 - val_acc: 0.6920\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.67969 to 0.69203, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_7.h5\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.3193 - acc: 0.8789 - val_loss: 0.6937 - val_acc: 0.6964\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.69203 to 0.69641, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_7.h5\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 53s 178ms/step - loss: 0.3126 - acc: 0.8816 - val_loss: 0.7765 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69641\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 53s 178ms/step - loss: 0.3038 - acc: 0.8790 - val_loss: 0.8404 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69641\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.2812 - acc: 0.8926 - val_loss: 1.0333 - val_acc: 0.6084\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69641\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 53s 177ms/step - loss: 0.2859 - acc: 0.8859 - val_loss: 0.8989 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69641\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 52s 173ms/step - loss: 0.2562 - acc: 0.9079 - val_loss: 0.8613 - val_acc: 0.6648\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69641\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 54s 180ms/step - loss: 0.2497 - acc: 0.9062 - val_loss: 1.0069 - val_acc: 0.6178\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69641\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 52s 175ms/step - loss: 0.2279 - acc: 0.9184 - val_loss: 1.0176 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.69641\n",
      "##############################\n",
      "Iteration 8: Validation on F08\n",
      "##############################\n",
      "input_27 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_25[0][0]                   \n",
      "                                                                 input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_16 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_17 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 512)          0           global_average_pooling2d_16[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 512)          0           global_average_pooling2d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_16 (Subtract)          (None, 512)          0           layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_33 (Multiply)          (None, 512)          0           layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_34 (Multiply)          (None, 512)          0           layer_normalization_17[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_8 (RBF)                     (None, 512)          512         layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_32 (Multiply)          (None, 512)          0           subtract_16[0][0]                \n",
      "                                                                 subtract_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_17 (Subtract)          (None, 512)          0           multiply_33[0][0]                \n",
      "                                                                 multiply_34[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_35 (Multiply)          (None, 512)          0           layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 2048)         0           rbf_8[0][0]                      \n",
      "                                                                 multiply_32[0][0]                \n",
      "                                                                 subtract_17[0][0]                \n",
      "                                                                 multiply_35[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 32)           65568       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          4224        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 128)          0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 32)           4128        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 32)           0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            33          dropout_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 56s 182ms/step - loss: 0.7191 - acc: 0.5788 - val_loss: 0.5952 - val_acc: 0.7047\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70469, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_8.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 54s 180ms/step - loss: 0.5653 - acc: 0.7232 - val_loss: 0.5440 - val_acc: 0.7359\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.70469 to 0.73594, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_8.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 54s 181ms/step - loss: 0.5008 - acc: 0.7717 - val_loss: 0.5480 - val_acc: 0.7247\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.73594\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 54s 182ms/step - loss: 0.4388 - acc: 0.8118 - val_loss: 0.5503 - val_acc: 0.7353\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.73594\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.4143 - acc: 0.8281 - val_loss: 0.5024 - val_acc: 0.7691\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.73594 to 0.76906, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_8.h5\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.3665 - acc: 0.8491 - val_loss: 0.4998 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76906 to 0.77125, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_8.h5\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 56s 187ms/step - loss: 0.3604 - acc: 0.8550 - val_loss: 0.5608 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77125\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 56s 186ms/step - loss: 0.3277 - acc: 0.8703 - val_loss: 0.5424 - val_acc: 0.7552\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77125\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 55s 185ms/step - loss: 0.3196 - acc: 0.8773 - val_loss: 0.5435 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77125\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 56s 188ms/step - loss: 0.3055 - acc: 0.8800 - val_loss: 0.5371 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77125\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 52s 174ms/step - loss: 0.2760 - acc: 0.8970 - val_loss: 0.5834 - val_acc: 0.7409\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77125\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 56s 189ms/step - loss: 0.2981 - acc: 0.8836 - val_loss: 0.5951 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.77125\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 54s 181ms/step - loss: 0.2653 - acc: 0.8955 - val_loss: 0.5809 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.77125\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 55s 184ms/step - loss: 0.2497 - acc: 0.9106 - val_loss: 0.5874 - val_acc: 0.7414\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.77125\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 195s 653ms/step - loss: 0.2433 - acc: 0.9152 - val_loss: 0.6382 - val_acc: 0.7300\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.77125\n",
      "##############################\n",
      "Iteration 9: Validation on F09\n",
      "##############################\n",
      "input_30 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_28 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_19 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 512)          0           global_average_pooling2d_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 512)          0           global_average_pooling2d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_18 (Subtract)          (None, 512)          0           layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_37 (Multiply)          (None, 512)          0           layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_38 (Multiply)          (None, 512)          0           layer_normalization_19[0][0]     \n",
      "                                                                 layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_9 (RBF)                     (None, 512)          512         layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_36 (Multiply)          (None, 512)          0           subtract_18[0][0]                \n",
      "                                                                 subtract_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_19 (Subtract)          (None, 512)          0           multiply_37[0][0]                \n",
      "                                                                 multiply_38[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_39 (Multiply)          (None, 512)          0           layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 2048)         0           rbf_9[0][0]                      \n",
      "                                                                 multiply_36[0][0]                \n",
      "                                                                 subtract_19[0][0]                \n",
      "                                                                 multiply_39[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 32)           65568       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 32)           0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 128)          4224        dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 128)          0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 32)           4128        dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 32)           0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1)            33          dropout_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "300/300 [==============================] - 162s 515ms/step - loss: 0.7236 - acc: 0.5562 - val_loss: 0.6370 - val_acc: 0.6517\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65172, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_9.h5\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 57s 190ms/step - loss: 0.5595 - acc: 0.7283 - val_loss: 0.6296 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65172 to 0.67547, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_9.h5\n",
      "Epoch 3/15\n",
      "300/300 [==============================] - 57s 190ms/step - loss: 0.4792 - acc: 0.7863 - val_loss: 0.6441 - val_acc: 0.6866\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67547 to 0.68656, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_9.h5\n",
      "Epoch 4/15\n",
      "300/300 [==============================] - 56s 187ms/step - loss: 0.4560 - acc: 0.7928 - val_loss: 0.6191 - val_acc: 0.7047\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.68656 to 0.70469, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_9.h5\n",
      "Epoch 5/15\n",
      "300/300 [==============================] - 57s 190ms/step - loss: 0.3892 - acc: 0.8373 - val_loss: 0.6165 - val_acc: 0.7139\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70469 to 0.71391, saving model to /root/KinshipRecognition/log/model/ensemble_vgg16_default_finetuneTrue_9.h5\n",
      "Epoch 6/15\n",
      "300/300 [==============================] - 57s 190ms/step - loss: 0.3706 - acc: 0.8590 - val_loss: 0.7147 - val_acc: 0.6642\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.71391\n",
      "Epoch 7/15\n",
      "300/300 [==============================] - 57s 189ms/step - loss: 0.3652 - acc: 0.8548 - val_loss: 0.7117 - val_acc: 0.6772\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.71391\n",
      "Epoch 8/15\n",
      "300/300 [==============================] - 57s 189ms/step - loss: 0.3333 - acc: 0.8663 - val_loss: 0.7395 - val_acc: 0.6691\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.71391\n",
      "Epoch 9/15\n",
      "300/300 [==============================] - 57s 190ms/step - loss: 0.3127 - acc: 0.8735 - val_loss: 0.7309 - val_acc: 0.6525\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.71391\n",
      "Epoch 10/15\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.2988 - acc: 0.8845 - val_loss: 0.7458 - val_acc: 0.6831\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.71391\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2e-05.\n",
      "Epoch 11/15\n",
      "300/300 [==============================] - 55s 183ms/step - loss: 0.2827 - acc: 0.8919 - val_loss: 0.7142 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.71391\n",
      "Epoch 12/15\n",
      "300/300 [==============================] - 105s 352ms/step - loss: 0.2651 - acc: 0.8976 - val_loss: 0.8921 - val_acc: 0.6284\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.71391\n",
      "Epoch 13/15\n",
      "300/300 [==============================] - 195s 652ms/step - loss: 0.2776 - acc: 0.8939 - val_loss: 0.8527 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.71391\n",
      "Epoch 14/15\n",
      "300/300 [==============================] - 51s 169ms/step - loss: 0.2549 - acc: 0.9049 - val_loss: 0.7655 - val_acc: 0.6809\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.71391\n",
      "Epoch 15/15\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.2264 - acc: 0.9199 - val_loss: 0.8435 - val_acc: 0.6741\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.71391\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "\n",
    "    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_families_list[i], \n",
    "                                                                                    TRAIN_FOLDERS_PATH,\n",
    "                                                                                    TRAIN_FILE_PATH)\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.33, patience=PATIENCE, verbose=1,\n",
    "                                         min_lr=0.00002)\n",
    "    callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "    \n",
    "    model = SiameseNetwork(BASE_MODEL, fine_tune=FINE_TUNE, dist=DIST,optimizer=Adam(LEARNING_RATE)).build()\n",
    "    \n",
    "    history = model.fit(gen(train, train_person_to_images_map, INPUT_SHAPE, TRAIN_FOLDERS_PATH, batch_size=16), \n",
    "                        validation_data=gen(val, val_person_to_images_map, INPUT_SHAPE, TRAIN_FOLDERS_PATH, batch_size=16), \n",
    "                        epochs=EPOCHS, steps_per_epoch=300, validation_steps=200,\n",
    "                        verbose=1, callbacks=callbacks_list, \n",
    "                        use_multiprocessing=False, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 0: Validation on F00\n",
      "##############################\n",
      "input_33 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_20 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_21 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 512)          0           global_average_pooling2d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 512)          0           global_average_pooling2d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_20 (Subtract)          (None, 512)          0           layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_41 (Multiply)          (None, 512)          0           layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_42 (Multiply)          (None, 512)          0           layer_normalization_21[0][0]     \n",
      "                                                                 layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_10 (RBF)                    (None, 512)          512         layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_40 (Multiply)          (None, 512)          0           subtract_20[0][0]                \n",
      "                                                                 subtract_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_21 (Subtract)          (None, 512)          0           multiply_41[0][0]                \n",
      "                                                                 multiply_42[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_43 (Multiply)          (None, 512)          0           layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2048)         0           rbf_10[0][0]                     \n",
      "                                                                 multiply_40[0][0]                \n",
      "                                                                 subtract_21[0][0]                \n",
      "                                                                 multiply_43[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 32)           65568       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 32)           0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 128)          4224        dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 128)          0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 32)           4128        dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 32)           0           dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 1)            33          dropout_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n",
      "##############################\n",
      "Iteration 1: Validation on F01\n",
      "##############################\n",
      "input_36 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_34 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_35 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_34[0][0]                   \n",
      "                                                                 input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_22 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_23 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 512)          0           global_average_pooling2d_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 512)          0           global_average_pooling2d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_22 (Subtract)          (None, 512)          0           layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_45 (Multiply)          (None, 512)          0           layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_46 (Multiply)          (None, 512)          0           layer_normalization_23[0][0]     \n",
      "                                                                 layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_11 (RBF)                    (None, 512)          512         layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_44 (Multiply)          (None, 512)          0           subtract_22[0][0]                \n",
      "                                                                 subtract_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_23 (Subtract)          (None, 512)          0           multiply_45[0][0]                \n",
      "                                                                 multiply_46[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_47 (Multiply)          (None, 512)          0           layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 2048)         0           rbf_11[0][0]                     \n",
      "                                                                 multiply_44[0][0]                \n",
      "                                                                 subtract_23[0][0]                \n",
      "                                                                 multiply_47[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 32)           65568       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 32)           0           dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 128)          4224        dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 128)          0           dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 32)           4128        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 32)           0           dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 1)            33          dropout_35[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 2: Validation on F02\n",
      "##############################\n",
      "input_39 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_24 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_25 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 512)          0           global_average_pooling2d_24[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 512)          0           global_average_pooling2d_25[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_24 (Subtract)          (None, 512)          0           layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_49 (Multiply)          (None, 512)          0           layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_50 (Multiply)          (None, 512)          0           layer_normalization_25[0][0]     \n",
      "                                                                 layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_12 (RBF)                    (None, 512)          512         layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_48 (Multiply)          (None, 512)          0           subtract_24[0][0]                \n",
      "                                                                 subtract_24[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_25 (Subtract)          (None, 512)          0           multiply_49[0][0]                \n",
      "                                                                 multiply_50[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_51 (Multiply)          (None, 512)          0           layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 2048)         0           rbf_12[0][0]                     \n",
      "                                                                 multiply_48[0][0]                \n",
      "                                                                 subtract_25[0][0]                \n",
      "                                                                 multiply_51[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 32)           65568       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 32)           0           dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 128)          4224        dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 128)          0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 32)           4128        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 32)           0           dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 1)            33          dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n",
      "##############################\n",
      "Iteration 3: Validation on F03\n",
      "##############################\n",
      "input_42 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_40 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_41 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_40[0][0]                   \n",
      "                                                                 input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_26 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_27 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 512)          0           global_average_pooling2d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 512)          0           global_average_pooling2d_27[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_26 (Subtract)          (None, 512)          0           layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_53 (Multiply)          (None, 512)          0           layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_54 (Multiply)          (None, 512)          0           layer_normalization_27[0][0]     \n",
      "                                                                 layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_13 (RBF)                    (None, 512)          512         layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_52 (Multiply)          (None, 512)          0           subtract_26[0][0]                \n",
      "                                                                 subtract_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_27 (Subtract)          (None, 512)          0           multiply_53[0][0]                \n",
      "                                                                 multiply_54[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_55 (Multiply)          (None, 512)          0           layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 2048)         0           rbf_13[0][0]                     \n",
      "                                                                 multiply_52[0][0]                \n",
      "                                                                 subtract_27[0][0]                \n",
      "                                                                 multiply_55[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 32)           65568       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 32)           0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 128)          4224        dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 128)          0           dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 32)           4128        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 32)           0           dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 1)            33          dropout_41[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 4: Validation on F04\n",
      "##############################\n",
      "input_45 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_43 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_43[0][0]                   \n",
      "                                                                 input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_28 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_29 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 512)          0           global_average_pooling2d_28[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 512)          0           global_average_pooling2d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_28 (Subtract)          (None, 512)          0           layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_57 (Multiply)          (None, 512)          0           layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_58 (Multiply)          (None, 512)          0           layer_normalization_29[0][0]     \n",
      "                                                                 layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_14 (RBF)                    (None, 512)          512         layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_56 (Multiply)          (None, 512)          0           subtract_28[0][0]                \n",
      "                                                                 subtract_28[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_29 (Subtract)          (None, 512)          0           multiply_57[0][0]                \n",
      "                                                                 multiply_58[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_59 (Multiply)          (None, 512)          0           layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 2048)         0           rbf_14[0][0]                     \n",
      "                                                                 multiply_56[0][0]                \n",
      "                                                                 subtract_29[0][0]                \n",
      "                                                                 multiply_59[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 32)           65568       concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 32)           0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 128)          4224        dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 128)          0           dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 32)           4128        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 32)           0           dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 1)            33          dropout_44[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n",
      "##############################\n",
      "Iteration 5: Validation on F05\n",
      "##############################\n",
      "input_48 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_46 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_46[0][0]                   \n",
      "                                                                 input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_30 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_31 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 512)          0           global_average_pooling2d_30[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 512)          0           global_average_pooling2d_31[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_30 (Subtract)          (None, 512)          0           layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_61 (Multiply)          (None, 512)          0           layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_62 (Multiply)          (None, 512)          0           layer_normalization_31[0][0]     \n",
      "                                                                 layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_15 (RBF)                    (None, 512)          512         layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_60 (Multiply)          (None, 512)          0           subtract_30[0][0]                \n",
      "                                                                 subtract_30[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_31 (Subtract)          (None, 512)          0           multiply_61[0][0]                \n",
      "                                                                 multiply_62[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_63 (Multiply)          (None, 512)          0           layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 2048)         0           rbf_15[0][0]                     \n",
      "                                                                 multiply_60[0][0]                \n",
      "                                                                 subtract_31[0][0]                \n",
      "                                                                 multiply_63[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 32)           65568       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 32)           0           dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 128)          4224        dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 128)          0           dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 32)           4128        dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 32)           0           dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 1)            33          dropout_47[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 6: Validation on F06\n",
      "##############################\n",
      "input_51 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_49 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_49[0][0]                   \n",
      "                                                                 input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_32 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_33 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 512)          0           global_average_pooling2d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 512)          0           global_average_pooling2d_33[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_32 (Subtract)          (None, 512)          0           layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_65 (Multiply)          (None, 512)          0           layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_66 (Multiply)          (None, 512)          0           layer_normalization_33[0][0]     \n",
      "                                                                 layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_16 (RBF)                    (None, 512)          512         layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_64 (Multiply)          (None, 512)          0           subtract_32[0][0]                \n",
      "                                                                 subtract_32[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_33 (Subtract)          (None, 512)          0           multiply_65[0][0]                \n",
      "                                                                 multiply_66[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_67 (Multiply)          (None, 512)          0           layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 2048)         0           rbf_16[0][0]                     \n",
      "                                                                 multiply_64[0][0]                \n",
      "                                                                 subtract_33[0][0]                \n",
      "                                                                 multiply_67[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 32)           65568       concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 32)           0           dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 128)          4224        dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 128)          0           dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 32)           4128        dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 32)           0           dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 1)            33          dropout_50[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n",
      "##############################\n",
      "Iteration 7: Validation on F07\n",
      "##############################\n",
      "input_54 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_52 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_53 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_52[0][0]                   \n",
      "                                                                 input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_34 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_35 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 512)          0           global_average_pooling2d_34[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, 512)          0           global_average_pooling2d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_34 (Subtract)          (None, 512)          0           layer_normalization_34[0][0]     \n",
      "                                                                 layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_69 (Multiply)          (None, 512)          0           layer_normalization_34[0][0]     \n",
      "                                                                 layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_70 (Multiply)          (None, 512)          0           layer_normalization_35[0][0]     \n",
      "                                                                 layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_17 (RBF)                    (None, 512)          512         layer_normalization_34[0][0]     \n",
      "                                                                 layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_68 (Multiply)          (None, 512)          0           subtract_34[0][0]                \n",
      "                                                                 subtract_34[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_35 (Subtract)          (None, 512)          0           multiply_69[0][0]                \n",
      "                                                                 multiply_70[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_71 (Multiply)          (None, 512)          0           layer_normalization_34[0][0]     \n",
      "                                                                 layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 2048)         0           rbf_17[0][0]                     \n",
      "                                                                 multiply_68[0][0]                \n",
      "                                                                 subtract_35[0][0]                \n",
      "                                                                 multiply_71[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 32)           65568       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 32)           0           dense_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 128)          4224        dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 128)          0           dense_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 32)           4128        dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 32)           0           dense_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 1)            33          dropout_53[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 8: Validation on F08\n",
      "##############################\n",
      "input_57 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_55 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_56 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_55[0][0]                   \n",
      "                                                                 input_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_36 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_37 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 512)          0           global_average_pooling2d_36[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 512)          0           global_average_pooling2d_37[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_36 (Subtract)          (None, 512)          0           layer_normalization_36[0][0]     \n",
      "                                                                 layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_73 (Multiply)          (None, 512)          0           layer_normalization_36[0][0]     \n",
      "                                                                 layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_74 (Multiply)          (None, 512)          0           layer_normalization_37[0][0]     \n",
      "                                                                 layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_18 (RBF)                    (None, 512)          512         layer_normalization_36[0][0]     \n",
      "                                                                 layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_72 (Multiply)          (None, 512)          0           subtract_36[0][0]                \n",
      "                                                                 subtract_36[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_37 (Subtract)          (None, 512)          0           multiply_73[0][0]                \n",
      "                                                                 multiply_74[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_75 (Multiply)          (None, 512)          0           layer_normalization_36[0][0]     \n",
      "                                                                 layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 2048)         0           rbf_18[0][0]                     \n",
      "                                                                 multiply_72[0][0]                \n",
      "                                                                 subtract_37[0][0]                \n",
      "                                                                 multiply_75[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 32)           65568       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 32)           0           dense_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 128)          4224        dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 128)          0           dense_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_74 (Dense)                (None, 32)           4128        dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 32)           0           dense_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 1)            33          dropout_56[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n",
      "##############################\n",
      "Iteration 9: Validation on F09\n",
      "##############################\n",
      "input_60 False\n",
      "conv1_1 False\n",
      "conv1_2 False\n",
      "pool1 False\n",
      "conv2_1 False\n",
      "conv2_2 False\n",
      "pool2 False\n",
      "conv3_1 False\n",
      "conv3_2 False\n",
      "conv3_3 False\n",
      "pool3 False\n",
      "conv4_1 False\n",
      "conv4_2 False\n",
      "conv4_3 False\n",
      "pool4 False\n",
      "conv5_1 False\n",
      "conv5_2 False\n",
      "conv5_3 True\n",
      "pool5 True\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_58 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_59 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_vgg16 (Functional)      (None, None, None, 5 14714688    input_58[0][0]                   \n",
      "                                                                 input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_38 (Gl (None, 512)          0           vggface_vgg16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_39 (Gl (None, 512)          0           vggface_vgg16[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 512)          0           global_average_pooling2d_38[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, 512)          0           global_average_pooling2d_39[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_38 (Subtract)          (None, 512)          0           layer_normalization_38[0][0]     \n",
      "                                                                 layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_77 (Multiply)          (None, 512)          0           layer_normalization_38[0][0]     \n",
      "                                                                 layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_78 (Multiply)          (None, 512)          0           layer_normalization_39[0][0]     \n",
      "                                                                 layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "rbf_19 (RBF)                    (None, 512)          512         layer_normalization_38[0][0]     \n",
      "                                                                 layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_76 (Multiply)          (None, 512)          0           subtract_38[0][0]                \n",
      "                                                                 subtract_38[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_39 (Subtract)          (None, 512)          0           multiply_77[0][0]                \n",
      "                                                                 multiply_78[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_79 (Multiply)          (None, 512)          0           layer_normalization_38[0][0]     \n",
      "                                                                 layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 2048)         0           rbf_19[0][0]                     \n",
      "                                                                 multiply_76[0][0]                \n",
      "                                                                 subtract_39[0][0]                \n",
      "                                                                 multiply_79[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 32)           65568       concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 32)           0           dense_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 128)          4224        dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 128)          0           dense_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_78 (Dense)                (None, 32)           4128        dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 32)           0           dense_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 1)            33          dropout_59[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 14,789,153\n",
      "Trainable params: 2,434,273\n",
      "Non-trainable params: 12,354,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_path = f\"{BASE_PATH}/data/test/\"\n",
    "submission = pd.read_csv(f'{BASE_PATH}/data/test_ds.csv')\n",
    "preds_for_sub = np.zeros(submission.shape[0])\n",
    "all_preds = list()\n",
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "    \n",
    "    model = SiameseNetwork(BASE_MODEL, fine_tune=FINE_TUNE, dist=DIST,optimizer=Adam(LEARNING_RATE)).build()\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    # Predictions\n",
    "    step = BATCH_SIZE * 2\n",
    "    predictions = []\n",
    "    for j in range(0, len(submission.p1.values), step):\n",
    "        X1 = submission.p1.values[j:j+step]\n",
    "        X1 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X1])\n",
    "        X2 = submission.p2.values[j:j+step]\n",
    "        X2 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X2])\n",
    "        if len(X1) < step:\n",
    "            diff = step - len(X1)\n",
    "            aux_X = submission.p1.values[:diff]\n",
    "            aux_X = np.array([read_img(test_path + x, INPUT_SHAPE) for x in aux_X])\n",
    "            aux_X1 = np.r_[X1, aux_X]\n",
    "            aux_X2 = np.r_[X2, aux_X]\n",
    "            pred = model.predict([aux_X1, aux_X2]).ravel().tolist()\n",
    "            pred = pred[:len(X1)]\n",
    "        else:\n",
    "            pred = model.predict([X1, X2]).ravel().tolist()\n",
    "        predictions += pred        \n",
    "\n",
    "    all_preds.append(np.array(predictions))\n",
    "    preds_for_sub += np.array(predictions) / len(val_families_list)\n",
    "\n",
    "    \n",
    "all_preds = np.asarray(all_preds).T\n",
    "submission['score'] = preds_for_sub\n",
    "pd.DataFrame(all_preds).to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}_allpreds.csv\", index=False)\n",
    "submission.to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1798\n",
      "3000 \n",
      "\n",
      "0.2441397536545992\n",
      "0.056035458017140626\n",
      "0.6418508321046829\n",
      "0.8277451455593109\n",
      "0.3316828101873398\n",
      "0.187518893647939\n",
      "0.4885197073221207\n",
      "0.051225732732564214\n",
      "0.8306846559047699\n",
      "0.21835185950621966\n",
      "0.09915623171254992\n",
      "0.07654145467095076\n",
      "0.7290391743183138\n",
      "0.6528122067451477\n",
      "0.0479622072307393\n",
      "0.7597625464200972\n",
      "0.23195278644561765\n",
      "0.3822233133018017\n",
      "0.1327974282205105\n",
      "0.16195716969668866\n",
      "0.6589959889650344\n",
      "0.21032285541296006\n",
      "0.3245303988456726\n",
      "0.11561125516891477\n",
      "0.6461426764726639\n",
      "0.17463964652270078\n",
      "0.10817232290282845\n",
      "0.31765098385512824\n",
      "0.28670673542656\n",
      "0.4687825370579957\n",
      "0.7079221695661545\n",
      "0.3499152407050133\n",
      "0.41689118370413786\n",
      "0.2039002156816423\n",
      "0.21337272226810455\n",
      "0.10151751330122351\n",
      "0.49420951791107653\n",
      "0.5998030148446559\n",
      "0.7320288002490998\n",
      "0.06408761695493013\n",
      "0.220848099142313\n",
      "0.07428454058244824\n",
      "0.5238889738917352\n",
      "0.6701879173517227\n",
      "0.5900618851184846\n",
      "0.28428158089518546\n",
      "0.7478068739175796\n",
      "0.11228138450533151\n",
      "0.8670121371746065\n",
      "0.1911881674081087\n",
      "0.24483842253684998\n",
      "0.5825713247060775\n",
      "0.38567879647016523\n",
      "0.8736724257469176\n",
      "0.4378242582082748\n",
      "0.20412831511348487\n",
      "0.26607525702565915\n",
      "0.3867853477597236\n",
      "0.3562217891216278\n",
      "0.10584115595556795\n",
      "0.1728415582329035\n",
      "0.08284822367131711\n",
      "0.7639666795730591\n",
      "0.8695747971534729\n",
      "0.05748563106171786\n",
      "0.4492916867136955\n",
      "0.3605432525277137\n",
      "0.33127826303243635\n",
      "0.38721931278705596\n",
      "0.8240284144878387\n",
      "0.32006237404420973\n",
      "0.3352731753140688\n",
      "0.3818638637661934\n",
      "0.3957641981542111\n",
      "0.16624428685754536\n",
      "0.5866644665598869\n",
      "0.4680488139390946\n",
      "0.03843176830559968\n",
      "0.3171493286266923\n",
      "0.34817898720502855\n",
      "0.8486817657947541\n",
      "0.2957331776618958\n",
      "0.64932122528553\n",
      "0.12388893831521272\n",
      "0.3447461545467376\n",
      "0.8312902003526688\n",
      "0.32179736737161874\n",
      "0.09739342350512743\n",
      "0.42556862682104113\n",
      "0.37149509713053697\n",
      "0.2645248305052519\n",
      "0.6390232652425766\n",
      "0.8354371875524522\n",
      "0.729192903637886\n",
      "0.8658917486667633\n",
      "0.6725563153624534\n",
      "0.9001108586788178\n",
      "0.10015310761518777\n",
      "0.7392162472009659\n",
      "0.039972461247816685\n",
      "0.23305622488260272\n",
      "0.14734843634068967\n",
      "0.3605062380433082\n",
      "0.5491682946681975\n",
      "0.36987214758992193\n",
      "0.3099180147051811\n",
      "0.40650372430682175\n",
      "0.7148449152708054\n",
      "0.13131155073642728\n",
      "0.7291046887636186\n",
      "0.39919445067644116\n",
      "0.718375849723816\n",
      "0.9368096768856049\n",
      "0.06958328355103731\n",
      "0.09525767168961466\n",
      "0.11075026439502836\n",
      "0.13570953477174044\n",
      "0.7342947572469711\n",
      "0.5459478750824929\n",
      "0.4133824095129967\n",
      "0.05337976710870863\n",
      "0.533078320324421\n",
      "0.41929024923592806\n",
      "0.8284092485904694\n",
      "0.274704734236002\n",
      "0.5854443848133087\n",
      "0.4665023431181907\n",
      "0.22543648132123048\n",
      "0.9002854943275451\n",
      "0.11547881700098517\n",
      "0.15929146129637958\n",
      "0.5146034888923169\n",
      "0.364970151335001\n",
      "0.15103066861629486\n",
      "0.6003448955714702\n",
      "0.08469839924946428\n",
      "0.43740629851818086\n",
      "0.2477414071559906\n",
      "0.5044977053999901\n",
      "0.7760739654302597\n",
      "0.3300131097435951\n",
      "0.26329614687711\n",
      "0.7245312094688415\n",
      "0.8268534272909163\n",
      "0.31003448143601414\n",
      "0.05505369096063077\n",
      "0.44998186081647873\n",
      "0.058387151919305325\n",
      "0.9420554876327515\n",
      "0.32137641962617636\n",
      "0.4044698230922222\n",
      "0.12231862815096972\n",
      "0.17400535130873324\n",
      "0.04595184549689293\n",
      "0.24630340952426194\n",
      "0.4197198674082756\n",
      "0.8824787735939026\n",
      "0.07400255254469813\n",
      "0.719606564939022\n",
      "0.4695404008030892\n",
      "0.3246252406388521\n",
      "0.09972050166688859\n",
      "0.4659539442509412\n",
      "0.185841753333807\n",
      "0.575751519203186\n",
      "0.3465549051761627\n",
      "0.28096900321543217\n",
      "0.07925002872943877\n",
      "0.6187464267015458\n",
      "0.3284653339534998\n",
      "0.44621194675564774\n",
      "0.5920830391347409\n",
      "0.47337096035480497\n",
      "0.47396581806242466\n",
      "0.3040944730862975\n",
      "0.388483776524663\n",
      "0.22736003827303647\n",
      "0.08319158074446023\n",
      "0.6380097158253194\n",
      "0.31590093150734894\n",
      "0.19458833243697882\n",
      "0.4496187850832939\n",
      "0.20158926174044609\n",
      "0.3251747582107783\n",
      "0.04915572055615484\n",
      "0.788822278380394\n",
      "0.25696405870839956\n",
      "0.8377492576837541\n",
      "0.9083747625350952\n",
      "0.643785147368908\n",
      "0.03842525612562894\n",
      "0.5598907515406608\n",
      "0.5940086603164673\n",
      "0.6428986579179764\n",
      "0.8185198664665222\n",
      "0.15907116271555422\n",
      "0.09694506712257864\n",
      "0.7616657704114913\n",
      "0.6698829382658005\n",
      "0.12394705265760422\n",
      "0.7532439351081848\n",
      "0.6110621426254511\n",
      "0.7378153651952745\n",
      "0.29191719684749845\n",
      "0.45031063407659533\n",
      "0.3448990859091282\n",
      "0.7531918019056322\n",
      "0.4699552975594997\n",
      "0.2594941960647702\n",
      "0.7922142744064331\n",
      "0.30106526454910637\n",
      "0.07512407889589667\n",
      "0.40866312682628636\n",
      "0.2833807617425919\n",
      "0.45175662636756897\n",
      "0.19810914201661944\n",
      "0.05191276781260968\n",
      "0.6867576897144319\n",
      "0.8912242591381073\n",
      "0.10299358712509274\n",
      "0.8115105986595155\n",
      "0.10887925289571286\n",
      "0.7010310381650925\n",
      "0.8513970136642456\n",
      "0.049390054773539305\n",
      "0.5374250255525113\n",
      "0.3582292463630437\n",
      "0.03007951464969665\n",
      "0.048862856952473534\n",
      "0.054506441904231914\n",
      "0.24862051382660869\n",
      "0.09466293975710868\n",
      "0.37150670066475866\n",
      "0.23713855817914006\n",
      "0.4475206516683101\n",
      "0.3068747129291296\n",
      "0.3231460824608803\n",
      "0.10506615131162107\n",
      "0.8907013058662414\n",
      "0.777083745598793\n",
      "0.3301302745938301\n",
      "0.709891664981842\n",
      "0.3740422405302524\n",
      "0.6684200823307037\n",
      "0.22608860805630682\n",
      "0.6224499419331551\n",
      "0.10921431840397418\n",
      "0.8654544532299042\n",
      "0.5826668255031108\n",
      "0.07255651955492795\n",
      "0.596676803380251\n",
      "0.8134157776832581\n",
      "0.904932028055191\n",
      "0.8663699686527252\n",
      "0.844917756319046\n",
      "0.4404889449477195\n",
      "0.4780433313921094\n",
      "0.15538634434342385\n",
      "0.32117989063262936\n",
      "0.6539925605058671\n",
      "0.31438774019479754\n",
      "0.044223846448585395\n",
      "0.1291533180512488\n",
      "0.8522411763668061\n",
      "0.4388827342540026\n",
      "0.2897147139534354\n",
      "0.3859245575964451\n",
      "0.5742741361260415\n",
      "0.17246136965695769\n",
      "0.6798476994037629\n",
      "0.6449851334095\n",
      "0.03584862271090969\n",
      "0.5655946522951125\n",
      "0.12592999478802086\n",
      "0.14484310559928415\n",
      "0.8610150694847105\n",
      "0.035170274181291455\n",
      "0.8072308719158172\n",
      "0.7277325391769409\n",
      "0.7899461090564728\n",
      "0.4948594599962233\n",
      "0.6799176692962646\n",
      "0.9295793473720549\n",
      "0.7419647991657258\n",
      "0.4766688905656337\n",
      "0.34942837543785576\n",
      "0.2153049439191818\n",
      "0.14391262102872135\n",
      "0.6669316738843918\n",
      "0.6838836312294007\n",
      "0.6167448848485947\n",
      "0.15641769459471108\n",
      "0.8394692182540894\n",
      "0.7829926550388336\n",
      "0.9208760023117065\n",
      "0.2577202260494232\n",
      "0.02798345424234867\n",
      "0.6448771119117737\n",
      "0.5796043142676354\n",
      "0.171463031694293\n",
      "0.20196421332657338\n",
      "0.5001480340957641\n",
      "0.15457316292449833\n",
      "0.3577403984963894\n",
      "0.4499394997954369\n",
      "0.14978476986289024\n",
      "0.2950277470052242\n",
      "0.8970231175422667\n",
      "0.9178919017314912\n",
      "0.6410599581897258\n",
      "0.046882278379052886\n",
      "0.6218718767166137\n",
      "0.7171026974916459\n",
      "0.8680590569972992\n",
      "0.5878162689507007\n",
      "0.05314025832340121\n",
      "0.8730139851570129\n",
      "0.878466084599495\n",
      "0.1727493081241846\n",
      "0.7515144199132919\n",
      "0.3435929061844945\n",
      "0.7952016115188597\n",
      "0.7794640660285949\n",
      "0.6576979756355286\n",
      "0.2944060102105141\n",
      "0.1757518216967583\n",
      "0.8757485628128053\n",
      "0.30046597197651864\n",
      "0.6824373662471771\n",
      "0.45822309628129004\n",
      "0.6870693892240525\n",
      "0.3088843487203121\n",
      "0.16333411727100614\n",
      "0.7256380796432496\n",
      "0.21823771661147473\n",
      "0.7683078110218048\n",
      "0.43603006005287176\n",
      "0.613663886487484\n",
      "0.2507193922996521\n",
      "0.12769508296623827\n",
      "0.3070776160806417\n",
      "0.10913215209729969\n",
      "0.09539163094013929\n",
      "0.29831600021570914\n",
      "0.6571942299604416\n",
      "0.42425475642085075\n",
      "0.07382575212977827\n",
      "0.1356473587453365\n",
      "0.5215970903635025\n",
      "0.10843620561063289\n",
      "0.11271594786085187\n",
      "0.41885067410767085\n",
      "0.47283780463039865\n",
      "0.29395036268979313\n",
      "0.19606518149375915\n",
      "0.8459716200828553\n",
      "0.0577575312461704\n",
      "0.1062476273626089\n",
      "0.04388445406220853\n",
      "0.27444781251251693\n",
      "0.08457696270197632\n",
      "0.8738220930099487\n",
      "0.6430691622197628\n",
      "0.7461127579212189\n",
      "0.6299389258027076\n",
      "0.47986695170402527\n",
      "0.7630485832691192\n",
      "0.7878241688013076\n",
      "0.20839356277137994\n",
      "0.4219215668737888\n",
      "0.43319302946329113\n",
      "0.10843273280188442\n",
      "0.4361356101930141\n",
      "0.9269074320793153\n",
      "0.3004146568477154\n",
      "0.15457211090251805\n",
      "0.07904664874076842\n",
      "0.5669530391693115\n",
      "0.7794881969690323\n",
      "0.16415873449295756\n",
      "0.8006645500659942\n",
      "0.2005313958972693\n",
      "0.10009984169155357\n",
      "0.7222195982933043\n",
      "0.7683011382818223\n",
      "0.18937982358038424\n",
      "0.7606425702571868\n",
      "0.2234717532992363\n",
      "0.5494690462946892\n",
      "0.7270982474088669\n",
      "0.18966740192845466\n",
      "0.1686246797442436\n",
      "0.1270101783797145\n",
      "0.04582100356929004\n",
      "0.9089842379093169\n",
      "0.6010845482349395\n",
      "0.5838932678103447\n",
      "0.30565591901540756\n",
      "0.4293823890388012\n",
      "0.033593490021303296\n",
      "0.19795591048896313\n",
      "0.540210010111332\n",
      "0.30908383466303346\n",
      "0.6598741978406906\n",
      "0.19739953055977824\n",
      "0.8261829376220704\n",
      "0.6974460452795029\n",
      "0.22849406935274602\n",
      "0.14155072546564043\n",
      "0.647557583451271\n",
      "0.42992705032229434\n",
      "0.6073700040578842\n",
      "0.7718809545040131\n",
      "0.6771587789058685\n",
      "0.6826461315155029\n",
      "0.21658903621137143\n",
      "0.07799334675073624\n",
      "0.04286193749867379\n",
      "0.08052309746854007\n",
      "0.346307472884655\n",
      "0.25182301439344884\n",
      "0.3667049422860145\n",
      "0.7769818246364594\n",
      "0.2559540688991547\n",
      "0.30008151568472385\n",
      "0.5662295639514923\n",
      "0.18462093584239486\n",
      "0.8043932884931564\n",
      "0.30004537254571917\n",
      "0.2869056064635515\n",
      "0.44358777105808256\n",
      "0.6204271029680967\n",
      "0.7732409119606017\n",
      "0.1250258246436715\n",
      "0.1952820084989071\n",
      "0.49771908670663834\n",
      "0.14928784612566232\n",
      "0.701345607638359\n",
      "0.4767982423305511\n",
      "0.545683378726244\n",
      "0.7059778988361358\n",
      "0.7630045235157012\n",
      "0.9321550488471985\n",
      "0.8260744929313659\n",
      "0.7870611011981965\n",
      "0.1641887489706278\n",
      "0.35213582217693323\n",
      "0.34775153547525406\n",
      "0.22004940155893568\n",
      "0.07869090633466841\n",
      "0.39906226471066475\n",
      "0.4929458528757094\n",
      "0.060134654585272077\n",
      "0.8242491066455842\n",
      "0.6257583409547806\n",
      "0.7519356578588486\n",
      "0.38959962837398054\n",
      "0.37231676215305926\n",
      "0.5951907977461814\n",
      "0.6571655720472337\n",
      "0.8057499438524247\n",
      "0.3933469120413065\n",
      "0.09946984099224211\n",
      "0.8350917607545852\n",
      "0.12550212647765877\n",
      "0.2840460499748587\n",
      "0.23408571453765037\n",
      "0.8551755905151367\n",
      "0.6335130661725998\n",
      "0.7792452007532121\n",
      "0.9041577398777009\n",
      "0.23640415556728842\n",
      "0.07788638919591905\n",
      "0.3445005282759666\n",
      "0.3617630653083324\n",
      "0.07818914426025003\n",
      "0.6922082833945751\n",
      "0.4011023923754692\n",
      "0.06176906642504037\n",
      "0.35698261409997933\n",
      "0.47467931136488917\n",
      "0.2828168384730816\n",
      "0.23483162531629204\n",
      "0.34715532697737217\n",
      "0.31119963265955447\n",
      "0.32821472287178044\n",
      "0.20131577029824257\n",
      "0.3382026065140963\n",
      "0.7178349286317826\n",
      "0.7212594807147981\n",
      "0.0792345121037215\n",
      "0.12185349776409565\n",
      "0.1021800959482789\n",
      "0.12246031947433948\n",
      "0.03914302901830524\n",
      "0.301373915746808\n",
      "0.633628137409687\n",
      "0.5463531509041786\n",
      "0.1451391705777496\n",
      "0.25982967298477894\n",
      "0.8494129538536072\n",
      "0.6237172424793243\n",
      "0.39543722718954083\n",
      "0.5253713950514793\n",
      "0.7849222600460053\n",
      "0.3557375343516469\n",
      "0.16893901061266658\n",
      "0.09773614685982465\n",
      "0.18878731802105905\n",
      "0.21778853815048935\n",
      "0.20091681322082877\n",
      "0.1768087301403284\n",
      "0.4580439910292625\n",
      "0.419554328918457\n",
      "0.0544589102268219\n",
      "0.08283003959804773\n",
      "0.6982936590909958\n",
      "0.7432955771684646\n",
      "0.38986400291323664\n",
      "0.3627231294289231\n",
      "0.7583512514829638\n",
      "0.9038074970245362\n",
      "0.11651816610246897\n",
      "0.6496677339076996\n",
      "0.27717426512390375\n",
      "0.08227758118882778\n",
      "0.7612664461135864\n",
      "0.8062419414520264\n",
      "0.17060000225901606\n",
      "0.9185418665409089\n",
      "0.7186454325914382\n",
      "0.24876871109008786\n",
      "0.8751618266105653\n",
      "0.17390256747603416\n",
      "0.880984950065613\n",
      "0.04102876032702625\n",
      "0.24207895901054144\n",
      "0.3485277108848095\n",
      "0.16039868486113848\n",
      "0.38851376175880437\n",
      "0.055160920787602666\n",
      "0.4865027830004692\n",
      "0.13753246357664464\n",
      "0.3125923380255699\n",
      "0.16374349514953793\n",
      "0.18758865604177116\n",
      "0.6187672693282367\n",
      "0.8970600545406342\n",
      "0.5215920098125935\n",
      "0.22611297070980071\n",
      "0.9180022299289704\n",
      "0.18759953435510396\n",
      "0.20908023221418262\n",
      "0.740218710899353\n",
      "0.20695225144736468\n",
      "0.8782533764839172\n",
      "0.16639750069007275\n",
      "0.08468175521120427\n",
      "0.11748343082144856\n",
      "0.3099484886974096\n",
      "0.5632406070828437\n",
      "0.69977667927742\n",
      "0.2578249882906675\n",
      "0.8129909485578537\n",
      "0.4462103024125099\n",
      "0.5362128302454948\n",
      "0.12875719051808118\n",
      "0.9296081483364106\n",
      "0.4896819040179253\n",
      "0.40927217975258834\n",
      "0.438518288731575\n",
      "0.4589005054906011\n",
      "0.2408476269803941\n",
      "0.77999547123909\n",
      "0.8302097082138061\n",
      "0.18851463720202447\n",
      "0.19138647550716997\n",
      "0.10096713285893202\n",
      "0.41326984912157055\n",
      "0.42183487489819527\n",
      "0.6368730306625365\n",
      "0.6455197036266327\n",
      "0.5342695083469152\n",
      "0.11351921721361577\n",
      "0.8384411036968231\n",
      "0.25624635349959135\n",
      "0.5244806826114654\n",
      "0.2481599900871515\n",
      "0.1400519862771034\n",
      "0.3087943851947784\n",
      "0.7774508729577065\n",
      "0.29324034713208674\n",
      "0.29610149562358856\n",
      "0.8656593501567841\n",
      "0.4234039936214686\n",
      "0.3703875854611397\n",
      "0.4039169684052467\n",
      "0.5445416241884231\n",
      "0.05649262731894851\n",
      "0.2092242531478405\n",
      "0.6712305188179016\n",
      "0.10044144093990326\n",
      "0.39796222709119317\n",
      "0.2845596205443144\n",
      "0.16659168656915424\n",
      "0.3677369609475137\n",
      "0.14865663731470705\n",
      "0.13970094909891487\n",
      "0.3062562745064497\n",
      "0.7562462747097015\n",
      "0.6357295423746109\n",
      "0.38754608035087584\n",
      "0.29560364279896023\n",
      "0.33053374513983724\n",
      "0.2595587232150137\n",
      "0.16556611172854901\n",
      "0.6181252986192703\n",
      "0.3811267700046301\n",
      "0.4823158057406544\n",
      "0.04674525498412549\n",
      "0.08838410712778569\n",
      "0.917825210094452\n",
      "0.8275752305984497\n",
      "0.05075758835300803\n",
      "0.7156303018331527\n",
      "0.3032667713239789\n",
      "0.8305798590183259\n",
      "0.2891962885856629\n",
      "0.8294721961021424\n",
      "0.8272446632385253\n",
      "0.1312295773997903\n",
      "0.5719875991344452\n",
      "0.282203059643507\n",
      "0.8624116152524949\n",
      "0.19630756573751568\n",
      "0.8512897789478301\n",
      "0.7347069799900054\n",
      "0.17731710039079193\n",
      "0.04322585959453135\n",
      "0.20408633220940828\n",
      "0.6295814007520675\n",
      "0.15787984393537047\n",
      "0.12414033552631734\n",
      "0.08465831885114312\n",
      "0.3727245647460222\n",
      "0.8428482413291931\n",
      "0.052635407308116555\n",
      "0.11552950241602956\n",
      "0.31824743617326023\n",
      "0.14104982484132053\n",
      "0.3669758558273316\n",
      "0.2518347072415054\n",
      "0.2953019931912422\n",
      "0.4029166407883168\n",
      "0.1455290629528463\n",
      "0.8262551546096801\n",
      "0.293900060467422\n",
      "0.6348594069480896\n",
      "0.13250393318012357\n",
      "0.07380613065324723\n",
      "0.8942168533802033\n",
      "0.04124738797545433\n",
      "0.38413196876645084\n",
      "0.7090035647153854\n",
      "0.7469309985637664\n",
      "0.6899596810340881\n",
      "0.08470111088827252\n",
      "0.13852426093071699\n",
      "0.4214805141091347\n",
      "0.6391342207789421\n",
      "0.19342077299952504\n",
      "0.2921495517715812\n",
      "0.5515448540449143\n",
      "0.13559386152774094\n",
      "0.7325044840574265\n",
      "0.08436069786548615\n",
      "0.3503279469907284\n",
      "0.0524246433749795\n",
      "0.312099888175726\n",
      "0.16433710251003503\n",
      "0.1327839180827141\n",
      "0.7826373219490051\n",
      "0.9189839780330658\n",
      "0.8626076102256776\n",
      "0.6867883890867233\n",
      "0.06379166767001153\n",
      "0.8648279309272766\n",
      "0.636393104493618\n",
      "0.7968807518482208\n",
      "0.5069154411554336\n",
      "0.7935140013694764\n",
      "0.8074194967746734\n",
      "0.7399272590875626\n",
      "0.8255993247032165\n",
      "0.23872981127351525\n",
      "0.8432525277137756\n",
      "0.4512690871953965\n",
      "0.8489973545074462\n",
      "0.5154289722442627\n",
      "0.3866982473060489\n",
      "0.1939469520002604\n",
      "0.17696727309376\n",
      "0.19426966849714516\n",
      "0.8364445388317109\n",
      "0.46362898647785183\n",
      "0.1714780194684863\n",
      "0.0628649223363027\n",
      "0.38748381957411765\n",
      "0.08498503752052783\n",
      "0.8433425605297087\n",
      "0.6125863492488861\n",
      "0.868533432483673\n",
      "0.7969686627388001\n",
      "0.33846665993332864\n",
      "0.07118153171613814\n",
      "0.22656644778326154\n",
      "0.42315901517868043\n",
      "0.7208075731992722\n",
      "0.1678899198770523\n",
      "0.07054570522159338\n",
      "0.14354436583817004\n",
      "0.2898560511879623\n",
      "0.2972883768379688\n",
      "0.42111249640584\n",
      "0.48023807555437087\n",
      "0.35567655293270944\n",
      "0.25374894924461844\n",
      "0.15176397999748586\n",
      "0.100453880475834\n",
      "0.1445984593592584\n",
      "0.054588167928159244\n",
      "0.7043107517063618\n",
      "0.46488186269998555\n",
      "0.8429020047187806\n",
      "0.8366341054439543\n",
      "0.30619341433048247\n",
      "0.4486343488097191\n",
      "0.4067661866545677\n",
      "0.34158234149217603\n",
      "0.07159116829279809\n",
      "0.038871173001825805\n",
      "0.714616584777832\n",
      "0.046390281827189024\n",
      "0.2777732528746128\n",
      "0.7455555558204651\n",
      "0.024526958912611006\n",
      "0.838716894388199\n",
      "0.8160035252571106\n",
      "0.11800771784037352\n",
      "0.27723725540563465\n",
      "0.7178960353136062\n",
      "0.7988396942615509\n",
      "0.7573903739452361\n",
      "0.3093830294907093\n",
      "0.5368146881461143\n",
      "0.2915804281830787\n",
      "0.7800799638032913\n",
      "0.9618797779083252\n",
      "0.8329411447048187\n",
      "0.17989011760801074\n",
      "0.5465750802308321\n",
      "0.27465660385787494\n",
      "0.5179420188069344\n",
      "0.19125129375606773\n",
      "0.689015617966652\n",
      "0.08114396228920669\n",
      "0.3654774634167552\n",
      "0.2532677303999662\n",
      "0.09448150820098818\n",
      "0.10523411510512233\n",
      "0.9033047795295716\n",
      "0.13816045867279172\n",
      "0.26763145104050634\n",
      "0.14419772550463675\n",
      "0.19066251423209907\n",
      "0.24323692452162501\n",
      "0.49814247936010353\n",
      "0.09351341621950268\n",
      "0.8124480575323105\n",
      "0.11655700616538524\n",
      "0.11058740736916661\n",
      "0.5642933458089828\n",
      "0.2615811347961426\n",
      "0.11464389031752945\n",
      "0.16322588250041006\n",
      "0.6549871504306792\n",
      "0.17986499406397344\n",
      "0.1038509303703904\n",
      "0.5634823247790337\n",
      "0.20632601315155624\n",
      "0.8174407362937928\n",
      "0.13213636809960008\n",
      "0.16464614272117614\n",
      "0.09787946324795485\n",
      "0.20878512971103194\n",
      "0.4024441301822662\n",
      "0.4155795082449913\n",
      "0.7925166726112366\n",
      "0.4440451622009277\n",
      "0.5587357748299836\n",
      "0.7568042159080506\n",
      "0.11819584863260388\n",
      "0.8313979119062423\n",
      "0.5697447538375855\n",
      "0.32189943492412565\n",
      "0.11880758609622716\n",
      "0.8609150886535644\n",
      "0.7235426992177963\n",
      "0.22890756214037536\n",
      "0.6097525238990783\n",
      "0.7669129312038421\n",
      "0.42944395095109944\n",
      "0.05120387529022991\n",
      "0.03727442696690559\n",
      "0.5312732666730882\n",
      "0.06104300385341048\n",
      "0.5914252951741218\n",
      "0.8784122943878173\n",
      "0.22959447801113125\n",
      "0.09018072560429573\n",
      "0.6338680803775787\n",
      "0.057288946630433206\n",
      "0.8752202808856964\n",
      "0.19472894705832008\n",
      "0.13404290638864041\n",
      "0.545081526041031\n",
      "0.5585886016488075\n",
      "0.15387437818571922\n",
      "0.642148993909359\n",
      "0.9404496550559996\n",
      "0.025501394411548972\n",
      "0.5461373969912529\n",
      "0.9042090237140656\n",
      "0.0461040306603536\n",
      "0.12095811776816845\n",
      "0.09035960212349889\n",
      "0.32350374311208724\n",
      "0.7191088229417802\n",
      "0.05766588384285569\n",
      "0.7339481204748154\n",
      "0.34424188956618307\n",
      "0.9069901287555694\n",
      "0.16795449233613907\n",
      "0.6718972027301788\n",
      "0.3728998405858874\n",
      "0.6743962854146958\n",
      "0.19331470103934406\n",
      "0.09468008670955896\n",
      "0.5794334799051285\n",
      "0.09253420922905209\n",
      "0.31479405909776687\n",
      "0.7831685602664946\n",
      "0.6764017846435308\n",
      "0.2299159704707563\n",
      "0.22188836969435216\n",
      "0.5852469131350516\n",
      "0.07609282247722149\n",
      "0.2667296126484871\n",
      "0.5337062194943428\n",
      "0.05471396651118994\n",
      "0.6078275457024575\n",
      "0.3170284179970623\n",
      "0.879147481918335\n",
      "0.03134520528838038\n",
      "0.018720410997048017\n",
      "0.044747915118932724\n",
      "0.0864627311937511\n",
      "0.06179163814522326\n",
      "0.09450970892794432\n",
      "0.25807160809636115\n",
      "0.3965649425983429\n",
      "0.07097626712638884\n",
      "0.3525020122528076\n",
      "0.34117199946194887\n",
      "0.7917043447494507\n",
      "0.1120518460869789\n",
      "0.5722449958324433\n",
      "0.2541405133903027\n",
      "0.40721606761217105\n",
      "0.5742013528943062\n",
      "0.06184877371415495\n",
      "0.635985180735588\n",
      "0.42904234752058984\n",
      "0.3559789776802063\n",
      "0.20837244503200056\n",
      "0.6363958030939101\n",
      "0.6130228847265243\n",
      "0.5293923437595367\n",
      "0.7781470298767089\n",
      "0.1821838106960058\n",
      "0.9353464126586916\n",
      "0.8251030862331391\n",
      "0.7836120605468752\n",
      "0.24150535762310027\n",
      "0.7070625394582748\n",
      "0.31296712458133696\n",
      "0.6946116998791694\n",
      "0.08950291345827281\n",
      "0.8782751619815827\n",
      "0.8249996364116667\n",
      "0.893284809589386\n",
      "0.037379752239212395\n",
      "0.12872533444315196\n",
      "0.3938855390995741\n",
      "0.7911686658859253\n",
      "0.8230193436145782\n",
      "0.43067530244588853\n",
      "0.5203660026192666\n",
      "0.5554390806704759\n",
      "0.6349243223667145\n",
      "0.1250345277134329\n",
      "0.7798517286777495\n",
      "0.8406406670808793\n",
      "0.3676309745758772\n",
      "0.21970624402165415\n",
      "0.8965772330760957\n",
      "0.46441372483968735\n",
      "0.14246097449213266\n",
      "0.7449412241578103\n",
      "0.5177729506045579\n",
      "0.25335964784026144\n",
      "0.895630657672882\n",
      "0.06004603365436196\n",
      "0.20411641038954256\n",
      "0.04502621875144541\n",
      "0.09024401837959886\n",
      "0.4777176320552826\n",
      "0.8352234721183778\n",
      "0.1732336523011327\n",
      "0.7686171889305115\n",
      "0.21818987671285867\n",
      "0.853613430261612\n",
      "0.2711182489991188\n",
      "0.0561529714614153\n",
      "0.6893526971340179\n",
      "0.41273597590625294\n",
      "0.38781441450119014\n",
      "0.1409821027657017\n",
      "0.32563385367393494\n",
      "0.38770539164543155\n",
      "0.36940805837512014\n",
      "0.38403659388422967\n",
      "0.4967683620750904\n",
      "0.3827614128589631\n",
      "0.5294014878571034\n",
      "0.13134131487458947\n",
      "0.428560283780098\n",
      "0.41596292108297345\n",
      "0.04889760250225664\n",
      "0.23950965069234373\n",
      "0.9364854454994201\n",
      "0.5551864832639695\n",
      "0.14120686268433927\n",
      "0.2799296710640192\n",
      "0.3666028460487723\n",
      "0.5778116717934608\n",
      "0.8592211782932283\n",
      "0.6801012516021728\n",
      "0.5612520668655634\n",
      "0.32941890209913255\n",
      "0.10800058636814355\n",
      "0.6997193589806556\n",
      "0.40571400523185736\n",
      "0.23204002100974325\n",
      "0.5835850983858109\n",
      "0.5337641313672066\n",
      "0.6289992868900299\n",
      "0.11668341197073459\n",
      "0.5494172856211662\n",
      "0.8349518835544586\n",
      "0.8923731803894044\n",
      "0.42447376176714896\n",
      "0.49006226658821106\n",
      "0.4539264384657145\n",
      "0.5635733872652054\n",
      "0.6688476592302323\n",
      "0.6052160223945975\n",
      "0.07350350562483073\n",
      "0.4472398875281215\n",
      "0.8273623943328858\n",
      "0.8227866947650909\n",
      "0.04546077479608357\n",
      "0.7951821841299535\n",
      "0.879364401102066\n",
      "0.24438514374196532\n",
      "0.17940829321742058\n",
      "0.8673244267702103\n",
      "0.20504951123148205\n",
      "0.31551127061247825\n",
      "0.2800847483798862\n",
      "0.8330220162868499\n",
      "0.5413275081664324\n",
      "0.1730499326251447\n",
      "0.7937655240297318\n",
      "0.04329476240091026\n",
      "0.29441838115453717\n",
      "0.8445259273052216\n",
      "0.22856948003172872\n",
      "0.17377472519874573\n",
      "0.185727458819747\n",
      "0.6052890881896018\n",
      "0.28226454369723797\n",
      "0.27866096049547195\n",
      "0.7335286498069763\n",
      "0.3476140178740025\n",
      "0.05966183040291071\n",
      "0.5170140199363231\n",
      "0.3420958884060382\n",
      "0.06517011856194585\n",
      "0.6050444662570954\n",
      "0.6807568132877351\n",
      "0.7679499119520187\n",
      "0.3958836402744055\n",
      "0.06725492859259248\n",
      "0.25049003036692735\n",
      "0.6961871802806854\n",
      "0.18772983346134425\n",
      "0.4296540826559067\n",
      "0.27575275748968125\n",
      "0.37096027880907056\n",
      "0.3231075577437877\n",
      "0.3618165425956249\n",
      "0.3912745758891105\n",
      "0.28743590973317623\n",
      "0.5900779366493225\n",
      "0.4999199211597443\n",
      "0.31749175274744634\n",
      "0.4235211826860905\n",
      "0.41288866773247723\n",
      "0.7342007219791412\n",
      "0.10859629195183515\n",
      "0.9368191123008728\n",
      "0.7781980633735657\n",
      "0.14475515708327294\n",
      "0.7341922670602798\n",
      "0.6914615750312805\n",
      "0.4987829208374023\n",
      "0.840414994955063\n",
      "0.29954868368804455\n",
      "0.20407191067934038\n",
      "0.22537486003711818\n",
      "0.23499514441937208\n",
      "0.37391820922493935\n",
      "0.4580088481307029\n",
      "0.21474550366401673\n",
      "0.1372801475226879\n",
      "0.11647231336683037\n",
      "0.27917966805398464\n",
      "0.048556511243805285\n",
      "0.11835321336984635\n",
      "0.14907357525080445\n",
      "0.5447670899331569\n",
      "0.07667716317810118\n",
      "0.402592626772821\n",
      "0.6309000611305238\n",
      "0.47116293348371985\n",
      "0.7449350327253341\n",
      "0.14589501973241567\n",
      "0.5317095100879669\n",
      "0.08816072950139642\n",
      "0.04612738883588463\n",
      "0.11966308252885938\n",
      "0.3330195195972919\n",
      "0.601648473739624\n",
      "0.2297461401671171\n",
      "0.07811871343292297\n",
      "0.8277183532714845\n",
      "0.6394098371267318\n",
      "0.044587919791229065\n",
      "0.32494952380657194\n",
      "0.3510303363204002\n",
      "0.5825707629323007\n",
      "0.17012351304292683\n",
      "0.7714972198009491\n",
      "0.27325106859207154\n",
      "0.8696698963642121\n",
      "0.216719532944262\n",
      "0.09101954144425689\n",
      "0.07637664023786782\n",
      "0.5190802834928037\n",
      "0.9682979941368103\n",
      "0.10167450839653612\n",
      "0.23865548484027388\n",
      "0.4959496259689331\n",
      "0.06541118281893432\n",
      "0.4852106742560863\n",
      "0.10565889608114958\n",
      "0.4940858915448189\n",
      "0.052063886635005475\n",
      "0.40929739922285074\n",
      "0.37131063882261517\n",
      "0.5097688876092433\n",
      "0.10978300436399879\n",
      "0.14647854245267808\n",
      "0.30262056216597555\n",
      "0.741293978691101\n",
      "0.5216353744268417\n",
      "0.161878001736477\n",
      "0.26157082496210937\n",
      "0.38840173482894896\n",
      "0.307665978372097\n",
      "0.44197290986776355\n",
      "0.7169699639081955\n",
      "0.0716344649437815\n",
      "0.5306579537689685\n",
      "0.41877634227275845\n",
      "0.046013092249631876\n",
      "0.432429951429367\n",
      "0.13737164847552777\n",
      "0.41570828706026075\n",
      "0.25079470239579676\n",
      "0.160148554854095\n",
      "0.6991305857896805\n",
      "0.7427762597799301\n",
      "0.42261452749371525\n",
      "0.8060387313365937\n",
      "0.05718086296692491\n",
      "0.5795800566673279\n",
      "0.9284342944622039\n",
      "0.41519218236207966\n",
      "0.51543530523777\n",
      "0.5648882925510407\n",
      "0.24222215004265307\n",
      "0.1477689454331994\n",
      "0.16085286028683185\n",
      "0.5242168944329023\n",
      "0.6195682510733606\n",
      "0.9363557398319244\n",
      "0.7150776028633119\n",
      "0.2930249910801649\n",
      "0.7313522547483444\n",
      "0.25122984908521173\n",
      "0.23110690005123616\n",
      "0.5912002548575401\n",
      "0.4594150081276894\n",
      "0.7572400063276291\n",
      "0.11850018268451096\n",
      "0.556313456594944\n",
      "0.19885723758488894\n",
      "0.09081767927855253\n",
      "0.5302485063672066\n",
      "0.5492728754878045\n",
      "0.433732770383358\n",
      "0.8996241152286528\n",
      "0.10877791964448986\n",
      "0.4525063186883927\n",
      "0.5071399495005607\n",
      "0.6154010631144047\n",
      "0.1662904916331172\n",
      "0.5851484306156635\n",
      "0.7657333493232727\n",
      "0.44220633134245874\n",
      "0.4325887575745583\n",
      "0.9301185309886932\n",
      "0.11048712879419328\n",
      "0.8597140222787856\n",
      "0.2998493338003755\n",
      "0.4669460169970989\n",
      "0.41715872734785076\n",
      "0.891530728340149\n",
      "0.5620177723467351\n",
      "0.2677779905498028\n",
      "0.0940531391184777\n",
      "0.6130107074975967\n",
      "0.7276846319437027\n",
      "0.049285336583852764\n",
      "0.6202186658978462\n",
      "0.566498339176178\n",
      "0.206643239967525\n",
      "0.21440454162657263\n",
      "0.8142224490642547\n",
      "0.31276242062449455\n",
      "0.7664409458637238\n",
      "0.8977820813655852\n",
      "0.24982085451483724\n",
      "0.8637811601161955\n",
      "0.3106030136346817\n",
      "0.4491836130619049\n",
      "0.27384738195687536\n",
      "0.4613350920379161\n",
      "0.07029618010856212\n",
      "0.1572114869952202\n",
      "0.1279910646378994\n",
      "0.17362072383984925\n",
      "0.7311448514461516\n",
      "0.10127708017826081\n",
      "0.06918249996379018\n",
      "0.41378830969333646\n",
      "0.8227095335721969\n",
      "0.429325969517231\n",
      "0.18450027499347926\n",
      "0.37777455337345595\n",
      "0.9181541442871095\n",
      "0.08300378257408737\n",
      "0.29469251111149786\n",
      "0.5807364121079446\n",
      "0.3120515186339617\n",
      "0.7787344038486481\n",
      "0.8980940163135528\n",
      "0.23701024390757086\n",
      "0.7824616819620133\n",
      "0.8044568419456484\n",
      "0.14729177188128234\n",
      "0.23727664500474932\n",
      "0.5831438392400742\n",
      "0.8722314178943634\n",
      "0.05450474200770259\n",
      "0.06545887778047474\n",
      "0.04815955986268819\n",
      "0.8286880791187287\n",
      "0.7964567214250565\n",
      "0.1098298673518002\n",
      "0.44011248759925364\n",
      "0.07950670805294066\n",
      "0.13454975951462983\n",
      "0.8889436990022659\n",
      "0.5189312800765038\n",
      "0.743159008026123\n",
      "0.5095461070537567\n",
      "0.46355901360511775\n",
      "0.1467104958370328\n",
      "0.09505720101296902\n",
      "0.0644580172840506\n",
      "0.2230045571923256\n",
      "0.4717115607112647\n",
      "0.8198903083801269\n",
      "0.20987192764878274\n",
      "0.1638817224651575\n",
      "0.38083282858133316\n",
      "0.11710292687639592\n",
      "0.7460119396448134\n",
      "0.8857303977012634\n",
      "0.07431496568024157\n",
      "0.44897394478321073\n",
      "0.05919086867943406\n",
      "0.5470944866538049\n",
      "0.06413785272743552\n",
      "0.8722227096557617\n",
      "0.7226560831069946\n",
      "0.18917395807802675\n",
      "0.6152589350938796\n",
      "0.3037642389535904\n",
      "0.11208776840940117\n",
      "0.6417132705450058\n",
      "0.3561867706477642\n",
      "0.2544459316879511\n",
      "0.7831971943378448\n",
      "0.9368808805942536\n",
      "0.12017480325885116\n",
      "0.3135166957974434\n",
      "0.050870524812489756\n",
      "0.09470512196421622\n",
      "0.3494789347052574\n",
      "0.8842511057853698\n",
      "0.05773860430344939\n",
      "0.12015607631765306\n",
      "0.19914947226643562\n",
      "0.07947008712217211\n",
      "0.14413407295942307\n",
      "0.687323322892189\n",
      "0.926037210226059\n",
      "0.031960654677823185\n",
      "0.47424713596701623\n",
      "0.1534695071168244\n",
      "0.4069015469402075\n",
      "0.480641333386302\n",
      "0.6076041609048843\n",
      "0.8786197185516358\n",
      "0.26405080240219836\n",
      "0.8289153397083282\n",
      "0.04623084058985114\n",
      "0.7521283924579621\n",
      "0.6937223076820372\n",
      "0.7129042834043503\n",
      "0.6413750484585762\n",
      "0.5603549614548682\n",
      "0.07353974250145255\n",
      "0.4357484064996242\n",
      "0.38460708707571034\n",
      "0.7341762572526932\n",
      "0.16380569376051426\n",
      "0.23930744230747222\n",
      "0.4304296039044857\n",
      "0.10691713853739201\n",
      "0.10103305224329233\n",
      "0.15603223801590502\n",
      "0.07117905591148883\n",
      "0.6094269782304764\n",
      "0.18524760138243437\n",
      "0.388141407072544\n",
      "0.14199492167681455\n",
      "0.6780670285224913\n",
      "0.6876760438084601\n",
      "0.06278169206343591\n",
      "0.17997083291411403\n",
      "0.5691033840179442\n",
      "0.9227160394191741\n",
      "0.11300555719062684\n",
      "0.34048985689878464\n",
      "0.09765971805900335\n",
      "0.04457185491919517\n",
      "0.38858770579099655\n",
      "0.2853760872036219\n",
      "0.8263882249593736\n",
      "0.43351401686668395\n",
      "0.028107662685215473\n",
      "0.4604414485394955\n",
      "0.30903840959072115\n",
      "0.08367650187574327\n",
      "0.6410480871796608\n",
      "0.6120992690324785\n",
      "0.22952520884573457\n",
      "0.28533226773142817\n",
      "0.9073059856891631\n",
      "0.7396272003650665\n",
      "0.09697207966819406\n",
      "0.852230566740036\n",
      "0.05470321450848132\n",
      "0.5514026850461959\n",
      "0.663955244421959\n",
      "0.7546881079673767\n",
      "0.7081140249967576\n",
      "0.04112776552792638\n",
      "0.8984172582626343\n",
      "0.6073012456297875\n",
      "0.2976372018456459\n",
      "0.10771801350638269\n",
      "0.09265337819233536\n",
      "0.786897674202919\n",
      "0.6698782533407212\n",
      "0.583663785457611\n",
      "0.8751675963401795\n",
      "0.9050464153289796\n",
      "0.7414321035146714\n",
      "0.36862633191049093\n",
      "0.7505078315734862\n",
      "0.03169830893166364\n",
      "0.6023472741246223\n",
      "0.056928207539021974\n",
      "0.4676524624228477\n",
      "0.25299944765865806\n",
      "0.8389584004878998\n",
      "0.5058018792420625\n",
      "0.09274374078959227\n",
      "0.40494736433029177\n",
      "0.6625975936651229\n",
      "0.728301253914833\n",
      "0.8626262336969376\n",
      "0.9165947198867797\n",
      "0.1289687853306532\n",
      "0.1374032229650766\n",
      "0.19197767795994877\n",
      "0.3638297833502293\n",
      "0.8998121321201324\n",
      "0.1655731122009456\n",
      "0.2891110546886921\n",
      "0.04132531010545791\n",
      "0.11201970125548541\n",
      "0.7189908184111119\n",
      "0.4692678421735763\n",
      "0.19232313260436057\n",
      "0.5394350424408912\n",
      "0.3175454087555408\n",
      "0.6967879801988602\n",
      "0.9299358785152435\n",
      "0.7022665351629257\n",
      "0.8890523374080659\n",
      "0.9251354992389679\n",
      "0.07207437744364144\n",
      "0.04282401611562818\n",
      "0.27292200513184073\n",
      "0.07214911803603172\n",
      "0.4581320069730282\n",
      "0.7412757426500322\n",
      "0.3146989408880472\n",
      "0.3975687593221664\n",
      "0.7703221112489699\n",
      "0.22915529347956184\n",
      "0.38708345582708714\n",
      "0.5586804375052452\n",
      "0.7442643016576768\n",
      "0.5988690719008446\n",
      "0.6702211692929269\n",
      "0.09355937205255033\n",
      "0.5321966148912907\n",
      "0.820787924528122\n",
      "0.8886447429656983\n",
      "0.6670875132083893\n",
      "0.12786502381786705\n",
      "0.1420165654271841\n",
      "0.5491907954216003\n",
      "0.40465517081320285\n",
      "0.47877981364727024\n",
      "0.4264980122447014\n",
      "0.8352192491292955\n",
      "0.11122773904353378\n",
      "0.6271782189607621\n",
      "0.5922486692667006\n",
      "0.05654858830384911\n",
      "0.458637847751379\n",
      "0.6290763199329376\n",
      "0.10529129845090211\n",
      "0.8523891866207124\n",
      "0.03006421497557312\n",
      "0.30727765914052724\n",
      "0.5054916340857744\n",
      "0.08915157634764911\n",
      "0.08757679108530284\n",
      "0.059710104204714294\n",
      "0.09678359157405793\n",
      "0.3938226103782653\n",
      "0.5425481829792261\n",
      "0.601561675965786\n",
      "0.14725881135091187\n",
      "0.3327362183481455\n",
      "0.5568324029445648\n",
      "0.8252312779426575\n",
      "0.10713754771277308\n",
      "0.2716673411428928\n",
      "0.5149211786687374\n",
      "0.915900158882141\n",
      "0.2153579074889421\n",
      "0.5678325712680816\n",
      "0.16330252047628163\n",
      "0.20339085087180137\n",
      "0.06494196392595769\n",
      "0.8604601383209228\n",
      "0.10679603349417448\n",
      "0.19320033304393291\n",
      "0.19541383869946005\n",
      "0.5086203023791314\n",
      "0.8007949322462082\n",
      "0.7580741435289383\n",
      "0.0346858074888587\n",
      "0.5690740942955018\n",
      "0.26653423756361005\n",
      "0.5589009523391724\n",
      "0.8894815146923065\n",
      "0.6897264659404755\n",
      "0.8436044692993164\n",
      "0.6632085770368575\n",
      "0.10537928957492114\n",
      "0.19884441290050742\n",
      "0.21935329642146828\n",
      "0.7586788833141327\n",
      "0.1997848026454449\n",
      "0.5349715184420347\n",
      "0.3445435203611851\n",
      "0.17083393279463055\n",
      "0.12041225619614124\n",
      "0.6165583416819572\n",
      "0.6417984992265702\n",
      "0.620864275097847\n",
      "0.4286250509321689\n",
      "0.8135948717594148\n",
      "0.306034130975604\n",
      "0.08940207702107726\n",
      "0.36496367603540425\n",
      "0.7253117382526396\n",
      "0.7434010058641434\n",
      "0.7016161322593689\n",
      "0.8375130832195282\n",
      "0.21324719265103342\n",
      "0.2271868595853448\n",
      "0.3347589619457722\n",
      "0.3856101751327515\n",
      "0.03917043013498187\n",
      "0.4596459805965424\n",
      "0.0547653709538281\n",
      "0.8891630947589874\n",
      "0.2510376028716564\n",
      "0.2070540931075811\n",
      "0.1766081787645817\n",
      "0.10905885929241778\n",
      "0.7103781312704087\n",
      "0.4073048360645772\n",
      "0.08321367213502529\n",
      "0.7103245735168457\n",
      "0.2760535384528339\n",
      "0.05934705529361964\n",
      "0.32661400325596335\n",
      "0.8307309627532957\n",
      "0.14411867819726465\n",
      "0.19851783984340726\n",
      "0.8902226507663727\n",
      "0.39917414039373394\n",
      "0.7917660742998123\n",
      "0.31209481172263626\n",
      "0.0991291606798768\n",
      "0.478365209698677\n",
      "0.3929202955216169\n",
      "0.5405410617589951\n",
      "0.47551416642963884\n",
      "0.6799171775579453\n",
      "0.5109323278069496\n",
      "0.5528353255242109\n",
      "0.6183314085006715\n",
      "0.5381369546055794\n",
      "0.5436123564839362\n",
      "0.5542883336544037\n",
      "0.6238683298230171\n",
      "0.3838194578886032\n",
      "0.10910256351344287\n",
      "0.4959893148392439\n",
      "0.8199885249137879\n",
      "0.8269547283649445\n",
      "0.8927622437477112\n",
      "0.38509465977549556\n",
      "0.5400749683380127\n",
      "0.19778071902692324\n",
      "0.42506437972188\n",
      "0.34442485980689524\n",
      "0.49337338879704473\n",
      "0.7925003111362457\n",
      "0.18951545879244802\n",
      "0.0411776616703719\n",
      "0.15442320108413699\n",
      "0.3575526103377342\n",
      "0.1314822055399418\n",
      "0.8933712065219879\n",
      "0.05858386224135756\n",
      "0.049622469674795874\n",
      "0.3272832602262497\n",
      "0.8052392542362212\n",
      "0.6644689828157425\n",
      "0.12371464911848308\n",
      "0.8795344412326812\n",
      "0.409767946600914\n",
      "0.5072368443012237\n",
      "0.2187560938298702\n",
      "0.6057377055287362\n",
      "0.7265076071023941\n",
      "0.3757238209247589\n",
      "0.3134809616953134\n",
      "0.09120557038113475\n",
      "0.3975859608501196\n",
      "0.12109497950877994\n",
      "0.1537040112540126\n",
      "0.07703106566332281\n",
      "0.6694359123706817\n",
      "0.085983151383698\n",
      "0.4490651018917561\n",
      "0.6005811870098114\n",
      "0.06250261040404438\n",
      "0.6446566000580788\n",
      "0.04658604748547077\n",
      "0.07203957065939905\n",
      "0.10475485967472194\n",
      "0.7842707842588423\n",
      "0.06646093772724271\n",
      "0.12305608573369682\n",
      "0.3847070515155792\n",
      "0.18682003244757653\n",
      "0.6168466061353683\n",
      "0.36630098819732665\n",
      "0.579232719540596\n",
      "0.09645719341933727\n",
      "0.400946063734591\n",
      "0.3412804361432791\n",
      "0.05578603921458126\n",
      "0.2421279892325401\n",
      "0.8957579255104063\n",
      "0.6441666424274444\n",
      "0.23772605247795578\n",
      "0.49254993498325345\n",
      "0.8717757523059845\n",
      "0.3463266219943762\n",
      "0.17250689333304764\n",
      "0.7444276928901672\n",
      "0.5143138483166695\n",
      "0.07225032816641032\n",
      "0.06629981291480362\n",
      "0.16600712798535822\n",
      "0.499627199023962\n",
      "0.8869602501392365\n",
      "0.2387940961867571\n",
      "0.10006076861172913\n",
      "0.0406186958309263\n",
      "0.5106016863137484\n",
      "0.7616246938705444\n",
      "0.2943349055945873\n",
      "0.32040491979569197\n",
      "0.5815348096191882\n",
      "0.0802931976504624\n",
      "0.2654284914955497\n",
      "0.9289270102977754\n",
      "0.8056513011455536\n",
      "0.10520997811108829\n",
      "0.23385330587625505\n",
      "0.7011329472064972\n",
      "0.5313138127326965\n",
      "0.5807611674070358\n",
      "0.38092106655240066\n",
      "0.5004986017942429\n",
      "0.5280926644802093\n",
      "0.36124364212155335\n",
      "0.07985736355185509\n",
      "0.3164381794631481\n",
      "0.6966520845890045\n",
      "0.48908128887414937\n",
      "0.36234221458435056\n",
      "0.2522215832024812\n",
      "0.4507155142724514\n",
      "0.2067356701940298\n",
      "0.2503178089857101\n",
      "0.11598576279357076\n",
      "0.055720910197123885\n",
      "0.12591150254011155\n",
      "0.6933167114853859\n",
      "0.27166913263499737\n",
      "0.32802927196025855\n",
      "0.8359367966651916\n",
      "0.6673568904399872\n",
      "0.6729858711361886\n",
      "0.9501103758811951\n",
      "0.6349987059831619\n",
      "0.6108232706785203\n",
      "0.5227357387542724\n",
      "0.07571977102197706\n",
      "0.7336457103490829\n",
      "0.8559082329273223\n",
      "0.23396777799353005\n",
      "0.4369385093450546\n",
      "0.3803846318274737\n",
      "0.7035624086856841\n",
      "0.6732757285237312\n",
      "0.05579605717211962\n",
      "0.29872834235429757\n",
      "0.22371160648763178\n",
      "0.3284567013382912\n",
      "0.4808896295726299\n",
      "0.19330150093883278\n",
      "0.8293951153755189\n",
      "0.8292329490184783\n",
      "0.5393168598413468\n",
      "0.5032933548092842\n",
      "0.17351672574877738\n",
      "0.8290615648031235\n",
      "0.2155067542567849\n",
      "0.031589122978039084\n",
      "0.5702747583389283\n",
      "0.2725257247686386\n",
      "0.2765464168041944\n",
      "0.3575412049889565\n",
      "0.47578560486435895\n",
      "0.13213184410706164\n",
      "0.09624146134592594\n",
      "0.879053008556366\n",
      "0.7812153816223145\n",
      "0.8922267973423005\n",
      "0.5916988611221314\n",
      "0.6408525317907333\n",
      "0.405049704015255\n",
      "0.12290406776592135\n",
      "0.3307763092219829\n",
      "0.34214685801416633\n",
      "0.5620692133903502\n",
      "0.7070497035980224\n",
      "0.32726962119340897\n",
      "0.5593054711818696\n",
      "0.27108746021986013\n",
      "0.2718558065593243\n",
      "0.39405674487352377\n",
      "0.8575075924396516\n",
      "0.3185990065336227\n",
      "0.442827171832323\n",
      "0.14916970515623687\n",
      "0.513919699192047\n",
      "0.17352429367601874\n",
      "0.5155181601643563\n",
      "0.4395357511937618\n",
      "0.6321388766169548\n",
      "0.45029048025608065\n",
      "0.39178921580314635\n",
      "0.14460751209408043\n",
      "0.09124523056671024\n",
      "0.07240113667212426\n",
      "0.36281822547316556\n",
      "0.41725761294364927\n",
      "0.13065508138388396\n",
      "0.5246824845671654\n",
      "0.3838897615671158\n",
      "0.5726945206522941\n",
      "0.6028581172227859\n",
      "0.2402271625585854\n",
      "0.9349953532218932\n",
      "0.4665226023644209\n",
      "0.22152126748114823\n",
      "0.75282883644104\n",
      "0.37392802014946935\n",
      "0.5776975981891156\n",
      "0.7861907109618186\n",
      "0.6976238965988159\n",
      "0.7718205511569977\n",
      "0.600494846701622\n",
      "0.7634986817836761\n",
      "0.4665634542703629\n",
      "0.8432812780141831\n",
      "0.585288706421852\n",
      "0.8704347074031832\n",
      "0.12009814269840718\n",
      "0.3605485945940018\n",
      "0.3098256461322308\n",
      "0.056948393722996114\n",
      "0.8909890949726105\n",
      "0.11878183465451002\n",
      "0.11506350305862725\n",
      "0.7434290871024132\n",
      "0.8361142158508301\n",
      "0.6856674581766128\n",
      "0.41785503923892975\n",
      "0.7831943035125732\n",
      "0.05927236741408705\n",
      "0.16940482705831528\n",
      "0.5399482458829878\n",
      "0.7263549283146858\n",
      "0.8415669977664948\n",
      "0.6783239483833313\n",
      "0.16446388252079489\n",
      "0.8299651443958282\n",
      "0.6989342629909514\n",
      "0.6530588418245317\n",
      "0.6173402458429337\n",
      "0.9129770815372467\n",
      "0.7636316061019898\n",
      "0.42255291417241103\n",
      "0.4960508964955807\n",
      "0.14589762985706325\n",
      "0.39183387756347654\n",
      "0.6360271275043488\n",
      "0.4369610629975796\n",
      "0.9458562612533569\n",
      "0.49214612208306785\n",
      "0.6671470016241073\n",
      "0.1461079116910696\n",
      "0.5674916267395019\n",
      "0.05733742564916611\n",
      "0.07018621461465954\n",
      "0.768510812520981\n",
      "0.19571553617715837\n",
      "0.8199183493852615\n",
      "0.8477104425430299\n",
      "0.29005865231156347\n",
      "0.7355853170156479\n",
      "0.07048431783914566\n",
      "0.3703205555677414\n",
      "0.714210832118988\n",
      "0.52275600284338\n",
      "0.9252638697624205\n",
      "0.7918984919786453\n",
      "0.23814467974007134\n",
      "0.6547225296497345\n",
      "0.5825542360544205\n",
      "0.6928952455520629\n",
      "0.9167065441608429\n",
      "0.2982650574296713\n",
      "0.07292362763546409\n",
      "0.2001111499965191\n",
      "0.3718525290489197\n",
      "0.4654864422976971\n",
      "0.09595442349091172\n",
      "0.6999761164188385\n",
      "0.0886700976639986\n",
      "0.07050977726466953\n",
      "0.38650830686092374\n",
      "0.779721599817276\n",
      "0.09951321515254676\n",
      "0.24409418255090715\n",
      "0.8504893779754638\n",
      "0.5501635178923607\n",
      "0.8641741752624512\n",
      "0.1834807202219963\n",
      "0.15801426209509373\n",
      "0.9257554888725282\n",
      "0.05757892201654614\n",
      "0.20524097345769404\n",
      "0.6708985418081284\n",
      "0.6296022415161133\n",
      "0.4035018406808376\n",
      "0.8724782049655915\n",
      "0.1098039602395147\n",
      "0.8554312765598296\n",
      "0.7642256855964661\n",
      "0.048261234723031524\n",
      "0.6312246799468995\n",
      "0.6684799879789352\n",
      "0.6498329788446426\n",
      "0.5325554668903352\n",
      "0.7395856142044066\n",
      "0.2736290689557791\n",
      "0.6074768245220185\n",
      "0.16978523582220076\n",
      "0.3152710985392332\n",
      "0.6644945472478867\n",
      "0.43207165896892546\n",
      "0.06296240454539657\n",
      "0.49529593586921683\n",
      "0.024046449945308267\n",
      "0.3902319204062223\n",
      "0.10296016270294785\n",
      "0.12768714725971222\n",
      "0.15147564159706234\n",
      "0.6495633326470852\n",
      "0.19096034169197082\n",
      "0.3412052854895592\n",
      "0.9404114902019501\n",
      "0.6453581631183625\n",
      "0.5348883852362633\n",
      "0.7506271049380302\n",
      "0.13141302121803164\n",
      "0.619540411233902\n",
      "0.5002095341682434\n",
      "0.6937655299901963\n",
      "0.515571927651763\n",
      "0.1837333807721734\n",
      "0.8603048741817475\n",
      "0.273341042175889\n",
      "0.3239953577518463\n",
      "0.7527627170085908\n",
      "0.5146506890654564\n",
      "0.8249600231647491\n",
      "0.80082368850708\n",
      "0.05862299902364612\n",
      "0.36010893275961275\n",
      "0.05480991541408002\n",
      "0.7970746636390685\n",
      "0.13436922915279864\n",
      "0.7156018584966659\n",
      "0.6095933198928832\n",
      "0.05612542261369526\n",
      "0.8099062621593476\n",
      "0.09312310535460712\n",
      "0.2385581474751234\n",
      "0.050147550087422124\n",
      "0.514885775744915\n",
      "0.2954253852367401\n",
      "0.8268137156963349\n",
      "0.6467753246426582\n",
      "0.7812076270580292\n",
      "0.07990032820962369\n",
      "0.5654501736164094\n",
      "0.4821088820695877\n",
      "0.7768904209136963\n",
      "0.4138084009289742\n",
      "0.11275055478326974\n",
      "0.031144054792821407\n",
      "0.6617460504174233\n",
      "0.8067849591374397\n",
      "0.1571959052234888\n",
      "0.28733047042042015\n",
      "0.4318281468003988\n",
      "0.2760281290858984\n",
      "0.928859519958496\n",
      "0.6443980544805528\n",
      "0.5086810819804668\n",
      "0.8046079337596893\n",
      "0.07482463186606765\n",
      "0.03426624252460897\n",
      "0.04985497170127928\n",
      "0.5337110817432403\n",
      "0.7938876241445542\n",
      "0.3911248235031962\n",
      "0.6620108604431152\n",
      "0.3917412281036377\n",
      "0.8995174169540405\n",
      "0.41144829597324134\n",
      "0.6763035282492639\n",
      "0.6198613464832305\n",
      "0.6437899976968765\n",
      "0.7380304485559465\n",
      "0.21420491896569727\n",
      "0.8879151999950409\n",
      "0.09877821793779731\n",
      "0.9259166181087494\n",
      "0.5815058752894402\n",
      "0.07819634545594453\n",
      "0.7758775293827057\n",
      "0.6092899918556214\n",
      "0.27025920972228046\n",
      "0.8513592064380646\n",
      "0.42552202790975574\n",
      "0.3188403122127056\n",
      "0.1276705779135227\n",
      "0.8964111387729644\n",
      "0.30123236775398254\n",
      "0.46642964035272594\n",
      "0.45389648079872136\n",
      "0.2801101941615343\n",
      "0.29834018386900424\n",
      "0.23761038798838852\n",
      "0.8304193973541261\n",
      "0.5142421498894691\n",
      "0.38191893137991434\n",
      "0.7157164871692657\n",
      "0.26673231068998576\n",
      "0.5812696702778339\n",
      "0.5491598367691041\n",
      "0.431595953553915\n",
      "0.12059857384301721\n",
      "0.4822205737233161\n",
      "0.9094184994697571\n",
      "0.851464119553566\n",
      "0.7619470447301865\n",
      "0.14336101412773133\n",
      "0.29270752817392354\n",
      "0.8752795577049256\n",
      "0.2290731368586421\n",
      "0.9283847272396087\n",
      "0.2616980500519276\n",
      "0.06616828395053745\n",
      "0.15773226851597427\n",
      "0.5022384360432625\n",
      "0.5467218965291977\n",
      "0.3186488967388868\n",
      "0.7465321987867355\n",
      "0.13015731745399536\n",
      "0.8144330680370331\n",
      "0.1728081416338682\n",
      "0.8929552912712095\n",
      "0.7661754727363587\n",
      "0.7029217012226581\n",
      "0.25459897741675375\n",
      "0.6637972086668014\n",
      "0.5374105870723724\n",
      "0.47336978018283843\n",
      "0.38105180077254774\n",
      "0.3447027519345284\n",
      "0.13827498238533736\n",
      "0.07785033611580729\n",
      "0.09328249823302032\n",
      "0.1263323998078704\n",
      "0.7992873609066009\n",
      "0.5979731321334839\n",
      "0.6770986169576645\n",
      "0.3122007805854082\n",
      "0.04852459826506675\n",
      "0.6860575854778289\n",
      "0.26225842600688337\n",
      "0.11082660313695669\n",
      "0.801821756362915\n",
      "0.740377649664879\n",
      "0.06716667849104853\n",
      "0.6249202415347099\n",
      "0.24318751357495788\n",
      "0.7376169577240944\n",
      "0.17836504951119422\n",
      "0.10358830909244716\n",
      "0.1736884202808142\n",
      "0.18020107056945564\n",
      "0.23505735453218218\n",
      "0.23769808765500783\n",
      "0.2852432031184435\n",
      "0.8740011513233186\n",
      "0.8638104587793352\n",
      "0.18194731725379823\n",
      "0.12410836769267916\n",
      "0.3565823692828417\n",
      "0.7418655484914779\n",
      "0.5148973602801562\n",
      "0.536479888856411\n",
      "0.08242499786429107\n",
      "0.6977999418973924\n",
      "0.22552317529916763\n",
      "0.788396281003952\n",
      "0.6972156137228012\n",
      "0.8412361562252044\n",
      "0.4460921958088875\n",
      "0.8129805624485016\n",
      "0.12230725549161435\n",
      "0.0657884558197111\n",
      "0.10840353136882187\n",
      "0.7849416851997375\n",
      "0.13361685797572134\n",
      "0.6255729898810387\n",
      "0.06919350903481246\n",
      "0.12509598694741728\n",
      "0.7537087291479112\n",
      "0.6967325747013092\n",
      "0.4318410664796829\n",
      "0.20633546188473703\n",
      "0.3236234494484961\n",
      "0.29525799453258517\n",
      "0.05597636932507158\n",
      "0.18977267555892466\n",
      "0.48359228707849977\n",
      "0.3836556002497673\n",
      "0.2973430559039115\n",
      "0.22623141482472423\n",
      "0.10040569417178631\n",
      "0.5796677261590957\n",
      "0.6075205534696578\n",
      "0.29888426549732683\n",
      "0.3385605677962303\n",
      "0.06173792784102262\n",
      "0.5915316954255103\n",
      "0.7778231620788574\n",
      "0.07364669237285854\n",
      "0.19262731466442345\n",
      "0.5945716917514801\n",
      "0.410718796402216\n",
      "0.6424078226089478\n",
      "0.9025557994842529\n",
      "0.760341364145279\n",
      "0.30536591447889805\n",
      "0.9415464818477631\n",
      "0.05064516263082624\n",
      "0.053460118710063405\n",
      "0.6556213825941085\n",
      "0.21145203597843648\n",
      "0.1521405665203929\n",
      "0.9458200514316558\n",
      "0.04444273207336665\n",
      "0.1831215091049671\n",
      "0.6426919519901275\n",
      "0.5641171008348465\n",
      "0.06286532916128634\n",
      "0.18524814657866953\n",
      "0.5565076187252999\n",
      "0.07947418158873915\n",
      "0.4056373424828053\n",
      "0.0899068790487945\n",
      "0.053745331196114424\n",
      "0.3300892538391053\n",
      "0.656707976013422\n",
      "0.871860909461975\n",
      "0.41833768934011456\n",
      "0.08318571466952564\n",
      "0.1772402752190828\n",
      "0.3127170577645302\n",
      "0.46533100754022605\n",
      "0.20787851139903069\n",
      "0.32808242179453373\n",
      "0.5864299416542053\n",
      "0.7693000942468645\n",
      "0.05955840200185776\n",
      "0.2153171407058835\n",
      "0.4712773974984884\n",
      "0.6256453484296798\n",
      "0.5187682449817657\n",
      "0.94909570813179\n",
      "0.07902717082761228\n",
      "0.3486154740676284\n",
      "0.7927944004535674\n",
      "0.16694872127845886\n",
      "0.39731760919094083\n",
      "0.09940223833546043\n",
      "0.5271075598895549\n",
      "0.1388508504256606\n",
      "0.18991839326918125\n",
      "0.26703710593283175\n",
      "0.2801315996795892\n",
      "0.7479057937860488\n",
      "0.7108531832695008\n",
      "0.24702082592993974\n",
      "0.6257985830307007\n",
      "0.3004719004034996\n",
      "0.9221147119998933\n",
      "0.5590953558683396\n",
      "0.47834173738956454\n",
      "0.7938064873218538\n",
      "0.711031174659729\n",
      "0.10204310859553516\n",
      "0.9503625214099883\n",
      "0.19741243906319142\n",
      "0.357377166673541\n",
      "0.5625451087951661\n",
      "0.7833447128534317\n",
      "0.5929354183375836\n",
      "0.3173953663557768\n",
      "0.2947513110935687\n",
      "0.5096377275884152\n",
      "0.40477575063705445\n",
      "0.18388122497126455\n",
      "0.6136904060840607\n",
      "0.28683463484048843\n",
      "0.23292994080111387\n",
      "0.38172980695962905\n",
      "0.22140665426850317\n",
      "0.5163580715656281\n",
      "0.40892407521605495\n",
      "0.4359739862382412\n",
      "0.16537910141050818\n",
      "0.49055248331278567\n",
      "0.8869095861911774\n",
      "0.7763288766145705\n",
      "0.5438660308718681\n",
      "0.03793488750234247\n",
      "0.222518315911293\n",
      "0.5897683387622238\n",
      "0.5938084736466407\n",
      "0.35270599424839016\n",
      "0.11218714704737066\n",
      "0.5675437778234482\n",
      "0.5426502168178559\n",
      "0.040193789964541794\n",
      "0.8748310923576356\n",
      "0.8219266444444655\n",
      "0.17689676266163587\n",
      "0.0923777736723423\n",
      "0.602606450021267\n",
      "0.6586801812052727\n",
      "0.643394273519516\n",
      "0.8429745495319365\n",
      "0.920161634683609\n",
      "0.27251722337678075\n",
      "0.57507002055645\n",
      "0.2877850033342838\n",
      "0.13140251738950612\n",
      "0.47535267211496834\n",
      "0.029510498186573385\n",
      "0.1872782714664936\n",
      "0.7722253262996674\n",
      "0.4598271962255239\n",
      "0.2928320780396461\n",
      "0.8966520428657532\n",
      "0.5206730701029301\n",
      "0.04391098311170936\n",
      "0.518541120737791\n",
      "0.43841292932629583\n",
      "0.4188757717609406\n",
      "0.08232660545036195\n",
      "0.15568992495536804\n",
      "0.11056184144690634\n",
      "0.8334457159042358\n",
      "0.5827835783362387\n",
      "0.7849116683006286\n",
      "0.14339048829860984\n",
      "0.3502788573503494\n",
      "0.11138592809438706\n",
      "0.19593708012253047\n",
      "0.1878233740106225\n",
      "0.28883332647383214\n",
      "0.5537840366363525\n",
      "0.5317337438464165\n",
      "0.31551672145724297\n",
      "0.46924398243427273\n",
      "0.9433143317699431\n",
      "0.06492975223809481\n",
      "0.49248865395784386\n",
      "0.22641777396202084\n",
      "0.8317945480346679\n",
      "0.1155925214290619\n",
      "0.14409639388322829\n",
      "0.1334757507313043\n",
      "0.17094901253003625\n",
      "0.15232112705707548\n",
      "0.6680197149515152\n",
      "0.43906457126140597\n",
      "0.6799009755253793\n",
      "0.6555555760860443\n",
      "0.098839378869161\n",
      "0.09181624474003912\n",
      "0.06290911436080933\n",
      "0.6430692628026009\n",
      "0.5804867990314961\n",
      "0.9031669914722443\n",
      "0.8038178741931916\n",
      "0.38835527822375293\n",
      "0.1328692057169974\n",
      "0.4845299631357194\n",
      "0.38895176351070404\n",
      "0.09216885697096587\n",
      "0.4194177392870188\n",
      "0.6220029458403588\n",
      "0.5037183925509453\n",
      "0.7330904752016068\n",
      "0.6282667636871339\n",
      "0.2659009799361229\n",
      "0.23747305721044543\n",
      "0.5722395449876786\n",
      "0.7425559103488923\n",
      "0.4663590297102928\n",
      "0.5396510049700737\n",
      "0.8134190380573274\n",
      "0.4834638088941574\n",
      "0.6359004020690918\n",
      "0.7839956521987915\n",
      "0.24353658296167852\n",
      "0.8288405239582062\n",
      "0.18818936608731743\n",
      "0.6384218007326126\n",
      "0.8144948899745942\n",
      "0.6808129295706749\n",
      "0.6980834692716599\n",
      "0.33885250296443703\n",
      "0.41139789223670964\n",
      "0.42183553874492646\n",
      "0.16039553619921207\n",
      "0.39854621738195417\n",
      "0.7603394202888012\n",
      "0.2989017598330974\n",
      "0.6699739217758178\n",
      "0.43056408166885374\n",
      "0.253150530345738\n",
      "0.0620006491895765\n",
      "0.6567267969250679\n",
      "0.11292036483064294\n",
      "0.06790674948133528\n",
      "0.3180144503712654\n",
      "0.8749509155750275\n",
      "0.8865030050277709\n",
      "0.46794208288192746\n",
      "0.5339308068156242\n",
      "0.9327181696891784\n",
      "0.254272916726768\n",
      "0.15297302547842262\n",
      "0.5593482904136181\n",
      "0.5641375049948691\n",
      "0.8652115643024445\n",
      "0.16194413490593434\n",
      "0.08455806048586964\n",
      "0.17452428610995413\n",
      "0.9326067686080931\n",
      "0.040450326609425255\n",
      "0.08239412358961999\n",
      "0.4296249330043792\n",
      "0.044048557383939625\n",
      "0.2623481348156929\n",
      "0.6776036381721497\n",
      "0.05464928606525063\n",
      "0.8373997867107392\n",
      "0.7692521631717683\n",
      "0.570947390794754\n",
      "0.4499005157500505\n",
      "0.27801588661968707\n",
      "0.42693795338273055\n",
      "0.29103348366916176\n",
      "0.05296231701504439\n",
      "0.6496872693300246\n",
      "0.8180619955062868\n",
      "0.8749850213527679\n",
      "0.12810249058529735\n",
      "0.3516068294644356\n",
      "0.5023772403597832\n",
      "0.4697327204048633\n",
      "0.25156327029690145\n",
      "0.5607044458389282\n",
      "0.7422238737344741\n",
      "0.29712727181613446\n",
      "0.47969347536563867\n",
      "0.4925933480262757\n",
      "0.10988511922769248\n",
      "0.5551536679267883\n",
      "0.7915179610252382\n",
      "0.2465717136859894\n",
      "0.07414593931753187\n",
      "0.07382598589174451\n",
      "0.6924312323331834\n",
      "0.09286862099543214\n",
      "0.9221464514732362\n",
      "0.44585548639297484\n",
      "0.3366944015026092\n",
      "0.42538167200982574\n",
      "0.05359819484874606\n",
      "0.7315495535731315\n",
      "0.09988684058189393\n",
      "0.11787487510591746\n",
      "0.7965006291866301\n",
      "0.1943113088607788\n",
      "0.7402112036943436\n",
      "0.774399960041046\n",
      "0.8283850610256195\n",
      "0.9551187872886658\n",
      "0.15178555361926555\n",
      "0.23559819906949997\n",
      "0.6434785544872283\n",
      "0.21046697720885277\n",
      "0.13401954993605614\n",
      "0.8475775837898255\n",
      "0.34557722453027967\n",
      "0.8181271910667419\n",
      "0.32171269003301867\n",
      "0.1944965595379472\n",
      "0.8204646140336991\n",
      "0.6195488661527633\n",
      "0.6287819534540176\n",
      "0.7667862296104431\n",
      "0.7640034765005111\n",
      "0.13015806935727597\n",
      "0.2331976175308228\n",
      "0.20307880630716682\n",
      "0.3876471646130085\n",
      "0.6881460472941399\n",
      "0.5871517837047577\n",
      "0.9375742197036743\n",
      "0.48961046859622004\n",
      "0.41134429946541784\n",
      "0.03520311242900788\n",
      "0.16365556716918944\n",
      "0.7990429043769836\n",
      "0.18984709782525896\n",
      "0.4461811674758792\n",
      "0.1783062849193811\n",
      "0.3213336996734142\n",
      "0.35387710863724353\n",
      "0.18635996989905834\n",
      "0.4996806830167771\n",
      "0.6999304920434951\n",
      "0.13282541707158088\n",
      "0.513726606965065\n",
      "0.9109164834022522\n",
      "0.40651867538690567\n",
      "0.32447656281292436\n",
      "0.78041070997715\n",
      "0.0370105710811913\n",
      "0.7022093892097473\n",
      "0.2776324011385441\n",
      "0.18660835213959218\n",
      "0.7542766571044922\n",
      "0.7114671111106873\n",
      "0.21503096036612987\n",
      "0.08706914400681852\n",
      "0.8341606199741364\n",
      "0.18652001470327376\n",
      "0.7441048748791219\n",
      "0.056547396909445526\n",
      "0.08310548919253051\n",
      "0.09298720508813858\n",
      "0.7836094737052918\n",
      "0.20568960569798947\n",
      "0.618761184811592\n",
      "0.6936230182647705\n",
      "0.5355880931019783\n",
      "0.11730934483930469\n",
      "0.7356117844581604\n",
      "0.05953018893487751\n",
      "0.21125650350004432\n",
      "0.39072119891643525\n",
      "0.36291676834225656\n",
      "0.5019331157207488\n",
      "0.043766168877482414\n",
      "0.06491040904074907\n",
      "0.37842517122626307\n",
      "0.656253570318222\n",
      "0.16359544349834323\n",
      "0.3301838222891092\n",
      "0.2591310262680054\n",
      "0.3583866372704506\n",
      "0.6372892715036869\n",
      "0.2937890026718378\n",
      "0.6788015209138394\n",
      "0.5957308650016784\n",
      "0.15105918888002634\n",
      "0.21419525695964692\n",
      "0.6668244093656539\n",
      "0.174644457641989\n",
      "0.7000828385353088\n",
      "0.30468781962990754\n",
      "0.03732807566411793\n",
      "0.4989853888750076\n",
      "0.8896379172801971\n",
      "0.30834148153662677\n",
      "0.06448092120699585\n",
      "0.1548447147011757\n",
      "0.2291204012930393\n",
      "0.032949624885804954\n",
      "0.7627981096506118\n",
      "0.4035011723637581\n",
      "0.18583604879677298\n",
      "0.5753893233835697\n",
      "0.5378529015928507\n",
      "0.769257351756096\n",
      "0.772925552725792\n",
      "0.5706404328346253\n",
      "0.1234070533886552\n",
      "0.3866294596344233\n",
      "0.2917912095785141\n",
      "0.5431014418601989\n",
      "0.6913758486509324\n",
      "0.7384050995111466\n",
      "0.7765053570270539\n",
      "0.7910897761583328\n",
      "0.38372918367385866\n",
      "0.43966808095574383\n",
      "0.8467029213905335\n",
      "0.927549433708191\n",
      "0.5551794022321701\n",
      "0.07378216814249754\n",
      "0.8519342005252838\n",
      "0.13500480093061926\n",
      "0.09014571579173208\n",
      "0.858355313539505\n",
      "0.1725233096629381\n",
      "0.04299579404760152\n",
      "0.9482306003570556\n",
      "0.05833784965798259\n",
      "0.06626673596911134\n",
      "0.8623005270957945\n",
      "0.6533073484897614\n",
      "0.5298470929265022\n",
      "0.06478290036320687\n",
      "0.521639223396778\n",
      "0.5071409538388252\n",
      "0.4212249636650085\n",
      "0.6409121379256248\n",
      "0.04657600917853415\n",
      "0.2727001806721091\n",
      "0.39316361024975777\n",
      "0.7140888035297394\n",
      "0.26223100796341897\n",
      "0.07934573506936429\n",
      "0.053387247514911\n",
      "0.42868983894586565\n",
      "0.2746563540771604\n",
      "0.25486742369830606\n",
      "0.7008039325475692\n",
      "0.27073304504156115\n",
      "0.8199788212776183\n",
      "0.8176323413848877\n",
      "0.9096106350421906\n",
      "0.20191415715962646\n",
      "0.10365631366148591\n",
      "0.629545672237873\n",
      "0.4252649396657944\n",
      "0.5569792747497558\n",
      "0.8976270258426667\n",
      "0.423669983446598\n",
      "0.09649681197479366\n",
      "0.48821848183870314\n",
      "0.8648919343948365\n",
      "0.10421141618862748\n",
      "0.43596111312508584\n",
      "0.08411243229638787\n",
      "0.21935282647609713\n",
      "0.41674078255891794\n",
      "0.0928406462073326\n",
      "0.4277103319764137\n",
      "0.7971178948879243\n",
      "0.24565739817917345\n",
      "0.27095557553693655\n",
      "0.1276803639251739\n",
      "0.4628062702715397\n",
      "0.0911277567502111\n",
      "0.5295597776770592\n",
      "0.8373341977596283\n",
      "0.39074371792376045\n",
      "0.05505572310648858\n",
      "0.8587416529655456\n",
      "0.18102576285600663\n",
      "0.08437949307262897\n",
      "0.23081844132393597\n",
      "0.052601793827489016\n",
      "0.4276541516184807\n",
      "0.5252220764756202\n",
      "0.28635613732039933\n",
      "0.6200322180986404\n",
      "0.35928624719381336\n",
      "0.4380573824048042\n",
      "0.4229019537568092\n",
      "0.12186766667291522\n",
      "0.414649248868227\n",
      "0.23874415904283525\n",
      "0.6472091883420944\n",
      "0.07612222661264242\n",
      "0.7435437023639679\n",
      "0.09746469692327082\n",
      "0.3985434427857399\n",
      "0.2661025196313858\n",
      "0.38576663136482237\n",
      "0.6274830162525177\n",
      "0.6969182454049586\n",
      "0.08729243371635675\n",
      "0.8058386266231536\n",
      "0.7576044529676438\n",
      "0.04511147825978696\n",
      "0.5430787794291974\n",
      "0.07237142417579889\n",
      "0.26983786169439555\n",
      "0.09431901006028055\n",
      "0.7035877585411072\n",
      "0.058293766877613964\n",
      "0.16020668866112828\n",
      "0.28719182461500165\n",
      "0.7390527188777923\n",
      "0.6469219312071801\n",
      "0.30634678504429763\n",
      "0.18796959295868879\n",
      "0.2566412828862667\n",
      "0.1715229231864214\n",
      "0.6085599526762963\n",
      "0.11331455186009406\n",
      "0.5852388352155686\n",
      "0.07931463811546566\n",
      "0.22713880334049466\n",
      "0.11368074398487807\n",
      "0.6998020589351653\n",
      "0.0629964656662196\n",
      "0.10919720651581882\n",
      "0.0472904535010457\n",
      "0.19930663006380203\n",
      "0.22310500778257847\n",
      "0.6991501063108445\n",
      "0.7752389460802078\n",
      "0.18608631510287524\n",
      "0.36811328679323196\n",
      "0.16057454189285633\n",
      "0.31628256030380725\n",
      "0.6508595228195191\n",
      "0.4200798358768224\n",
      "0.28595632091164586\n",
      "0.3165272101759911\n",
      "0.577910865843296\n",
      "0.8651362121105195\n",
      "0.09276891853660346\n",
      "0.1636158123612404\n",
      "0.06617753398604691\n",
      "0.5906429916620255\n",
      "0.13885914199054242\n",
      "0.5154076989740133\n",
      "0.6532901495695114\n",
      "0.40625176914036265\n",
      "0.07172596636228262\n",
      "0.5661883771419526\n",
      "0.3435270408168435\n",
      "0.6172529544681311\n",
      "0.07214349145069719\n",
      "0.5439917773008347\n",
      "0.9239614546298981\n",
      "0.5365305975079536\n",
      "0.3728833004832267\n",
      "0.448955100774765\n",
      "0.03272448319476098\n",
      "0.5659657388925553\n",
      "0.71095412671566\n",
      "0.7885688900947571\n",
      "0.2967986490577459\n",
      "0.5058578666299581\n",
      "0.7763106763362885\n",
      "0.18780794125050304\n",
      "0.5873405233025552\n",
      "0.0866834294050932\n",
      "0.5717967003583908\n",
      "0.045135610713623465\n",
      "0.4290055431425571\n",
      "0.5933455154299735\n",
      "0.06065006963908673\n",
      "0.812984722852707\n",
      "0.6494204074144364\n",
      "0.30439547747373574\n",
      "0.03329391323495656\n",
      "0.9327364206314087\n",
      "0.20946294805034996\n",
      "0.13748604762367905\n",
      "0.21685374677181243\n",
      "0.1958502884954214\n",
      "0.09120323511306196\n",
      "0.15355356251820923\n",
      "0.22804146520793436\n",
      "0.7896887123584747\n",
      "0.7211169838905335\n",
      "0.31804679669439795\n",
      "0.8534582912921906\n",
      "0.14807213312014939\n",
      "0.8940606534481049\n",
      "0.9215465128421784\n",
      "0.06563631547614933\n",
      "0.30606200098991393\n",
      "0.3291346557438373\n",
      "0.0844394971150905\n",
      "0.16407151371240616\n",
      "0.6030084036290645\n",
      "0.15142168533056977\n",
      "0.39905713871121407\n",
      "0.5679075002670287\n",
      "0.6083207607269288\n",
      "0.9201644212007521\n",
      "0.20432037897408006\n",
      "0.2383894383907318\n",
      "0.6200080081820488\n",
      "0.6239722222089767\n",
      "0.08139937287196516\n",
      "0.025848708022385838\n",
      "0.4825686771422625\n",
      "0.7000197410583497\n",
      "0.6794938147068025\n",
      "0.177793803345412\n",
      "0.11032937956042589\n",
      "0.4944174483418464\n",
      "0.16327207107096908\n",
      "0.4399458080530167\n",
      "0.02949148612096906\n",
      "0.7132691621780396\n",
      "0.5512499086558819\n",
      "0.4390373632311821\n",
      "0.6604460410773754\n",
      "0.5180908668786287\n",
      "0.6777983009815216\n",
      "0.7026560842990874\n",
      "0.3566435724496841\n",
      "0.5583717972040176\n",
      "0.8091779053211212\n",
      "0.6788221269845963\n",
      "0.07592500573955477\n",
      "0.6557953208684921\n",
      "0.7278459370136261\n",
      "0.36127305403351784\n",
      "0.3438515797257424\n",
      "0.6329839706420899\n",
      "0.2634348748251796\n",
      "0.8341640949249267\n",
      "0.15554500389844178\n",
      "0.86348095536232\n",
      "0.18609233032912015\n",
      "0.7457123994827272\n",
      "0.22831972204148768\n",
      "0.28264510678127414\n",
      "0.11155300056561829\n",
      "0.9462563991546631\n",
      "0.047890983661636716\n",
      "0.8902679383754731\n",
      "0.12660324862226843\n",
      "0.06890560523606837\n",
      "0.1177295302040875\n",
      "0.26424455028027294\n",
      "0.17597495410591366\n",
      "0.6612815558910371\n",
      "0.3862578868865967\n",
      "0.7085068076848985\n",
      "0.16153104733675722\n",
      "0.8017279416322708\n",
      "0.5085173919796944\n",
      "0.07093904428184032\n",
      "0.06627857210114599\n",
      "0.04174397573806346\n",
      "0.1312663488090038\n",
      "0.13790665036067368\n",
      "0.6360504865646361\n",
      "0.10454440424218772\n",
      "0.565785776078701\n",
      "0.049631897639483225\n",
      "0.5558268621563911\n",
      "0.40897168070077894\n",
      "0.11213424857705831\n",
      "0.8627058207988739\n",
      "0.2958956904709339\n",
      "0.29429445397108794\n",
      "0.5319550439715385\n",
      "0.7683139741420745\n",
      "0.17125458642840385\n",
      "0.23371247202157977\n",
      "0.22620937693864107\n",
      "0.5098410964012146\n",
      "0.16405959017574787\n",
      "0.2258571658283472\n",
      "0.1919281730428338\n",
      "0.18902628682553768\n",
      "0.13691384084522723\n",
      "0.6425975456833839\n",
      "0.7971616744995118\n",
      "0.6445761442184449\n",
      "0.29219791758805513\n",
      "0.8281708240509034\n",
      "0.05450269058346748\n",
      "0.07988496595062315\n",
      "0.23641726970672605\n",
      "0.8846885085105896\n",
      "0.10523160584270955\n",
      "0.5900675535202026\n",
      "0.31396476924419403\n",
      "0.45150390863418577\n",
      "0.057994812354445466\n",
      "0.47525787204504016\n",
      "0.2561414249241352\n",
      "0.5883121430873871\n",
      "0.8509477555751801\n",
      "0.17566189756616948\n",
      "0.10051267389208077\n",
      "0.03788309392984956\n",
      "0.13144560046494005\n",
      "0.07775758495554327\n",
      "0.8749886393547059\n",
      "0.33722097985446453\n",
      "0.9116355061531067\n",
      "0.8798258006572723\n",
      "0.5040218293666839\n",
      "0.08867300632409751\n",
      "0.19949926603585483\n",
      "0.026303270109929142\n",
      "0.47858057916164404\n",
      "0.2764674738049507\n",
      "0.8771994888782502\n",
      "0.49958472400903703\n",
      "0.19365539662539957\n",
      "0.6770744830369949\n",
      "0.25376342611853037\n",
      "0.3230311959981918\n",
      "0.9061562001705169\n",
      "0.7832386136054993\n",
      "0.4033992893993854\n",
      "0.29470406323671344\n",
      "0.06924514272250236\n",
      "0.5108734235167504\n",
      "0.434835296869278\n",
      "0.8412015497684479\n",
      "0.49710141718387596\n",
      "0.32458181902766226\n",
      "0.5794106535613537\n",
      "0.6059974253177643\n",
      "0.523326325789094\n",
      "0.15219317302107813\n",
      "0.935793912410736\n",
      "0.0419443336315453\n",
      "0.8484629869461059\n",
      "0.041193927824497226\n",
      "0.6239409558475016\n",
      "0.11179742049425842\n",
      "0.9145117342472078\n",
      "0.28231019526720047\n",
      "0.4012888390570879\n",
      "0.3160834260284901\n",
      "0.21847880650311705\n",
      "0.26035366477444766\n",
      "0.40799131393432614\n",
      "0.7039612740278244\n",
      "0.2489775006659329\n",
      "0.2809270404279232\n",
      "0.18636905513703825\n",
      "0.47598044946789747\n",
      "0.07486086953431367\n",
      "0.09343328131362798\n",
      "0.0869040708988905\n",
      "0.8905987381935119\n",
      "0.13041325225494801\n",
      "0.8443363249301911\n",
      "0.3508513603359461\n",
      "0.6206640988588333\n",
      "0.15977564677596093\n",
      "0.8477823436260223\n",
      "0.5560047879815102\n",
      "0.7801960349082947\n",
      "0.7912164807319639\n",
      "0.13117175353690982\n",
      "0.8744876265525818\n",
      "0.871342095732689\n",
      "0.2048030376434326\n",
      "0.19172805557027464\n",
      "0.05287378109060228\n",
      "0.7749396979808807\n",
      "0.64372239112854\n",
      "0.6706023573875427\n",
      "0.6383105814456939\n",
      "0.10043379622511567\n",
      "0.4877741985023022\n",
      "0.10100193219259382\n",
      "0.12031253306195142\n",
      "0.6098552525043488\n",
      "0.04448848927859217\n",
      "0.149708691239357\n",
      "0.8791668832302094\n",
      "0.8060314059257507\n",
      "0.8858541309833525\n",
      "0.1965921934694052\n",
      "0.24806194044649602\n",
      "0.35747525542974473\n",
      "0.5494567826390266\n",
      "0.7707943499088286\n",
      "0.5940521657466888\n",
      "0.8117429375648498\n",
      "0.8931745052337648\n",
      "0.2947273120284081\n",
      "0.1921895656734705\n",
      "0.12488349061459304\n",
      "0.8772608399391175\n",
      "0.07299334977287798\n",
      "0.8212808936834334\n",
      "0.137792512960732\n",
      "0.09821632192470134\n",
      "0.5431746691465378\n",
      "0.10870378015097232\n",
      "0.6217585168778896\n",
      "0.3870779467746615\n",
      "0.6467677757143975\n",
      "0.08416330460458994\n",
      "0.16059009442105893\n",
      "0.34691663850098853\n",
      "0.3857614286243915\n",
      "0.2646008390933275\n",
      "0.5529078498482705\n",
      "0.14328676457516848\n",
      "0.4243004783987999\n",
      "0.6548431813716888\n",
      "0.7001359529793262\n",
      "0.04995666877366603\n",
      "0.5388539642095566\n",
      "0.868322491645813\n",
      "0.48645433783531195\n",
      "0.6376746870577334\n",
      "0.23215949945151806\n",
      "0.4443164512515068\n",
      "0.7113044083118439\n",
      "0.20809951815754177\n",
      "0.08348726872354748\n",
      "0.2619909606873989\n",
      "0.20702858809381725\n",
      "0.5517940729856492\n",
      "0.07303447816520929\n",
      "0.057841552235186104\n",
      "0.2913894580677152\n",
      "0.3837566405534744\n",
      "0.15187353417277336\n",
      "0.5066258754581213\n",
      "0.11652874057181178\n",
      "0.3341063521802425\n",
      "0.8742141544818879\n",
      "0.606182473897934\n",
      "0.08526229788549244\n",
      "0.2048100482672453\n",
      "0.09832032211124897\n",
      "0.5077584549784661\n",
      "0.08195049017667771\n",
      "0.6286639511585236\n",
      "0.058733755769208085\n",
      "0.13677535019814968\n",
      "0.5637251518666744\n",
      "0.5749130204319954\n",
      "0.22119582742452623\n",
      "0.8937391281127931\n",
      "0.553770598769188\n",
      "0.21650146776810286\n",
      "0.33574917986989017\n",
      "0.3836594089865685\n",
      "0.043200374115258455\n",
      "0.8771408289670944\n",
      "0.2890900239348412\n",
      "0.9087192356586455\n",
      "0.7482135176658631\n",
      "0.0906408289913088\n",
      "0.061761637125164276\n",
      "0.14075191440060733\n",
      "0.2631703851744533\n",
      "0.46500130593776706\n",
      "0.5818585380911827\n",
      "0.17065371293574572\n",
      "0.09452437069267033\n",
      "0.05132287223823369\n",
      "0.7568572014570236\n",
      "0.11127387061715124\n",
      "0.31229440048336976\n",
      "0.4662393897771836\n",
      "0.11846267171204089\n",
      "0.3276490636169911\n",
      "0.7187261819839478\n",
      "0.04451318942010403\n",
      "0.08717634878121315\n",
      "0.2557177729904652\n",
      "0.6874329656362534\n",
      "0.6361204475164413\n",
      "0.05534575269557536\n",
      "0.1264544461853802\n",
      "0.6924605309963225\n",
      "0.5236224740743637\n",
      "0.0717076123226434\n",
      "0.6470351010560988\n",
      "0.2208141578361392\n",
      "0.32288185171782974\n",
      "0.18223639177158474\n",
      "0.04178438936360181\n",
      "0.031748192431405184\n",
      "0.13144762814044952\n",
      "0.8241240799427032\n",
      "0.0373317623976618\n",
      "0.16371795460581778\n",
      "0.19960671942681074\n",
      "0.3622957222163677\n",
      "0.3260825969278812\n",
      "0.3711144842207432\n",
      "0.3186483316123486\n",
      "0.08548194067552686\n",
      "0.26686675325036047\n",
      "0.545507837086916\n",
      "0.6271290108561516\n",
      "0.23574390597641465\n",
      "0.17515668654814362\n",
      "0.3107070446014405\n",
      "0.2724651411175728\n",
      "0.054430325375869865\n",
      "0.9356985926628113\n",
      "0.6962381124496461\n",
      "0.4182286530733109\n",
      "0.8302893817424775\n",
      "0.4683642879128457\n",
      "0.6508054107427598\n",
      "0.11764453463256358\n",
      "0.36346129029989244\n",
      "0.3556136012077332\n",
      "0.36110441200435156\n",
      "0.3411383513361215\n",
      "0.9151489019393921\n",
      "0.5781772300601005\n",
      "0.6945650160312653\n",
      "0.0856637679040432\n",
      "0.21318393144756556\n",
      "0.31093442514538766\n",
      "0.6174819335341454\n",
      "0.10604711314663293\n",
      "0.9102593302726746\n",
      "0.8859121799468993\n",
      "0.5176700308918953\n",
      "0.5782934803515672\n",
      "0.6572357326745988\n",
      "0.18397115170955655\n",
      "0.39056323617696764\n",
      "0.5059115163981914\n",
      "0.06799457324668766\n",
      "0.6140937119722366\n",
      "0.23523844256997112\n",
      "0.0466965779196471\n",
      "0.05720991687849164\n",
      "0.8036366879940032\n",
      "0.3709605172276496\n",
      "0.5860842492431403\n",
      "0.922320717573166\n",
      "0.24240499120205644\n",
      "0.09900784203782678\n",
      "0.7580782264471053\n",
      "0.4540442910045386\n",
      "0.5461215980350971\n",
      "0.05375140439718962\n",
      "0.5178849533200265\n",
      "0.20130125787109135\n",
      "0.8971303939819336\n",
      "0.14920002333819868\n",
      "0.16035284311510623\n",
      "0.32880630865693095\n",
      "0.6806362897157668\n",
      "0.44925979869440197\n",
      "0.5577668122947217\n",
      "0.5819619290530682\n",
      "0.20872096940875054\n",
      "0.7512795805931093\n",
      "0.3253602771088481\n",
      "0.3923784375190735\n",
      "0.8286965668201447\n",
      "0.04695836692117155\n",
      "0.22723261769860983\n",
      "0.3060173023492098\n",
      "0.12933481670916078\n",
      "0.48720072358846667\n",
      "0.20563806407153606\n",
      "0.7940184116363527\n",
      "0.11805728180333971\n",
      "0.35888075996190305\n",
      "0.5949982032179834\n",
      "0.42631578296422956\n",
      "0.2029874235391617\n",
      "0.6077954187989235\n",
      "0.5150628462433815\n",
      "0.6887818068265916\n",
      "0.8569555640220643\n",
      "0.7881232440471649\n",
      "0.32517332322895526\n",
      "0.1818016843870282\n",
      "0.21066985651850695\n",
      "0.1057953964918852\n",
      "0.6807015836238861\n",
      "0.9077234268188477\n",
      "0.37191538568586113\n",
      "0.14329031454399227\n",
      "0.5324559226632118\n",
      "0.6597270220518112\n",
      "0.13587903175503013\n",
      "0.5399748519062996\n",
      "0.07033728454262018\n",
      "0.8990358650684356\n",
      "0.091317172953859\n",
      "0.14736201483756303\n",
      "0.03254887058865279\n",
      "0.180571873113513\n",
      "0.7268503189086915\n",
      "0.12573360837996006\n",
      "0.9474445641040802\n",
      "0.09567145388573409\n",
      "0.5270688369870187\n",
      "0.04890915718860925\n",
      "0.1740773907862604\n",
      "0.05661487770266832\n",
      "0.7939637720584869\n",
      "0.7394599586725235\n",
      "0.5052846536040306\n",
      "0.5835700213909149\n",
      "0.4749379321932793\n",
      "0.2751758132129908\n",
      "0.07755394484847784\n",
      "0.8646654605865478\n",
      "0.4685713239014148\n",
      "0.7781776249408721\n",
      "0.843131238222122\n",
      "0.7801570385694504\n",
      "0.1565922431647778\n",
      "0.21166843995451928\n",
      "0.7713696479797364\n",
      "0.5150743447244167\n",
      "0.08610133910551666\n",
      "0.12299705566838384\n",
      "0.140381146594882\n",
      "0.7848386287689209\n",
      "0.06269888516981155\n",
      "0.16840188642963766\n",
      "0.36118910070508714\n",
      "0.8413900315761568\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(preds_for_sub <= 0.5))\n",
    "print(len(preds_for_sub), '\\n')\n",
    "for line in preds_for_sub:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
