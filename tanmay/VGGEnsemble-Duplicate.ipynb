{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/rcmalli/keras-vggface.git\n",
    "# !pip install keras_vggface\n",
    "# !pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 16:18:52.517822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VGGFace compatible with TensorFlow2.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "from tf2_keras_vggface.utils import preprocess_input\n",
    "from tf2_keras_vggface.vggface import VGGFace\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BASE_MODEL = 'senet50'\n",
    "INPUT_SHAPE = (224, 224,)\n",
    "IGNORE_BOTTOM_NLAYERS_TUNE = -6 #resnet50:-5; vgg16: -2\n",
    "IGNORE_TOP_NLAYERS_TUNE = 0\n",
    "FINE_TUNE = True\n",
    "EPOCHS = 25\n",
    "\n",
    "# Modify paths as per your method of saving them\n",
    "BASE_PATH = \"/root/KinshipRecognition\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_PATH}/data/aug_train_ds.csv\"\n",
    "TRAIN_FOLDERS_PATH = f\"{BASE_PATH}/data/train/train-faces/\"\n",
    "\n",
    "# Output file\n",
    "MODEL_NAME = f\"ensemble_vggface_{BASE_MODEL}_finetune6_dense32-128-32_drop05\"\n",
    "\n",
    "# All images belonging to families F09** will be used to create the validation set while training the model\n",
    "# For final submission, you can add these to the training data as well\n",
    "# val_families_list = [\"F06\"]\n",
    "# val_families_list = [\"F02\",\"F04\",\"F06\",\"F08\", \"F09\"]\n",
    "val_families_list = [\"F00\", \"F01\", \"F02\", \"F03\", \"F04\", \"F05\", \"F06\", \"F07\", \"F08\", \"F09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val(family_name):\n",
    "\n",
    "    val_families = family_name\n",
    "\n",
    "    all_images = glob(TRAIN_FOLDERS_PATH + \"*/*/*.jpg\")\n",
    "    train_images = [x for x in all_images if val_families not in x]\n",
    "    val_images = [x for x in all_images if val_families in x]\n",
    "\n",
    "    train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "    for x in train_images:\n",
    "        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "    val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    for x in val_images:\n",
    "        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "    relationships = pd.read_csv(TRAIN_FILE_PATH)\n",
    "    relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
    "    relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]    \n",
    "\n",
    "    train = [x for x in relationships if val_families not in x[0]]\n",
    "    val = [x for x in relationships if val_families in x[0]]\n",
    "    return train, val, train_person_to_images_map, val_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path, input_shape):\n",
    "    img = cv2.imread(path, -1)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = cv2.normalize(img,  np.zeros(img.shape[:2]), 0, 255, cv2.NORM_MINMAX)\n",
    "    return np.array(img).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(list_tuples, person_to_images_map, input_shape, batch_size=16, normalization='base'):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "            labels.append(tup[2])\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Original images preprocessed\n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X1])\n",
    "        \n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X2])\n",
    "        \n",
    "        # Mirrored images\n",
    "        X1_mirror = np.asarray([cv2.flip(x, 1) for x in X1])\n",
    "        X2_mirror = np.asarray([cv2.flip(x, 1) for x in X2])\n",
    "        X1 = np.r_[X1, X1_mirror]\n",
    "        X2 = np.r_[X2, X2_mirror]\n",
    "        \n",
    "        yield [X1, X2], np.r_[labels, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(model_name, fine_tune=True):\n",
    "    input_1 = Input(shape=INPUT_SHAPE + (3,))\n",
    "    input_2 = Input(shape=INPUT_SHAPE + (3,))\n",
    "\n",
    "    backbone = VGGFace(model=model_name, include_top=False)\n",
    "    for x in backbone.layers:\n",
    "        x.trainable = False\n",
    "\n",
    "    if fine_tune:\n",
    "        for x in backbone.layers[:IGNORE_BOTTOM_NLAYERS_TUNE]:\n",
    "            x.trainable = False\n",
    "        if IGNORE_TOP_NLAYERS_TUNE == 0:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:]:\n",
    "                x.trainable = True\n",
    "        else:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:-IGNORE_TOP_NLAYERS_TUNE]:\n",
    "                x.trainable = True\n",
    "\n",
    "    for x in backbone.layers:\n",
    "        print(x.name, x.trainable)\n",
    "\n",
    "    x1 = backbone(input_1)\n",
    "    x2 = backbone(input_2)\n",
    "\n",
    "    x1 = GlobalAvgPool2D()(x1)\n",
    "    x2 = GlobalAvgPool2D()(x2)\n",
    "\n",
    "    x1 = LayerNormalization(axis=-1, epsilon=0.001, center=False, scale=False)(x1)\n",
    "    x2 = LayerNormalization(axis=-1, epsilon=0.001, center=False, scale=False)(x2)\n",
    "\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "    x1_ = Multiply()([x1, x1])\n",
    "    x2_ = Multiply()([x2, x2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    x5 = Multiply()([x1, x2])\n",
    "    x = Concatenate(axis=-1)([x3, x4, x5])\n",
    "        \n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=True)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(32, activation=\"tanh\")(x)\n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=False)(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    out = Dense(1, kernel_regularizer=L2(.01), activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00002))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 0: Validation on F00\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "\n",
    "    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_families_list[i])\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.3, patience=30, verbose=1)\n",
    "    callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    \n",
    "    history = model.fit(gen(train, train_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        validation_data=gen(val, val_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        epochs=EPOCHS, steps_per_epoch=300, validation_steps=200,\n",
    "                        verbose=1, callbacks=callbacks_list, \n",
    "                        use_multiprocessing=False, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = f\"{BASE_PATH}/data/test/\"\n",
    "submission = pd.read_csv(f'{BASE_PATH}/data/test_ds.csv')\n",
    "preds_for_sub = np.zeros(submission.shape[0])\n",
    "all_preds = list()\n",
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = []\n",
    "    for j in range(0, len(submission.p1.values), 32):\n",
    "        X1 = submission.p1.values[j:j+32]\n",
    "        X1 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X1])\n",
    "\n",
    "        X2 = submission.p2.values[j:j+32]\n",
    "        X2 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X2])\n",
    "\n",
    "        pred = model.predict([X1, X2]).ravel().tolist()\n",
    "        predictions += pred    \n",
    "    \n",
    "    all_preds.append(np.array(predictions))\n",
    "    preds_for_sub += np.array(predictions) / len(val_families_list)\n",
    "\n",
    "    \n",
    "all_preds = np.asarray(all_preds)\n",
    "submission['score'] = preds_for_sub\n",
    "pd.DataFrame(all_preds).to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}_allpreds.csv\", index=False)\n",
    "submission.to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds_for_sub <= 0.5))\n",
    "print(len(preds_for_sub), '\\n')\n",
    "for line in preds_for_sub:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
