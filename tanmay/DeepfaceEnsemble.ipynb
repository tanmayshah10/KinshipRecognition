{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras_vggface\n",
    "# !pip install keras_applications\n",
    "# !pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 05:39:58.469055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "from deepface.commons.functions import find_input_shape, normalize_input\n",
    "from deepface.DeepFace import build_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 05:39:59.512521: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-11 05:39:59.513381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-11 05:39:59.615340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-08-11 05:39:59.615376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-11 05:39:59.616967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-11 05:39:59.617025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-08-11 05:39:59.618405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-11 05:39:59.618660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-11 05:39:59.620234: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-11 05:39:59.621091: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-11 05:39:59.624500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-11 05:39:59.627124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-11 05:39:59.627571: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-11 05:39:59.637294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-08-11 05:39:59.637362: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-11 05:39:59.637402: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-11 05:39:59.637429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-08-11 05:39:59.637456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-11 05:39:59.637493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-11 05:39:59.637521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-11 05:39:59.637547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-11 05:39:59.637574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-11 05:39:59.645234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-11 05:39:59.645291: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-11 05:40:00.168227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-11 05:40:00.168259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-08-11 05:40:00.168263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-08-11 05:40:00.171953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30130 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)\n",
      "2021-08-11 05:40:00.172409: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BASE_MODEL = 'Facenet'\n",
    "IGNORE_TOP_NLAYERS_ARCH = 5\n",
    "IGNORE_BOTTOM_NLAYERS_TUNE = 0\n",
    "IGNORE_TOP_NLAYERS_TUNE = 0\n",
    "NORMALIZATION = 'base'\n",
    "FINE_TUNE = False\n",
    "\n",
    "# Modify paths as per your method of saving them\n",
    "BASE_PATH = \"/root/KinshipRecognition\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_PATH}/data/aug_train_ds.csv\"\n",
    "TRAIN_FOLDERS_PATH = f\"{BASE_PATH}/data/train/train-faces/\"\n",
    "\n",
    "# All images belonging to families F09** will be used to create the validation set while training the model\n",
    "# For final submission, you can add these to the training data as well\n",
    "# val_families_list = [\"F06\"]\n",
    "val_families_list = [\"F02\",\"F04\",\"F06\",\"F08\", \"F09\"]\n",
    "\n",
    "# Output file\n",
    "MODEL_NAME = f\"ensemble_deepface_{BASE_MODEL}_notune_dense32-128-32_drop05\"\n",
    "\n",
    "# Get input shape and normalization method.\n",
    "INPUT_SHAPE = find_input_shape(build_model(BASE_MODEL))\n",
    "NORMALIZATION = 'base' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val(family_name):\n",
    "\n",
    "    val_families = family_name\n",
    "\n",
    "    all_images = glob(TRAIN_FOLDERS_PATH + \"*/*/*.jpg\")\n",
    "    train_images = [x for x in all_images if val_families not in x]\n",
    "    val_images = [x for x in all_images if val_families in x]\n",
    "\n",
    "    train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "    for x in train_images:\n",
    "        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "    val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    for x in val_images:\n",
    "        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "    relationships = pd.read_csv(TRAIN_FILE_PATH)\n",
    "    relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
    "    relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]    \n",
    "\n",
    "    train = [x for x in relationships if val_families not in x[0]]\n",
    "    val = [x for x in relationships if val_families in x[0]]\n",
    "    return train, val, train_person_to_images_map, val_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path, input_shape, normalization='base'):\n",
    "    img = cv2.imread(path, -1)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = cv2.normalize(img,  np.zeros(img.shape[:2]), 0, 255, cv2.NORM_MINMAX)\n",
    "    img = normalize_input(img, normalization=NORMALIZATION)\n",
    "    return np.array(img).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(list_tuples, person_to_images_map, input_shape, batch_size=16, normalization='base'):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "            labels.append(tup[2])\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Original images preprocessed\n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X1])\n",
    "        \n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X2])\n",
    "        \n",
    "        # Mirrored images\n",
    "        X1_mirror = np.asarray([cv2.flip(x, 1) for x in X1])\n",
    "        X2_mirror = np.asarray([cv2.flip(x, 1) for x in X2])\n",
    "        X1 = np.r_[X1, X1_mirror]\n",
    "        X2 = np.r_[X2, X2_mirror]\n",
    "        \n",
    "        yield [X1, X2], np.r_[labels, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(model_name, fine_tune=True):\n",
    "    input_1 = Input(shape=INPUT_SHAPE + (3,))\n",
    "    input_2 = Input(shape=INPUT_SHAPE + (3,))\n",
    "    \n",
    "    backbone = build_model(BASE_MODEL)\n",
    "    backbone = Model(backbone.layers[0].input, backbone.layers[-IGNORE_TOP_NLAYERS_ARCH].output)\n",
    "    for x in backbone.layers:\n",
    "        x.trainable = False\n",
    "\n",
    "    if fine_tune:\n",
    "        for x in backbone.layers[:IGNORE_BOTTOM_NLAYERS_TUNE]:\n",
    "            x.trainable = False\n",
    "        if IGNORE_TOP_NLAYERS_TUNE == 0:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:]:\n",
    "                x.trainable = True\n",
    "        else:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:-IGNORE_TOP_NLAYERS_TUNE]:\n",
    "                x.trainable = True\n",
    "\n",
    "    for x in backbone.layers:\n",
    "        print(x.name, x.trainable)\n",
    "                \n",
    "    x1 = backbone(input_1)\n",
    "    x2 = backbone(input_2)\n",
    "\n",
    "    x1 = GlobalAvgPool2D()(x1)\n",
    "    x2 = GlobalAvgPool2D()(x2)\n",
    "\n",
    "    x1 = LayerNormalization(axis=-1, epsilon=0.001, center=False, scale=False)(x1)\n",
    "    x2 = LayerNormalization(axis=-1, epsilon=0.001, center=False, scale=False)(x2)\n",
    "\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "    x1_ = Multiply()([x1, x1])\n",
    "    x2_ = Multiply()([x2, x2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    x5 = Multiply()([x1, x2])\n",
    "    x = Concatenate(axis=-1)([x3, x4, x5])\n",
    "        \n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=True)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(32, activation=\"tanh\")(x)\n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=False)(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    out = Dense(1, kernel_regularizer=L2(.01), activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "Conv2d_1a_3x3\n",
      "Conv2d_1a_3x3_BatchNorm\n",
      "Conv2d_1a_3x3_Activation\n",
      "Conv2d_2a_3x3\n",
      "Conv2d_2a_3x3_BatchNorm\n",
      "Conv2d_2a_3x3_Activation\n",
      "Conv2d_2b_3x3\n",
      "Conv2d_2b_3x3_BatchNorm\n",
      "Conv2d_2b_3x3_Activation\n",
      "MaxPool_3a_3x3\n",
      "Conv2d_3b_1x1\n",
      "Conv2d_3b_1x1_BatchNorm\n",
      "Conv2d_3b_1x1_Activation\n",
      "Conv2d_4a_3x3\n",
      "Conv2d_4a_3x3_BatchNorm\n",
      "Conv2d_4a_3x3_Activation\n",
      "Conv2d_4b_3x3\n",
      "Conv2d_4b_3x3_BatchNorm\n",
      "Conv2d_4b_3x3_Activation\n",
      "Block35_1_Branch_2_Conv2d_0a_1x1\n",
      "Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_1_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Block35_1_Branch_1_Conv2d_0a_1x1\n",
      "Block35_1_Branch_2_Conv2d_0b_3x3\n",
      "Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_1_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block35_1_Branch_2_Conv2d_0b_3x3_Activation\n",
      "Block35_1_Branch_0_Conv2d_1x1\n",
      "Block35_1_Branch_1_Conv2d_0b_3x3\n",
      "Block35_1_Branch_2_Conv2d_0c_3x3\n",
      "Block35_1_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm\n",
      "Block35_1_Branch_0_Conv2d_1x1_Activation\n",
      "Block35_1_Branch_1_Conv2d_0b_3x3_Activation\n",
      "Block35_1_Branch_2_Conv2d_0c_3x3_Activation\n",
      "Block35_1_Concatenate\n",
      "Block35_1_Conv2d_1x1\n",
      "lambda\n",
      "add\n",
      "Block35_1_Activation\n",
      "Block35_2_Branch_2_Conv2d_0a_1x1\n",
      "Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_2_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Block35_2_Branch_1_Conv2d_0a_1x1\n",
      "Block35_2_Branch_2_Conv2d_0b_3x3\n",
      "Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_2_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block35_2_Branch_2_Conv2d_0b_3x3_Activation\n",
      "Block35_2_Branch_0_Conv2d_1x1\n",
      "Block35_2_Branch_1_Conv2d_0b_3x3\n",
      "Block35_2_Branch_2_Conv2d_0c_3x3\n",
      "Block35_2_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm\n",
      "Block35_2_Branch_0_Conv2d_1x1_Activation\n",
      "Block35_2_Branch_1_Conv2d_0b_3x3_Activation\n",
      "Block35_2_Branch_2_Conv2d_0c_3x3_Activation\n",
      "Block35_2_Concatenate\n",
      "Block35_2_Conv2d_1x1\n",
      "lambda_1\n",
      "add_1\n",
      "Block35_2_Activation\n",
      "Block35_3_Branch_2_Conv2d_0a_1x1\n",
      "Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_3_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Block35_3_Branch_1_Conv2d_0a_1x1\n",
      "Block35_3_Branch_2_Conv2d_0b_3x3\n",
      "Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_3_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block35_3_Branch_2_Conv2d_0b_3x3_Activation\n",
      "Block35_3_Branch_0_Conv2d_1x1\n",
      "Block35_3_Branch_1_Conv2d_0b_3x3\n",
      "Block35_3_Branch_2_Conv2d_0c_3x3\n",
      "Block35_3_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm\n",
      "Block35_3_Branch_0_Conv2d_1x1_Activation\n",
      "Block35_3_Branch_1_Conv2d_0b_3x3_Activation\n",
      "Block35_3_Branch_2_Conv2d_0c_3x3_Activation\n",
      "Block35_3_Concatenate\n",
      "Block35_3_Conv2d_1x1\n",
      "lambda_2\n",
      "add_2\n",
      "Block35_3_Activation\n",
      "Block35_4_Branch_2_Conv2d_0a_1x1\n",
      "Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_4_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Block35_4_Branch_1_Conv2d_0a_1x1\n",
      "Block35_4_Branch_2_Conv2d_0b_3x3\n",
      "Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_4_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block35_4_Branch_2_Conv2d_0b_3x3_Activation\n",
      "Block35_4_Branch_0_Conv2d_1x1\n",
      "Block35_4_Branch_1_Conv2d_0b_3x3\n",
      "Block35_4_Branch_2_Conv2d_0c_3x3\n",
      "Block35_4_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm\n",
      "Block35_4_Branch_0_Conv2d_1x1_Activation\n",
      "Block35_4_Branch_1_Conv2d_0b_3x3_Activation\n",
      "Block35_4_Branch_2_Conv2d_0c_3x3_Activation\n",
      "Block35_4_Concatenate\n",
      "Block35_4_Conv2d_1x1\n",
      "lambda_3\n",
      "add_3\n",
      "Block35_4_Activation\n",
      "Block35_5_Branch_2_Conv2d_0a_1x1\n",
      "Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_5_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Block35_5_Branch_1_Conv2d_0a_1x1\n",
      "Block35_5_Branch_2_Conv2d_0b_3x3\n",
      "Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_5_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block35_5_Branch_2_Conv2d_0b_3x3_Activation\n",
      "Block35_5_Branch_0_Conv2d_1x1\n",
      "Block35_5_Branch_1_Conv2d_0b_3x3\n",
      "Block35_5_Branch_2_Conv2d_0c_3x3\n",
      "Block35_5_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm\n",
      "Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm\n",
      "Block35_5_Branch_0_Conv2d_1x1_Activation\n",
      "Block35_5_Branch_1_Conv2d_0b_3x3_Activation\n",
      "Block35_5_Branch_2_Conv2d_0c_3x3_Activation\n",
      "Block35_5_Concatenate\n",
      "Block35_5_Conv2d_1x1\n",
      "lambda_4\n",
      "add_4\n",
      "Block35_5_Activation\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation\n",
      "Mixed_6a_Branch_2_MaxPool_1a_3x3\n",
      "Mixed_6a\n",
      "Block17_1_Branch_1_Conv2d_0a_1x1\n",
      "Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_1_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block17_1_Branch_1_Conv2d_0b_1x7\n",
      "Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_1_Branch_1_Conv2d_0b_1x7_Activation\n",
      "Block17_1_Branch_0_Conv2d_1x1\n",
      "Block17_1_Branch_1_Conv2d_0c_7x1\n",
      "Block17_1_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_1_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_1_Branch_1_Conv2d_0c_7x1_Activation\n",
      "Block17_1_Concatenate\n",
      "Block17_1_Conv2d_1x1\n",
      "lambda_5\n",
      "add_5\n",
      "Block17_1_Activation\n",
      "Block17_2_Branch_2_Conv2d_0a_1x1\n",
      "Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_2_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Block17_2_Branch_2_Conv2d_0b_1x7\n",
      "Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_2_Branch_2_Conv2d_0b_1x7_Activation\n",
      "Block17_2_Branch_0_Conv2d_1x1\n",
      "Block17_2_Branch_2_Conv2d_0c_7x1\n",
      "Block17_2_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_2_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_2_Branch_2_Conv2d_0c_7x1_Activation\n",
      "Block17_2_Concatenate\n",
      "Block17_2_Conv2d_1x1\n",
      "lambda_6\n",
      "add_6\n",
      "Block17_2_Activation\n",
      "Block17_3_Branch_3_Conv2d_0a_1x1\n",
      "Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_3_Branch_3_Conv2d_0a_1x1_Activation\n",
      "Block17_3_Branch_3_Conv2d_0b_1x7\n",
      "Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_3_Branch_3_Conv2d_0b_1x7_Activation\n",
      "Block17_3_Branch_0_Conv2d_1x1\n",
      "Block17_3_Branch_3_Conv2d_0c_7x1\n",
      "Block17_3_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_3_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_3_Branch_3_Conv2d_0c_7x1_Activation\n",
      "Block17_3_Concatenate\n",
      "Block17_3_Conv2d_1x1\n",
      "lambda_7\n",
      "add_7\n",
      "Block17_3_Activation\n",
      "Block17_4_Branch_4_Conv2d_0a_1x1\n",
      "Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_4_Branch_4_Conv2d_0a_1x1_Activation\n",
      "Block17_4_Branch_4_Conv2d_0b_1x7\n",
      "Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_4_Branch_4_Conv2d_0b_1x7_Activation\n",
      "Block17_4_Branch_0_Conv2d_1x1\n",
      "Block17_4_Branch_4_Conv2d_0c_7x1\n",
      "Block17_4_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_4_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_4_Branch_4_Conv2d_0c_7x1_Activation\n",
      "Block17_4_Concatenate\n",
      "Block17_4_Conv2d_1x1\n",
      "lambda_8\n",
      "add_8\n",
      "Block17_4_Activation\n",
      "Block17_5_Branch_5_Conv2d_0a_1x1\n",
      "Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_5_Branch_5_Conv2d_0a_1x1_Activation\n",
      "Block17_5_Branch_5_Conv2d_0b_1x7\n",
      "Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_5_Branch_5_Conv2d_0b_1x7_Activation\n",
      "Block17_5_Branch_0_Conv2d_1x1\n",
      "Block17_5_Branch_5_Conv2d_0c_7x1\n",
      "Block17_5_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_5_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_5_Branch_5_Conv2d_0c_7x1_Activation\n",
      "Block17_5_Concatenate\n",
      "Block17_5_Conv2d_1x1\n",
      "lambda_9\n",
      "add_9\n",
      "Block17_5_Activation\n",
      "Block17_6_Branch_6_Conv2d_0a_1x1\n",
      "Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_6_Branch_6_Conv2d_0a_1x1_Activation\n",
      "Block17_6_Branch_6_Conv2d_0b_1x7\n",
      "Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_6_Branch_6_Conv2d_0b_1x7_Activation\n",
      "Block17_6_Branch_0_Conv2d_1x1\n",
      "Block17_6_Branch_6_Conv2d_0c_7x1\n",
      "Block17_6_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_6_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_6_Branch_6_Conv2d_0c_7x1_Activation\n",
      "Block17_6_Concatenate\n",
      "Block17_6_Conv2d_1x1\n",
      "lambda_10\n",
      "add_10\n",
      "Block17_6_Activation\n",
      "Block17_7_Branch_7_Conv2d_0a_1x1\n",
      "Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_7_Branch_7_Conv2d_0a_1x1_Activation\n",
      "Block17_7_Branch_7_Conv2d_0b_1x7\n",
      "Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_7_Branch_7_Conv2d_0b_1x7_Activation\n",
      "Block17_7_Branch_0_Conv2d_1x1\n",
      "Block17_7_Branch_7_Conv2d_0c_7x1\n",
      "Block17_7_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_7_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_7_Branch_7_Conv2d_0c_7x1_Activation\n",
      "Block17_7_Concatenate\n",
      "Block17_7_Conv2d_1x1\n",
      "lambda_11\n",
      "add_11\n",
      "Block17_7_Activation\n",
      "Block17_8_Branch_8_Conv2d_0a_1x1\n",
      "Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_8_Branch_8_Conv2d_0a_1x1_Activation\n",
      "Block17_8_Branch_8_Conv2d_0b_1x7\n",
      "Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_8_Branch_8_Conv2d_0b_1x7_Activation\n",
      "Block17_8_Branch_0_Conv2d_1x1\n",
      "Block17_8_Branch_8_Conv2d_0c_7x1\n",
      "Block17_8_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_8_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_8_Branch_8_Conv2d_0c_7x1_Activation\n",
      "Block17_8_Concatenate\n",
      "Block17_8_Conv2d_1x1\n",
      "lambda_12\n",
      "add_12\n",
      "Block17_8_Activation\n",
      "Block17_9_Branch_9_Conv2d_0a_1x1\n",
      "Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_9_Branch_9_Conv2d_0a_1x1_Activation\n",
      "Block17_9_Branch_9_Conv2d_0b_1x7\n",
      "Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_9_Branch_9_Conv2d_0b_1x7_Activation\n",
      "Block17_9_Branch_0_Conv2d_1x1\n",
      "Block17_9_Branch_9_Conv2d_0c_7x1\n",
      "Block17_9_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_9_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_9_Branch_9_Conv2d_0c_7x1_Activation\n",
      "Block17_9_Concatenate\n",
      "Block17_9_Conv2d_1x1\n",
      "lambda_13\n",
      "add_13\n",
      "Block17_9_Activation\n",
      "Block17_10_Branch_10_Conv2d_0a_1x1\n",
      "Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm\n",
      "Block17_10_Branch_10_Conv2d_0a_1x1_Activation\n",
      "Block17_10_Branch_10_Conv2d_0b_1x7\n",
      "Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm\n",
      "Block17_10_Branch_10_Conv2d_0b_1x7_Activation\n",
      "Block17_10_Branch_0_Conv2d_1x1\n",
      "Block17_10_Branch_10_Conv2d_0c_7x1\n",
      "Block17_10_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm\n",
      "Block17_10_Branch_0_Conv2d_1x1_Activation\n",
      "Block17_10_Branch_10_Conv2d_0c_7x1_Activation\n",
      "Block17_10_Concatenate\n",
      "Block17_10_Conv2d_1x1\n",
      "lambda_14\n",
      "add_14\n",
      "Block17_10_Activation\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation\n",
      "Mixed_7a_Branch_3_MaxPool_1a_3x3\n",
      "Mixed_7a\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3_Activation\n",
      "Block8_1_Branch_0_Conv2d_1x1\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1\n",
      "Block8_1_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm\n",
      "Block8_1_Branch_0_Conv2d_1x1_Activation\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1_Activation\n",
      "Block8_1_Concatenate\n",
      "Block8_1_Conv2d_1x1\n",
      "lambda_15\n",
      "add_15\n",
      "Block8_1_Activation\n",
      "Block8_2_Branch_2_Conv2d_0a_1x1\n",
      "Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm\n",
      "Block8_2_Branch_2_Conv2d_0a_1x1_Activation\n",
      "Block8_2_Branch_2_Conv2d_0b_1x3\n",
      "Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm\n",
      "Block8_2_Branch_2_Conv2d_0b_1x3_Activation\n",
      "Block8_2_Branch_0_Conv2d_1x1\n",
      "Block8_2_Branch_2_Conv2d_0c_3x1\n",
      "Block8_2_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm\n",
      "Block8_2_Branch_0_Conv2d_1x1_Activation\n",
      "Block8_2_Branch_2_Conv2d_0c_3x1_Activation\n",
      "Block8_2_Concatenate\n",
      "Block8_2_Conv2d_1x1\n",
      "lambda_16\n",
      "add_16\n",
      "Block8_2_Activation\n",
      "Block8_3_Branch_3_Conv2d_0a_1x1\n",
      "Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm\n",
      "Block8_3_Branch_3_Conv2d_0a_1x1_Activation\n",
      "Block8_3_Branch_3_Conv2d_0b_1x3\n",
      "Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm\n",
      "Block8_3_Branch_3_Conv2d_0b_1x3_Activation\n",
      "Block8_3_Branch_0_Conv2d_1x1\n",
      "Block8_3_Branch_3_Conv2d_0c_3x1\n",
      "Block8_3_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm\n",
      "Block8_3_Branch_0_Conv2d_1x1_Activation\n",
      "Block8_3_Branch_3_Conv2d_0c_3x1_Activation\n",
      "Block8_3_Concatenate\n",
      "Block8_3_Conv2d_1x1\n",
      "lambda_17\n",
      "add_17\n",
      "Block8_3_Activation\n",
      "Block8_4_Branch_4_Conv2d_0a_1x1\n",
      "Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm\n",
      "Block8_4_Branch_4_Conv2d_0a_1x1_Activation\n",
      "Block8_4_Branch_4_Conv2d_0b_1x3\n",
      "Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm\n",
      "Block8_4_Branch_4_Conv2d_0b_1x3_Activation\n",
      "Block8_4_Branch_0_Conv2d_1x1\n",
      "Block8_4_Branch_4_Conv2d_0c_3x1\n",
      "Block8_4_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm\n",
      "Block8_4_Branch_0_Conv2d_1x1_Activation\n",
      "Block8_4_Branch_4_Conv2d_0c_3x1_Activation\n",
      "Block8_4_Concatenate\n",
      "Block8_4_Conv2d_1x1\n",
      "lambda_18\n",
      "add_18\n",
      "Block8_4_Activation\n",
      "Block8_5_Branch_5_Conv2d_0a_1x1\n",
      "Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm\n",
      "Block8_5_Branch_5_Conv2d_0a_1x1_Activation\n",
      "Block8_5_Branch_5_Conv2d_0b_1x3\n",
      "Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm\n",
      "Block8_5_Branch_5_Conv2d_0b_1x3_Activation\n",
      "Block8_5_Branch_0_Conv2d_1x1\n",
      "Block8_5_Branch_5_Conv2d_0c_3x1\n",
      "Block8_5_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm\n",
      "Block8_5_Branch_0_Conv2d_1x1_Activation\n",
      "Block8_5_Branch_5_Conv2d_0c_3x1_Activation\n",
      "Block8_5_Concatenate\n",
      "Block8_5_Conv2d_1x1\n",
      "lambda_19\n",
      "add_19\n",
      "Block8_5_Activation\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1_Activation\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3_Activation\n",
      "Block8_6_Branch_0_Conv2d_1x1\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1\n",
      "Block8_6_Branch_0_Conv2d_1x1_BatchNorm\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm\n",
      "Block8_6_Branch_0_Conv2d_1x1_Activation\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1_Activation\n",
      "Block8_6_Concatenate\n",
      "Block8_6_Conv2d_1x1\n",
      "lambda_20\n",
      "add_20\n"
     ]
    }
   ],
   "source": [
    "# Print layers of backbone\n",
    "backbone = build_model(BASE_MODEL)\n",
    "backbone = Model(backbone.layers[0].input, backbone.layers[-IGNORE_TOP_NLAYERS_ARCH].output)\n",
    "for x in backbone.layers:\n",
    "    print(x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 0: Validation on F09\n",
      "##############################\n",
      "input_1 False\n",
      "Conv2d_1a_3x3 False\n",
      "Conv2d_1a_3x3_BatchNorm False\n",
      "Conv2d_1a_3x3_Activation False\n",
      "Conv2d_2a_3x3 False\n",
      "Conv2d_2a_3x3_BatchNorm False\n",
      "Conv2d_2a_3x3_Activation False\n",
      "Conv2d_2b_3x3 False\n",
      "Conv2d_2b_3x3_BatchNorm False\n",
      "Conv2d_2b_3x3_Activation False\n",
      "MaxPool_3a_3x3 False\n",
      "Conv2d_3b_1x1 False\n",
      "Conv2d_3b_1x1_BatchNorm False\n",
      "Conv2d_3b_1x1_Activation False\n",
      "Conv2d_4a_3x3 False\n",
      "Conv2d_4a_3x3_BatchNorm False\n",
      "Conv2d_4a_3x3_Activation False\n",
      "Conv2d_4b_3x3 False\n",
      "Conv2d_4b_3x3_BatchNorm False\n",
      "Conv2d_4b_3x3_Activation False\n",
      "Block35_1_Branch_2_Conv2d_0a_1x1 False\n",
      "Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_1_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Block35_1_Branch_1_Conv2d_0a_1x1 False\n",
      "Block35_1_Branch_2_Conv2d_0b_3x3 False\n",
      "Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_1_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block35_1_Branch_2_Conv2d_0b_3x3_Activation False\n",
      "Block35_1_Branch_0_Conv2d_1x1 False\n",
      "Block35_1_Branch_1_Conv2d_0b_3x3 False\n",
      "Block35_1_Branch_2_Conv2d_0c_3x3 False\n",
      "Block35_1_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm False\n",
      "Block35_1_Branch_0_Conv2d_1x1_Activation False\n",
      "Block35_1_Branch_1_Conv2d_0b_3x3_Activation False\n",
      "Block35_1_Branch_2_Conv2d_0c_3x3_Activation False\n",
      "Block35_1_Concatenate False\n",
      "Block35_1_Conv2d_1x1 False\n",
      "lambda False\n",
      "add False\n",
      "Block35_1_Activation False\n",
      "Block35_2_Branch_2_Conv2d_0a_1x1 False\n",
      "Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_2_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Block35_2_Branch_1_Conv2d_0a_1x1 False\n",
      "Block35_2_Branch_2_Conv2d_0b_3x3 False\n",
      "Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_2_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block35_2_Branch_2_Conv2d_0b_3x3_Activation False\n",
      "Block35_2_Branch_0_Conv2d_1x1 False\n",
      "Block35_2_Branch_1_Conv2d_0b_3x3 False\n",
      "Block35_2_Branch_2_Conv2d_0c_3x3 False\n",
      "Block35_2_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm False\n",
      "Block35_2_Branch_0_Conv2d_1x1_Activation False\n",
      "Block35_2_Branch_1_Conv2d_0b_3x3_Activation False\n",
      "Block35_2_Branch_2_Conv2d_0c_3x3_Activation False\n",
      "Block35_2_Concatenate False\n",
      "Block35_2_Conv2d_1x1 False\n",
      "lambda_1 False\n",
      "add_1 False\n",
      "Block35_2_Activation False\n",
      "Block35_3_Branch_2_Conv2d_0a_1x1 False\n",
      "Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_3_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Block35_3_Branch_1_Conv2d_0a_1x1 False\n",
      "Block35_3_Branch_2_Conv2d_0b_3x3 False\n",
      "Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_3_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block35_3_Branch_2_Conv2d_0b_3x3_Activation False\n",
      "Block35_3_Branch_0_Conv2d_1x1 False\n",
      "Block35_3_Branch_1_Conv2d_0b_3x3 False\n",
      "Block35_3_Branch_2_Conv2d_0c_3x3 False\n",
      "Block35_3_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm False\n",
      "Block35_3_Branch_0_Conv2d_1x1_Activation False\n",
      "Block35_3_Branch_1_Conv2d_0b_3x3_Activation False\n",
      "Block35_3_Branch_2_Conv2d_0c_3x3_Activation False\n",
      "Block35_3_Concatenate False\n",
      "Block35_3_Conv2d_1x1 False\n",
      "lambda_2 False\n",
      "add_2 False\n",
      "Block35_3_Activation False\n",
      "Block35_4_Branch_2_Conv2d_0a_1x1 False\n",
      "Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_4_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Block35_4_Branch_1_Conv2d_0a_1x1 False\n",
      "Block35_4_Branch_2_Conv2d_0b_3x3 False\n",
      "Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_4_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block35_4_Branch_2_Conv2d_0b_3x3_Activation False\n",
      "Block35_4_Branch_0_Conv2d_1x1 False\n",
      "Block35_4_Branch_1_Conv2d_0b_3x3 False\n",
      "Block35_4_Branch_2_Conv2d_0c_3x3 False\n",
      "Block35_4_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm False\n",
      "Block35_4_Branch_0_Conv2d_1x1_Activation False\n",
      "Block35_4_Branch_1_Conv2d_0b_3x3_Activation False\n",
      "Block35_4_Branch_2_Conv2d_0c_3x3_Activation False\n",
      "Block35_4_Concatenate False\n",
      "Block35_4_Conv2d_1x1 False\n",
      "lambda_3 False\n",
      "add_3 False\n",
      "Block35_4_Activation False\n",
      "Block35_5_Branch_2_Conv2d_0a_1x1 False\n",
      "Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_5_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Block35_5_Branch_1_Conv2d_0a_1x1 False\n",
      "Block35_5_Branch_2_Conv2d_0b_3x3 False\n",
      "Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_5_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block35_5_Branch_2_Conv2d_0b_3x3_Activation False\n",
      "Block35_5_Branch_0_Conv2d_1x1 False\n",
      "Block35_5_Branch_1_Conv2d_0b_3x3 False\n",
      "Block35_5_Branch_2_Conv2d_0c_3x3 False\n",
      "Block35_5_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm False\n",
      "Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm False\n",
      "Block35_5_Branch_0_Conv2d_1x1_Activation False\n",
      "Block35_5_Branch_1_Conv2d_0b_3x3_Activation False\n",
      "Block35_5_Branch_2_Conv2d_0c_3x3_Activation False\n",
      "Block35_5_Concatenate False\n",
      "Block35_5_Conv2d_1x1 False\n",
      "lambda_4 False\n",
      "add_4 False\n",
      "Block35_5_Activation False\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1 False\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3 False\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm False\n",
      "Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation False\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3 False\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3 False\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm False\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm False\n",
      "Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation False\n",
      "Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation False\n",
      "Mixed_6a_Branch_2_MaxPool_1a_3x3 False\n",
      "Mixed_6a False\n",
      "Block17_1_Branch_1_Conv2d_0a_1x1 False\n",
      "Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_1_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block17_1_Branch_1_Conv2d_0b_1x7 False\n",
      "Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_1_Branch_1_Conv2d_0b_1x7_Activation False\n",
      "Block17_1_Branch_0_Conv2d_1x1 False\n",
      "Block17_1_Branch_1_Conv2d_0c_7x1 False\n",
      "Block17_1_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_1_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_1_Branch_1_Conv2d_0c_7x1_Activation False\n",
      "Block17_1_Concatenate False\n",
      "Block17_1_Conv2d_1x1 False\n",
      "lambda_5 False\n",
      "add_5 False\n",
      "Block17_1_Activation False\n",
      "Block17_2_Branch_2_Conv2d_0a_1x1 False\n",
      "Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_2_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Block17_2_Branch_2_Conv2d_0b_1x7 False\n",
      "Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_2_Branch_2_Conv2d_0b_1x7_Activation False\n",
      "Block17_2_Branch_0_Conv2d_1x1 False\n",
      "Block17_2_Branch_2_Conv2d_0c_7x1 False\n",
      "Block17_2_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_2_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_2_Branch_2_Conv2d_0c_7x1_Activation False\n",
      "Block17_2_Concatenate False\n",
      "Block17_2_Conv2d_1x1 False\n",
      "lambda_6 False\n",
      "add_6 False\n",
      "Block17_2_Activation False\n",
      "Block17_3_Branch_3_Conv2d_0a_1x1 False\n",
      "Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_3_Branch_3_Conv2d_0a_1x1_Activation False\n",
      "Block17_3_Branch_3_Conv2d_0b_1x7 False\n",
      "Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_3_Branch_3_Conv2d_0b_1x7_Activation False\n",
      "Block17_3_Branch_0_Conv2d_1x1 False\n",
      "Block17_3_Branch_3_Conv2d_0c_7x1 False\n",
      "Block17_3_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_3_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_3_Branch_3_Conv2d_0c_7x1_Activation False\n",
      "Block17_3_Concatenate False\n",
      "Block17_3_Conv2d_1x1 False\n",
      "lambda_7 False\n",
      "add_7 False\n",
      "Block17_3_Activation False\n",
      "Block17_4_Branch_4_Conv2d_0a_1x1 False\n",
      "Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_4_Branch_4_Conv2d_0a_1x1_Activation False\n",
      "Block17_4_Branch_4_Conv2d_0b_1x7 False\n",
      "Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_4_Branch_4_Conv2d_0b_1x7_Activation False\n",
      "Block17_4_Branch_0_Conv2d_1x1 False\n",
      "Block17_4_Branch_4_Conv2d_0c_7x1 False\n",
      "Block17_4_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_4_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_4_Branch_4_Conv2d_0c_7x1_Activation False\n",
      "Block17_4_Concatenate False\n",
      "Block17_4_Conv2d_1x1 False\n",
      "lambda_8 False\n",
      "add_8 False\n",
      "Block17_4_Activation False\n",
      "Block17_5_Branch_5_Conv2d_0a_1x1 False\n",
      "Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_5_Branch_5_Conv2d_0a_1x1_Activation False\n",
      "Block17_5_Branch_5_Conv2d_0b_1x7 False\n",
      "Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_5_Branch_5_Conv2d_0b_1x7_Activation False\n",
      "Block17_5_Branch_0_Conv2d_1x1 False\n",
      "Block17_5_Branch_5_Conv2d_0c_7x1 False\n",
      "Block17_5_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_5_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_5_Branch_5_Conv2d_0c_7x1_Activation False\n",
      "Block17_5_Concatenate False\n",
      "Block17_5_Conv2d_1x1 False\n",
      "lambda_9 False\n",
      "add_9 False\n",
      "Block17_5_Activation False\n",
      "Block17_6_Branch_6_Conv2d_0a_1x1 False\n",
      "Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_6_Branch_6_Conv2d_0a_1x1_Activation False\n",
      "Block17_6_Branch_6_Conv2d_0b_1x7 False\n",
      "Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_6_Branch_6_Conv2d_0b_1x7_Activation False\n",
      "Block17_6_Branch_0_Conv2d_1x1 False\n",
      "Block17_6_Branch_6_Conv2d_0c_7x1 False\n",
      "Block17_6_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_6_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_6_Branch_6_Conv2d_0c_7x1_Activation False\n",
      "Block17_6_Concatenate False\n",
      "Block17_6_Conv2d_1x1 False\n",
      "lambda_10 False\n",
      "add_10 False\n",
      "Block17_6_Activation False\n",
      "Block17_7_Branch_7_Conv2d_0a_1x1 False\n",
      "Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_7_Branch_7_Conv2d_0a_1x1_Activation False\n",
      "Block17_7_Branch_7_Conv2d_0b_1x7 False\n",
      "Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_7_Branch_7_Conv2d_0b_1x7_Activation False\n",
      "Block17_7_Branch_0_Conv2d_1x1 False\n",
      "Block17_7_Branch_7_Conv2d_0c_7x1 False\n",
      "Block17_7_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_7_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_7_Branch_7_Conv2d_0c_7x1_Activation False\n",
      "Block17_7_Concatenate False\n",
      "Block17_7_Conv2d_1x1 False\n",
      "lambda_11 False\n",
      "add_11 False\n",
      "Block17_7_Activation False\n",
      "Block17_8_Branch_8_Conv2d_0a_1x1 False\n",
      "Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_8_Branch_8_Conv2d_0a_1x1_Activation False\n",
      "Block17_8_Branch_8_Conv2d_0b_1x7 False\n",
      "Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_8_Branch_8_Conv2d_0b_1x7_Activation False\n",
      "Block17_8_Branch_0_Conv2d_1x1 False\n",
      "Block17_8_Branch_8_Conv2d_0c_7x1 False\n",
      "Block17_8_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_8_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_8_Branch_8_Conv2d_0c_7x1_Activation False\n",
      "Block17_8_Concatenate False\n",
      "Block17_8_Conv2d_1x1 False\n",
      "lambda_12 False\n",
      "add_12 False\n",
      "Block17_8_Activation False\n",
      "Block17_9_Branch_9_Conv2d_0a_1x1 False\n",
      "Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_9_Branch_9_Conv2d_0a_1x1_Activation False\n",
      "Block17_9_Branch_9_Conv2d_0b_1x7 False\n",
      "Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_9_Branch_9_Conv2d_0b_1x7_Activation False\n",
      "Block17_9_Branch_0_Conv2d_1x1 False\n",
      "Block17_9_Branch_9_Conv2d_0c_7x1 False\n",
      "Block17_9_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_9_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_9_Branch_9_Conv2d_0c_7x1_Activation False\n",
      "Block17_9_Concatenate False\n",
      "Block17_9_Conv2d_1x1 False\n",
      "lambda_13 False\n",
      "add_13 False\n",
      "Block17_9_Activation False\n",
      "Block17_10_Branch_10_Conv2d_0a_1x1 False\n",
      "Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm False\n",
      "Block17_10_Branch_10_Conv2d_0a_1x1_Activation False\n",
      "Block17_10_Branch_10_Conv2d_0b_1x7 False\n",
      "Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm False\n",
      "Block17_10_Branch_10_Conv2d_0b_1x7_Activation False\n",
      "Block17_10_Branch_0_Conv2d_1x1 False\n",
      "Block17_10_Branch_10_Conv2d_0c_7x1 False\n",
      "Block17_10_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm False\n",
      "Block17_10_Branch_0_Conv2d_1x1_Activation False\n",
      "Block17_10_Branch_10_Conv2d_0c_7x1_Activation False\n",
      "Block17_10_Concatenate False\n",
      "Block17_10_Conv2d_1x1 False\n",
      "lambda_14 False\n",
      "add_14 False\n",
      "Block17_10_Activation False\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1 False\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1 False\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1 False\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3 False\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm False\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm False\n",
      "Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation False\n",
      "Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation False\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3 False\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3 False\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3 False\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm False\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm False\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm False\n",
      "Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation False\n",
      "Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation False\n",
      "Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation False\n",
      "Mixed_7a_Branch_3_MaxPool_1a_3x3 False\n",
      "Mixed_7a False\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1 False\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block8_1_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3 False\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm False\n",
      "Block8_1_Branch_1_Conv2d_0b_1x3_Activation False\n",
      "Block8_1_Branch_0_Conv2d_1x1 False\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1 False\n",
      "Block8_1_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm False\n",
      "Block8_1_Branch_0_Conv2d_1x1_Activation False\n",
      "Block8_1_Branch_1_Conv2d_0c_3x1_Activation False\n",
      "Block8_1_Concatenate False\n",
      "Block8_1_Conv2d_1x1 False\n",
      "lambda_15 False\n",
      "add_15 False\n",
      "Block8_1_Activation False\n",
      "Block8_2_Branch_2_Conv2d_0a_1x1 False\n",
      "Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm False\n",
      "Block8_2_Branch_2_Conv2d_0a_1x1_Activation False\n",
      "Block8_2_Branch_2_Conv2d_0b_1x3 False\n",
      "Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm False\n",
      "Block8_2_Branch_2_Conv2d_0b_1x3_Activation False\n",
      "Block8_2_Branch_0_Conv2d_1x1 False\n",
      "Block8_2_Branch_2_Conv2d_0c_3x1 False\n",
      "Block8_2_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm False\n",
      "Block8_2_Branch_0_Conv2d_1x1_Activation False\n",
      "Block8_2_Branch_2_Conv2d_0c_3x1_Activation False\n",
      "Block8_2_Concatenate False\n",
      "Block8_2_Conv2d_1x1 False\n",
      "lambda_16 False\n",
      "add_16 False\n",
      "Block8_2_Activation False\n",
      "Block8_3_Branch_3_Conv2d_0a_1x1 False\n",
      "Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm False\n",
      "Block8_3_Branch_3_Conv2d_0a_1x1_Activation False\n",
      "Block8_3_Branch_3_Conv2d_0b_1x3 False\n",
      "Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm False\n",
      "Block8_3_Branch_3_Conv2d_0b_1x3_Activation False\n",
      "Block8_3_Branch_0_Conv2d_1x1 False\n",
      "Block8_3_Branch_3_Conv2d_0c_3x1 False\n",
      "Block8_3_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm False\n",
      "Block8_3_Branch_0_Conv2d_1x1_Activation False\n",
      "Block8_3_Branch_3_Conv2d_0c_3x1_Activation False\n",
      "Block8_3_Concatenate False\n",
      "Block8_3_Conv2d_1x1 False\n",
      "lambda_17 False\n",
      "add_17 False\n",
      "Block8_3_Activation False\n",
      "Block8_4_Branch_4_Conv2d_0a_1x1 False\n",
      "Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm False\n",
      "Block8_4_Branch_4_Conv2d_0a_1x1_Activation False\n",
      "Block8_4_Branch_4_Conv2d_0b_1x3 False\n",
      "Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm False\n",
      "Block8_4_Branch_4_Conv2d_0b_1x3_Activation False\n",
      "Block8_4_Branch_0_Conv2d_1x1 False\n",
      "Block8_4_Branch_4_Conv2d_0c_3x1 False\n",
      "Block8_4_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm False\n",
      "Block8_4_Branch_0_Conv2d_1x1_Activation False\n",
      "Block8_4_Branch_4_Conv2d_0c_3x1_Activation False\n",
      "Block8_4_Concatenate False\n",
      "Block8_4_Conv2d_1x1 False\n",
      "lambda_18 False\n",
      "add_18 False\n",
      "Block8_4_Activation False\n",
      "Block8_5_Branch_5_Conv2d_0a_1x1 False\n",
      "Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm False\n",
      "Block8_5_Branch_5_Conv2d_0a_1x1_Activation False\n",
      "Block8_5_Branch_5_Conv2d_0b_1x3 False\n",
      "Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm False\n",
      "Block8_5_Branch_5_Conv2d_0b_1x3_Activation False\n",
      "Block8_5_Branch_0_Conv2d_1x1 False\n",
      "Block8_5_Branch_5_Conv2d_0c_3x1 False\n",
      "Block8_5_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm False\n",
      "Block8_5_Branch_0_Conv2d_1x1_Activation False\n",
      "Block8_5_Branch_5_Conv2d_0c_3x1_Activation False\n",
      "Block8_5_Concatenate False\n",
      "Block8_5_Conv2d_1x1 False\n",
      "lambda_19 False\n",
      "add_19 False\n",
      "Block8_5_Activation False\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1 False\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm False\n",
      "Block8_6_Branch_1_Conv2d_0a_1x1_Activation False\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3 False\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm False\n",
      "Block8_6_Branch_1_Conv2d_0b_1x3_Activation False\n",
      "Block8_6_Branch_0_Conv2d_1x1 False\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1 False\n",
      "Block8_6_Branch_0_Conv2d_1x1_BatchNorm False\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm False\n",
      "Block8_6_Branch_0_Conv2d_1x1_Activation False\n",
      "Block8_6_Branch_1_Conv2d_0c_3x1_Activation False\n",
      "Block8_6_Concatenate False\n",
      "Block8_6_Conv2d_1x1 False\n",
      "lambda_20 False\n",
      "add_20 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 3, 3, 1792)   22578384    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1792)         0           model_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1792)         0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 1792)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 1792)         0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 1792)         0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1792)         0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1792)         0           layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 1792)         0           subtract[0][0]                   \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 1792)         0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 1792)         0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5376)         0           multiply[0][0]                   \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           172064      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          4224        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           1056        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            33          dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 22,759,889\n",
      "Trainable params: 181,505\n",
      "Non-trainable params: 22,578,384\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 05:40:24.980620: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-08-11 05:40:25.003015: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n",
      "2021-08-11 05:40:32.401207: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-11 05:40:32.609550: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 40s 103ms/step - loss: 0.6827 - acc: 0.6379 - val_loss: 0.8419 - val_acc: 0.5127\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.51266, saving model to /root/KinshipRecognition/log/model/ensemble_deepface_Facenet_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 28s 93ms/step - loss: 0.6393 - acc: 0.6923 - val_loss: 0.7625 - val_acc: 0.5412\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.51266 to 0.54125, saving model to /root/KinshipRecognition/log/model/ensemble_deepface_Facenet_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 28s 93ms/step - loss: 0.6387 - acc: 0.6773 - val_loss: 0.7804 - val_acc: 0.5289\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.54125\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 27s 89ms/step - loss: 0.6226 - acc: 0.6970 - val_loss: 0.7685 - val_acc: 0.5320\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.54125\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 27s 89ms/step - loss: 0.6190 - acc: 0.6866 - val_loss: 0.7181 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.54125 to 0.57891, saving model to /root/KinshipRecognition/log/model/ensemble_deepface_Facenet_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 28s 92ms/step - loss: 0.6148 - acc: 0.6970 - val_loss: 0.7420 - val_acc: 0.5483\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.57891\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 28s 93ms/step - loss: 0.6235 - acc: 0.6916 - val_loss: 0.8285 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.57891\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 27s 89ms/step - loss: 0.6018 - acc: 0.7070 - val_loss: 0.7342 - val_acc: 0.5594\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.57891\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 26s 88ms/step - loss: 0.6131 - acc: 0.6996 - val_loss: 0.7582 - val_acc: 0.5439\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.57891\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 27s 90ms/step - loss: 0.5838 - acc: 0.7207 - val_loss: 0.7500 - val_acc: 0.5484\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.57891\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 27s 89ms/step - loss: 0.5835 - acc: 0.7210 - val_loss: 0.7760 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.57891\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 26s 88ms/step - loss: 0.5927 - acc: 0.7095 - val_loss: 0.8000 - val_acc: 0.5477\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.57891\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 27s 89ms/step - loss: 0.5913 - acc: 0.7113 - val_loss: 0.7424 - val_acc: 0.5694\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.57891\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 26s 88ms/step - loss: 0.5838 - acc: 0.7155 - val_loss: 0.7200 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.57891\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 27s 89ms/step - loss: 0.5514 - acc: 0.7459 - val_loss: 0.7804 - val_acc: 0.5523\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.57891\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 26s 88ms/step - loss: 0.5543 - acc: 0.7369 - val_loss: 0.7737 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.57891\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 26s 87ms/step - loss: 0.5815 - acc: 0.7067 - val_loss: 0.7683 - val_acc: 0.5628\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.57891\n",
      "Epoch 18/25\n",
      "206/300 [===================>..........] - ETA: 5s - loss: 0.5479 - acc: 0.7358"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31254/2307462477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFINE_TUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     history = model.fit(gen(train, train_person_to_images_map, INPUT_SHAPE, batch_size=16), \n\u001b[0m\u001b[1;32m     16\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_person_to_images_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "\n",
    "    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_families_list[i])\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.3, patience=30, verbose=1)\n",
    "    callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    \n",
    "    history = model.fit(gen(train, train_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        validation_data=gen(val, val_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        epochs=25, steps_per_epoch=300, validation_steps=200,\n",
    "                        verbose=1, callbacks=callbacks_list, \n",
    "                        use_multiprocessing=False, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = f\"{BASE_PATH}/data/test/\"\n",
    "submission = pd.read_csv(f'{BASE_PATH}/data/test_ds.csv')\n",
    "preds_for_sub = np.zeros(submission.shape[0])\n",
    "all_preds = list()\n",
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = []\n",
    "    for j in range(0, len(submission.p1.values), 32):\n",
    "        X1 = submission.p1.values[j:j+32]\n",
    "        X1 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X1])\n",
    "\n",
    "        X2 = submission.p2.values[j:j+32]\n",
    "        X2 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X2])\n",
    "\n",
    "        pred = model.predict([X1, X2]).ravel().tolist()\n",
    "        predictions += pred    \n",
    "    \n",
    "    all_preds.append(np.array(predictions))\n",
    "    preds_for_sub += np.array(predictions) / len(val_families_list)\n",
    "\n",
    "    \n",
    "all_preds = np.asarray(all_preds)\n",
    "submission['score'] = preds_for_sub\n",
    "pd.DataFrame(all_preds).to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}_allpreds.csv\", index=False)\n",
    "submission.to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(pred_for_sub <= 0.5))\n",
    "print(len(pred_for_sub) + '\\n')\n",
    "for line in preds_for_sub:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
