{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be203aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, LayerNormalization, BatchNormalization, Layer, Conv2D, InputSpec\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "from tf2_keras_vggface.utils import preprocess_input\n",
    "from tf2_keras_vggface.vggface import VGGFace\n",
    "\n",
    "import os\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd4a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BASE_MODEL = 'vgg16'\n",
    "INPUT_SHAPE = (224, 224,)\n",
    "\n",
    "# Modify paths as per your method of saving them\n",
    "BASE_PATH = \"/root/KinshipRecognition\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_PATH}/data/aug_train_ds.csv\"\n",
    "TRAIN_FOLDERS_PATH = f\"{BASE_PATH}/data/train/train-faces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebb08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path, input_shape):\n",
    "    img = cv2.imread(path, -1)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = cv2.normalize(img,  np.zeros(img.shape[:2]), 0, 255, cv2.NORM_MINMAX)\n",
    "    return np.array(img).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ede33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "backbone = VGGFace(model=BASE_MODEL, include_top=False)\n",
    "backbone.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a358a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample image\n",
    "sample_img = glob(F\"{TRAIN_FOLDERS_PATH}/*/*/*\")[1250]\n",
    "sample_img = read_img(sample_img, INPUT_SHAPE)\n",
    "sample_img = tf.Variable(np.expand_dims(sample_img, axis=0), dtype=tf.float32, trainable=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output for layer 1\n",
    "inputs = Input(shape=INPUT_SHAPE + (3,))\n",
    "seq = Sequential()\n",
    "seq.add(inputs)\n",
    "for l in backbone.layers[:12]:\n",
    "    seq.add(l)\n",
    "out = seq(sample_img)\n",
    "print(out.shape)\n",
    "ix = 1\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax = plt.subplot(4, 4, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(out[0, :, :, 32+ix-1], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5576d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionKernel(Layer):\n",
    "    \n",
    "    def __init__(self, units, kernel_dim2D, key_dim, \n",
    "                 query_dim=None, value_dim=None, output_dim=None,\n",
    "                 kernel_initializer=\"glorot_uniform\", kernel_regularizer='l2', **kwargs):\n",
    "    \n",
    "        super(SelfAttentionKernel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.units = units # Same as num_output_channels\n",
    "        self.key_dim = key_dim\n",
    "        self.query_dim = query_dim if query_dim else key_dim\n",
    "        self.value_dim = value_dim if value_dim else key_dim\n",
    "        self.output_dim = output_dim if output_dim else key_dim\n",
    "        assert len(kernel_dim.shape) == 2\n",
    "        self.kernel_dim2D = kernel_dim\n",
    "        self.kernel_len = kernel_dim[0] * kernel_dim[1]\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        \n",
    "        self.key_w = None\n",
    "        self.query_w = None\n",
    "        self.value_w = None\n",
    "        self.scale = self.key_dim ** -0.5\n",
    "        self.norm = LayerNormalization(axis=-1, epsilon=0.001, center=False, scale=False)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.query_w = self.add_weight(shape=(self.units, self.query_dim, self.output_dim),\n",
    "                                       initializer=self.kernel_initializer,\n",
    "                                       regularizer=self.kernel_regularizer,\n",
    "                                       trainable=True)\n",
    "        self.key_w = self.add_weight(shape=(self.units, self.key_dim, self.output_dim),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     trainable=True)\n",
    "        self.value_w = self.add_weight(shape=(self.units, self.value_dim, self.kernel_len),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, query, value, key=None):    \n",
    "        \n",
    "        key = key if key else value\n",
    "        # Required for self-attention\n",
    "        for i in range(len(query.shape)):\n",
    "            assert query.shape[i] == key.shape[i]\n",
    "            assert query.shape[i] == value.shape[i]\n",
    "        \n",
    "        qW = tf.matmul(self.query_w, tf.expand_dims(query, 1))\n",
    "        kW = tf.matmul(self.key_w, tf.expand_dims(key, 1))\n",
    "        dot = tf.matmul(qW, tf.transpose(kW, perm=(0, 1, 3, 2,)))\n",
    "        attn_w = tf.nn.softmax(dot * self.scale)\n",
    "        vW = tf.matmul(self.value_w, tf.expand_dims(value, 1))\n",
    "        flat_kernel = self.norm(tf.einsum('ijkl, ijlm -> ijkm', attn_w, vW))\n",
    "        \n",
    "        # (batch, heads/out_channels, input_channels(512) ,kernel_size (a, b))\n",
    "        \n",
    "        return kernel\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88d4e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import nn_ops\n",
    "import functools\n",
    "\n",
    "class FixedConv2D(Conv2D):\n",
    "\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 groups=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(FixedConv2D, self).__init__(filters,\n",
    "                                          kernel_size,\n",
    "                                          strides=(1, 1),\n",
    "                                          padding='valid',\n",
    "                                          data_format=None,\n",
    "                                          dilation_rate=(1, 1),\n",
    "                                          groups=1,\n",
    "                                          activation=None,\n",
    "                                          use_bias=True,\n",
    "                                          kernel_initializer='glorot_uniform',\n",
    "                                          bias_initializer='zeros',\n",
    "                                          kernel_regularizer=None,\n",
    "                                          bias_regularizer=None,\n",
    "                                          activity_regularizer=None,\n",
    "                                          kernel_constraint=None,\n",
    "                                          bias_constraint=None,\n",
    "                                          **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        input_channel = self._get_input_channel(input_shape)\n",
    "        if input_channel % self.groups != 0:\n",
    "            raise ValueError(\n",
    "              'The number of input channels must be evenly divisible by the number '\n",
    "              'of groups. Received groups={}, but the input has {} channels '\n",
    "              '(full input shape is {}).'.format(self.groups, input_channel,\n",
    "                                                 input_shape))\n",
    "        kernel_shape = self.kernel_size + (input_channel // self.groups,\n",
    "                                           self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=kernel_shape,\n",
    "            initializer='zeros',\n",
    "            trainable=False,\n",
    "            dtype=self.dtype)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "              name='bias',\n",
    "              shape=(self.filters,),\n",
    "              initializer=self.bias_initializer,\n",
    "              regularizer=self.bias_regularizer,\n",
    "              constraint=self.bias_constraint,\n",
    "              trainable=True,\n",
    "              dtype=self.dtype)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        channel_axis = self._get_channel_axis()\n",
    "        self.input_spec = InputSpec(min_ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_channel})\n",
    "\n",
    "        # Convert Keras formats to TF native formats.\n",
    "        if self.padding == 'causal':\n",
    "            tf_padding = 'VALID'  # Causal padding handled in `call`.\n",
    "        elif isinstance(self.padding, str):\n",
    "            tf_padding = self.padding.upper()\n",
    "        else:\n",
    "            tf_padding = self.padding\n",
    "        tf_dilations = list(self.dilation_rate)\n",
    "        tf_strides = list(self.strides)\n",
    "\n",
    "        tf_op_name = self.__class__.__name__\n",
    "        if tf_op_name == 'Conv1D':\n",
    "            tf_op_name = 'conv1d'  # Backwards compat.\n",
    "\n",
    "        self._convolution_op = functools.partial(\n",
    "            nn_ops.convolution_v2,\n",
    "            strides=tf_strides,\n",
    "            padding=tf_padding,\n",
    "            dilations=tf_dilations,\n",
    "            data_format=self._tf_data_format,\n",
    "            name=tf_op_name)\n",
    "        self.built = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28bcb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = FixedConv2D(2, (4, 4), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fece233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.build((224, 224, 3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f5d9d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weights[0].trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "699616b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 4, 3, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945047ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
