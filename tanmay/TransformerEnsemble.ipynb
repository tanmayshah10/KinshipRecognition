{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/rcmalli/keras-vggface.git\n",
    "# !pip install keras_vggface\n",
    "# !pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 19:11:51.870492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VGGFace compatible with TensorFlow2.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, LayerNormalization, BatchNormalization, Layer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "from tf2_keras_vggface.utils import preprocess_input\n",
    "from tf2_keras_vggface.vggface import VGGFace\n",
    "\n",
    "from tensorflow.python.ops import nn_ops\n",
    "import functools\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BASE_MODEL = 'vgg16'\n",
    "INPUT_SHAPE = (224, 224,)\n",
    "IGNORE_BOTTOM_NLAYERS_TUNE = -2 #senet50: -6; resnet50:-5; vgg16: -2\n",
    "IGNORE_TOP_NLAYERS_TUNE = 0\n",
    "FINE_TUNE = False\n",
    "EPOCHS = 25\n",
    "\n",
    "# Modify paths as per your method of saving them\n",
    "BASE_PATH = \"/root/KinshipRecognition\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_PATH}/data/aug_train_ds.csv\"\n",
    "TRAIN_FOLDERS_PATH = f\"{BASE_PATH}/data/train/train-faces/\"\n",
    "\n",
    "# Output file\n",
    "MODEL_NAME = f\"ensemble_vggface_{BASE_MODEL}_finetune8_dense32-128-32_drop05\"\n",
    "\n",
    "# All images belonging to families F09** will be used to create the validation set while training the model\n",
    "# For final submission, you can add these to the training data as well\n",
    "val_families_list = [\"F06\"]\n",
    "# val_families_list = [\"F02\",\"F04\",\"F06\",\"F08\", \"F09\"]\n",
    "# val_families_list = [\"F00\", \"F01\", \"F02\", \"F03\", \"F04\", \"F05\", \"F06\", \"F07\", \"F08\", \"F09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val(family_name):\n",
    "\n",
    "    val_families = family_name\n",
    "\n",
    "    all_images = glob(TRAIN_FOLDERS_PATH + \"*/*/*.jpg\")\n",
    "    train_images = [x for x in all_images if val_families not in x]\n",
    "    val_images = [x for x in all_images if val_families in x]\n",
    "\n",
    "    train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "    for x in train_images:\n",
    "        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "    val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    for x in val_images:\n",
    "        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "    relationships = pd.read_csv(TRAIN_FILE_PATH)\n",
    "    relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
    "    relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]    \n",
    "\n",
    "    train = [x for x in relationships if val_families not in x[0]]\n",
    "    val = [x for x in relationships if val_families in x[0]]\n",
    "    return train, val, train_person_to_images_map, val_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path, input_shape):\n",
    "    img = cv2.imread(path, -1)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = cv2.normalize(img,  np.zeros(img.shape[:2]), 0, 255, cv2.NORM_MINMAX)\n",
    "    return np.array(img).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(list_tuples, person_to_images_map, input_shape, batch_size=16, normalization='base'):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "            labels.append(tup[2])\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Original images preprocessed\n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X1])\n",
    "        \n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X2])\n",
    "        \n",
    "        # Mirrored images\n",
    "        X1_mirror = np.asarray([cv2.flip(x, 1) for x in X1])\n",
    "        X2_mirror = np.asarray([cv2.flip(x, 1) for x in X2])\n",
    "        X1 = np.r_[X1, X1_mirror]\n",
    "        X2 = np.r_[X2, X2_mirror]\n",
    "        \n",
    "        yield [X1, X2], np.r_[labels, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionKernel(Layer):\n",
    "    \n",
    "    def __init__(self, units, kernel_dim2D, value_dim, output_dim=None,\n",
    "                 kernel_initializer=\"glorot_uniform\", kernel_regularizer='l2', **kwargs):\n",
    "    \n",
    "        super(SelfAttentionKernel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.units = units # Same as num_output_channels\n",
    "        self.value_dim = value_dim\n",
    "        assert len(kernel_dim2D) == 2\n",
    "        self.kernel_dim2D = kernel_dim2D\n",
    "        self.kernel_len = kernel_dim2D[0] * kernel_dim2D[1]\n",
    "        self.output_dim = output_dim if output_dim else kernel_len\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        \n",
    "        self.key_w = None\n",
    "        self.query_w = None\n",
    "        self.value_w = None\n",
    "        self.scale = self.value_dim ** -0.5\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.query_w = self.add_weight(shape=(self.units, self.value_dim, self.output_dim),\n",
    "                                       initializer=self.kernel_initializer,\n",
    "                                       regularizer=self.kernel_regularizer,\n",
    "                                       trainable=True)\n",
    "        self.key_w = self.add_weight(shape=(self.units, self.value_dim, self.output_dim),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     trainable=True)\n",
    "        self.value_w = self.add_weight(shape=(self.units, self.value_dim, self.kernel_len),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, value):    \n",
    "        \n",
    "        # Query, values, and keys are of shape=(N, h, w, c)\n",
    "        \n",
    "        N, h, w, c = value.shape\n",
    "        value = tf.transpose(value, perm=(0, 3, 1, 2))\n",
    "        value = tf.reshape(value, shape=(N, c, h*w))\n",
    "        \n",
    "        qW = tf.matmul(tf.expand_dims(value, 1), self.query_w)\n",
    "        kW = tf.matmul(tf.expand_dims(value, 1), self.key_w)\n",
    "        dot = tf.matmul(qW, tf.transpose(kW, perm=(0, 1, 3, 2,)))\n",
    "        attn_w = tf.nn.softmax(dot * self.scale)\n",
    "        vW = tf.matmul(tf.expand_dims(value, 1), self.value_w)\n",
    "        flat_kernel = tf.einsum('ijkl, ijlm -> ijkm', attn_w, vW)\n",
    "        kernel = tf.reshape(flat_kernel, shape=(N, self.units, c, self.kernel_dim2D[0], self.kernel_dim2D[1]))\n",
    "        kernel = tf.transpose(kernel, perm=(0, 3, 4, 2, 1))\n",
    "        return kernel\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionConv2D(Layer):\n",
    "    def __init__(self, strides=[1, 1, 1, 1], padding='VALID',\n",
    "                 data_format=None, dilations=None, **kwargs):\n",
    "\n",
    "        super(CrossAttentionConv2D, self).__init__(**kwargs)\n",
    "        \n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.dilations = dilations\n",
    "        \n",
    "    def call(self, input_img, kernel):\n",
    "        \n",
    "        # Match batch and channels dimensions\n",
    "        assert kernel.shape[0] == input_img.shape[0]\n",
    "        assert kernel.shape[3] == input_img.shape[3]\n",
    "\n",
    "        n, h, w, in_c = input_img.shape\n",
    "        _, k_h, k_w, _, out_c = kernel.shape\n",
    "        \n",
    "        # Kernel shape -> (h, w, n*in_c, out_c)\n",
    "        kernel = tf.transpose(kernel, perm=(1, 2, 0, 3, 4))\n",
    "        kernel = tf.reshape(kernel, shape=(k_h, k_w, n*in_c, out_c))        \n",
    "        # Inputs shape -> (h, w, n*in_c)\n",
    "        input_img = tf.transpose(input_img, perm=(1, 2, 0, 3))\n",
    "        input_img = tf.reshape(input_img, shape=(1, h, w, n*in_c))        \n",
    "        # Do depth-wise convolution\n",
    "        conv_out = tf.nn.depthwise_conv2d(input_img, kernel, self.strides, self.padding, self.dilations)\n",
    "        # Output shape -> (n, out_h, out_w, out_c)\n",
    "        _, out_h, out_w = conv_out.shape[:3]\n",
    "        conv_out = tf.reshape(conv_out, shape=(out_h, out_w, n, in_c, out_c))\n",
    "        conv_out = tf.transpose(conv_out, perm=(2, 0, 1, 3, 4))\n",
    "        conv_out = tf.reduce_sum(conv_out, axis=3)\n",
    "        \n",
    "        return conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransformer(Layer):\n",
    "\n",
    "    def __init__(self, units, value_dim, kernel_dim2D, out_dim2D, out_channels=None, \n",
    "                 strides=[1, 1, 1, 1], padding='VALID', dense_config=None,\n",
    "                 kernel_initializer=\"glorot_uniform\", kernel_regularizer='l2',\n",
    "                 data_format=None, dilations=None, **kwargs):\n",
    "        \n",
    "        # value_dim = H*W\n",
    "        \n",
    "        super(ConvTransformer, self).__init__(**kwargs)\n",
    "        self.value_dim = value_dim\n",
    "        self.out_dim2D = out_dim2D\n",
    "        \n",
    "        self.norm = LayerNormalization(axis=[0, 1], epsilon=.0001, center=False, scale=False)\n",
    "        self.self_attn = SelfAttentionKernel(out_channels, (3, 3), value_dim, output_dim=out_channels)\n",
    "        self.cross_conv = CrossAttentionConv2D(strides=[1, 1, 1, 1], padding=padding)\n",
    "        self.dense_block = Sequential()\n",
    "        if dense_config:\n",
    "            for i in dense_config:\n",
    "                self.dense_block.add(Dense(i[0], activation=i[1]))\n",
    "        self.dense_block.add(Dense(out_dim2D[0] * out_dim2D[1], activation='linear'))\n",
    "        \n",
    "    def call(self, x1, x2):        \n",
    "        \n",
    "        assert x1.shape == x2.shape\n",
    "        \n",
    "        N, h, w, in_c = x1.shape\n",
    "        # Normalize input images by (H,W)\n",
    "        x1 = self.norm(x1)\n",
    "        x2 = self.norm(x2)\n",
    "        # Output from cross attention\n",
    "        conv_x1 = self.norm(self.cross_conv(x2, self.self_attn(x1)))\n",
    "        conv_x2 = self.norm(self.cross_conv(x1, self.self_attn(x2)))\n",
    "        \n",
    "        N, h, w, out_c = conv_x1.shape\n",
    "        # Flatten (H, W) dimension\n",
    "        conv_x1 = tf.transpose(conv_x1, perm=(0, 3, 1, 2))\n",
    "        conv_x2 = tf.transpose(conv_x2, perm=(0, 3, 1, 2))\n",
    "        conv_x1 = tf.reshape(conv_x1, shape=(N, out_c, h*w))\n",
    "        conv_x2 = tf.reshape(conv_x2, shape=(N, out_c, h*w))\n",
    "        # Feed into dense network\n",
    "        out_x1 = self.dense_block(conv_x1)\n",
    "        out_x2 = self.dense_block(conv_x2)\n",
    "        # Reshape to shape of image.\n",
    "        out_x1 = tf.reshape(out_x1, shape=(N, out_c, self.out_dim2D[0], self.out_dim2D[1]))\n",
    "        out_x2 = tf.reshape(out_x2, shape=(N, out_c, self.out_dim2D[0], self.out_dim2D[1]))\n",
    "        out_x1 = tf.transpose(out_x1, perm=(0, 2, 3, 1))        \n",
    "        out_x2 = tf.transpose(out_x2, perm=(0, 2, 3, 1))\n",
    "        out_x1 = self.norm(out_x1)\n",
    "        out_x2 = self.norm(out_x2)\n",
    "        \n",
    "        return out_x1, out_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(model_name, fine_tune=True):\n",
    "    input_1 = Input(shape=INPUT_SHAPE + (3,))\n",
    "    input_2 = Input(shape=INPUT_SHAPE + (3,))\n",
    "\n",
    "    backbone = VGGFace(model=model_name, include_top=False)\n",
    "    for x in backbone.layers:\n",
    "        x.trainable = False\n",
    "\n",
    "    if fine_tune:\n",
    "        for x in backbone.layers[:IGNORE_BOTTOM_NLAYERS_TUNE]:\n",
    "            x.trainable = False\n",
    "        if IGNORE_TOP_NLAYERS_TUNE == 0:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:]:\n",
    "                x.trainable = True\n",
    "        else:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:-IGNORE_TOP_NLAYERS_TUNE]:\n",
    "                x.trainable = True\n",
    "\n",
    "    for x in backbone.layers:\n",
    "        print(x.name, x.trainable)\n",
    "\n",
    "    x1 = backbone(input_1)\n",
    "    x2 = backbone(input_2)\n",
    "\n",
    "    conv_transformer = ConvTransformer(256, 49, (2, 2), (4, 4), out_channels=256, \n",
    "                                       strides=[1, 1, 1, 1], padding='SAME', \n",
    "                                       dense_config=((1024, 'relu',),))\n",
    "    x1, x2 = conv_transformer(x1, x2)\n",
    "    x = Concatenate(axis=-1)([Flatten(x1), Flatten(x2)])\n",
    "        \n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=True)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(32, activation=\"tanh\")(x)\n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=False)(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    out = Dense(1, kernel_regularizer=L2(.01), activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00002))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "\n",
    "    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_families_list[i])\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.3, patience=30, verbose=1)\n",
    "    callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    \n",
    "    history = model.fit(gen(train, train_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        validation_data=gen(val, val_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        epochs=EPOCHS, steps_per_epoch=300, validation_steps=200,\n",
    "                        verbose=1, callbacks=callbacks_list, \n",
    "                        use_multiprocessing=False, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = f\"{BASE_PATH}/data/test/\"\n",
    "submission = pd.read_csv(f'{BASE_PATH}/data/test_ds.csv')\n",
    "preds_for_sub = np.zeros(submission.shape[0])\n",
    "all_preds = list()\n",
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = []\n",
    "    for j in range(0, len(submission.p1.values), 32):\n",
    "        X1 = submission.p1.values[j:j+32]\n",
    "        X1 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X1])\n",
    "\n",
    "        X2 = submission.p2.values[j:j+32]\n",
    "        X2 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X2])\n",
    "\n",
    "        pred = model.predict([X1, X2]).ravel().tolist()\n",
    "        predictions += pred    \n",
    "    \n",
    "    all_preds.append(np.array(predictions))\n",
    "    preds_for_sub += np.array(predictions) / len(val_families_list)\n",
    "\n",
    "    \n",
    "all_preds = np.asarray(all_preds).T\n",
    "submission['score'] = preds_for_sub\n",
    "pd.DataFrame(all_preds).to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}_allpreds.csv\", index=False)\n",
    "submission.to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds_for_sub <= 0.5))\n",
    "print(len(preds_for_sub), '\\n')\n",
    "for line in preds_for_sub:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
