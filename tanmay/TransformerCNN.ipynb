{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/rcmalli/keras-vggface.git\n",
    "# !pip install keras_vggface\n",
    "# !pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-09 18:14:35.970063: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using VGGFace compatible with TensorFlow2.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "from tf2_keras_vggface.utils import preprocess_input\n",
    "from tf2_keras_vggface.vggface import VGGFace\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BASE_MODEL = 'senet50'\n",
    "INPUT_SHAPE = (224, 224,)\n",
    "IGNORE_BOTTOM_NLAYERS_TUNE = 1\n",
    "IGNORE_TOP_NLAYERS_TUNE = 1\n",
    "FINE_TUNE = False\n",
    "\n",
    "# Modify paths as per your method of saving them\n",
    "BASE_PATH = \"/root/KinshipRecognition\"\n",
    "TRAIN_FILE_PATH = f\"{BASE_PATH}/data/aug_train_ds.csv\"\n",
    "TRAIN_FOLDERS_PATH = f\"{BASE_PATH}/data/train/train-faces/\"\n",
    "\n",
    "# Output file\n",
    "MODEL_NAME = \"ensemble_vggface_senet50_notune_dense32-128-32_drop05\"\n",
    "\n",
    "# All images belonging to families F09** will be used to create the validation set while training the model\n",
    "# For final submission, you can add these to the training data as well\n",
    "# val_families_list = [\"F06\"]\n",
    "val_families_list = [\"F02\",\"F04\",\"F06\",\"F08\", \"F09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val(family_name):\n",
    "\n",
    "    val_families = family_name\n",
    "\n",
    "    all_images = glob(TRAIN_FOLDERS_PATH + \"*/*/*.jpg\")\n",
    "    train_images = [x for x in all_images if val_families not in x]\n",
    "    val_images = [x for x in all_images if val_families in x]\n",
    "\n",
    "    train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "    for x in train_images:\n",
    "        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "    val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "    for x in val_images:\n",
    "        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "    relationships = pd.read_csv(TRAIN_FILE_PATH)\n",
    "    relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
    "    relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]    \n",
    "\n",
    "    train = [x for x in relationships if val_families not in x[0]]\n",
    "    val = [x for x in relationships if val_families in x[0]]\n",
    "    return train, val, train_person_to_images_map, val_person_to_images_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path, input_shape):\n",
    "    img = cv2.imread(path, -1)\n",
    "    img = cv2.resize(img, input_shape)\n",
    "    img = cv2.normalize(img,  np.zeros(img.shape[:2]), 0, 255, cv2.NORM_MINMAX)\n",
    "    return np.array(img).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(list_tuples, person_to_images_map, input_shape, batch_size=16, normalization='base'):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "            labels.append(tup[2])\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Original images preprocessed\n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X1])\n",
    "        \n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2 = np.array([read_img(TRAIN_FOLDERS_PATH + x, input_shape) for x in X2])\n",
    "        \n",
    "        # Mirrored images\n",
    "        X1_mirror = np.asarray([cv2.flip(x, 1) for x in X1])\n",
    "        X2_mirror = np.asarray([cv2.flip(x, 1) for x in X2])\n",
    "        X1 = np.r_[X1, X1_mirror]\n",
    "        X2 = np.r_[X2, X2_mirror]\n",
    "        \n",
    "        yield [X1, X2], np.r_[labels, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(model_name, fine_tune=True):\n",
    "    input_1 = Input(shape=INPUT_SHAPE + (3,))\n",
    "    input_2 = Input(shape=INPUT_SHAPE + (3,))\n",
    "\n",
    "    backbone = VGGFace(model=model_name, include_top=False)\n",
    "    for x in backbone.layers:\n",
    "        x.trainable = False\n",
    "\n",
    "    if fine_tune:\n",
    "        for x in backbone.layers[:IGNORE_BOTTOM_NLAYERS_TUNE]:\n",
    "            x.trainable = False\n",
    "        if IGNORE_TOP_NLAYERS_TUNE == 0:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:]:\n",
    "                x.trainable = True\n",
    "        else:\n",
    "            for x in backbone.layers[IGNORE_BOTTOM_NLAYERS_TUNE:-IGNORE_TOP_NLAYERS_TUNE]:\n",
    "                x.trainable = True\n",
    "\n",
    "    for x in backbone.layers:\n",
    "        print(x.name, x.trainable)\n",
    "\n",
    "    x1 = backbone(input_1)\n",
    "    x2 = backbone(input_2)\n",
    "\n",
    "    x1 = GlobalAvgPool2D()(x1)\n",
    "    x2 = GlobalAvgPool2D()(x2)\n",
    "\n",
    "    x1 = LayerNormalization(axis=-1, epsilon=0.001, center=False, scale=False)(x1)\n",
    "    x2 = LayerNormalization(axis=-1, epsilon=0.001, center=False, scale=False)(x2)\n",
    "\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "    x1_ = Multiply()([x1, x1])\n",
    "    x2_ = Multiply()([x2, x2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    x5 = Multiply()([x1, x2])\n",
    "    x = Concatenate(axis=-1)([x3, x4, x5])\n",
    "        \n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=True)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    x = Dense(32, activation=\"tanh\")(x)\n",
    "#     x = LayerNormalization(axis=-1, epsilon=0.001, center=True, scale=False)(x)\n",
    "    x = Dropout(0.05)(x)    \n",
    "    out = Dense(1, kernel_regularizer=L2(.01), activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 0: Validation on F02\n",
      "##############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-09 18:14:56.924934: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-09 18:14:56.926006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-09 18:14:56.984028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-08-09 18:14:56.984091: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-09 18:14:56.987599: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-09 18:14:56.987709: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-08-09 18:14:56.990186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-09 18:14:56.990574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-09 18:14:56.992731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-09 18:14:56.993862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-09 18:14:56.999240: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-09 18:14:57.003166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-09 18:14:57.003977: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-09 18:14:57.014820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-08-09 18:14:57.014889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-09 18:14:57.014930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-09 18:14:57.014959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-08-09 18:14:57.014986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-09 18:14:57.015013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-09 18:14:57.015041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-09 18:14:57.015068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-09 18:14:57.015096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-09 18:14:57.022666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-08-09 18:14:57.022726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-09 18:14:57.555609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-09 18:14:57.555636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-08-09 18:14:57.555642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-08-09 18:14:57.559547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29704 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)\n",
      "2021-08-09 18:14:57.560093: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_senet50.h5\n",
      "104947712/104944616 [==============================] - 1s 0us/step\n",
      "input_3 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation False\n",
      "max_pooling2d False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_1 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_2 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d False\n",
      "reshape False\n",
      "conv2_1_1x1_down False\n",
      "activation_3 False\n",
      "conv2_1_1x1_up False\n",
      "activation_4 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add False\n",
      "activation_5 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_6 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_7 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_1 False\n",
      "reshape_1 False\n",
      "conv2_2_1x1_down False\n",
      "activation_8 False\n",
      "conv2_2_1x1_up False\n",
      "activation_9 False\n",
      "multiply_1 False\n",
      "add_1 False\n",
      "activation_10 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_11 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_12 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_2 False\n",
      "reshape_2 False\n",
      "conv2_3_1x1_down False\n",
      "activation_13 False\n",
      "conv2_3_1x1_up False\n",
      "activation_14 False\n",
      "multiply_2 False\n",
      "add_2 False\n",
      "activation_15 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_16 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_17 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_3 False\n",
      "reshape_3 False\n",
      "conv3_1_1x1_down False\n",
      "activation_18 False\n",
      "conv3_1_1x1_up False\n",
      "activation_19 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_3 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_3 False\n",
      "activation_20 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_21 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_22 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_4 False\n",
      "reshape_4 False\n",
      "conv3_2_1x1_down False\n",
      "activation_23 False\n",
      "conv3_2_1x1_up False\n",
      "activation_24 False\n",
      "multiply_4 False\n",
      "add_4 False\n",
      "activation_25 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_26 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_27 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_5 False\n",
      "reshape_5 False\n",
      "conv3_3_1x1_down False\n",
      "activation_28 False\n",
      "conv3_3_1x1_up False\n",
      "activation_29 False\n",
      "multiply_5 False\n",
      "add_5 False\n",
      "activation_30 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_31 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_32 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_6 False\n",
      "reshape_6 False\n",
      "conv3_4_1x1_down False\n",
      "activation_33 False\n",
      "conv3_4_1x1_up False\n",
      "activation_34 False\n",
      "multiply_6 False\n",
      "add_6 False\n",
      "activation_35 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_36 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_37 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_7 False\n",
      "reshape_7 False\n",
      "conv4_1_1x1_down False\n",
      "activation_38 False\n",
      "conv4_1_1x1_up False\n",
      "activation_39 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_7 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_7 False\n",
      "activation_40 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_41 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_42 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_8 False\n",
      "reshape_8 False\n",
      "conv4_2_1x1_down False\n",
      "activation_43 False\n",
      "conv4_2_1x1_up False\n",
      "activation_44 False\n",
      "multiply_8 False\n",
      "add_8 False\n",
      "activation_45 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_46 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_47 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_9 False\n",
      "reshape_9 False\n",
      "conv4_3_1x1_down False\n",
      "activation_48 False\n",
      "conv4_3_1x1_up False\n",
      "activation_49 False\n",
      "multiply_9 False\n",
      "add_9 False\n",
      "activation_50 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_51 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_52 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_10 False\n",
      "reshape_10 False\n",
      "conv4_4_1x1_down False\n",
      "activation_53 False\n",
      "conv4_4_1x1_up False\n",
      "activation_54 False\n",
      "multiply_10 False\n",
      "add_10 False\n",
      "activation_55 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_56 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_57 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_11 False\n",
      "reshape_11 False\n",
      "conv4_5_1x1_down False\n",
      "activation_58 False\n",
      "conv4_5_1x1_up False\n",
      "activation_59 False\n",
      "multiply_11 False\n",
      "add_11 False\n",
      "activation_60 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_61 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_62 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_12 False\n",
      "reshape_12 False\n",
      "conv4_6_1x1_down False\n",
      "activation_63 False\n",
      "conv4_6_1x1_up False\n",
      "activation_64 False\n",
      "multiply_12 False\n",
      "add_12 False\n",
      "activation_65 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_66 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_67 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_13 False\n",
      "reshape_13 False\n",
      "conv5_1_1x1_down False\n",
      "activation_68 False\n",
      "conv5_1_1x1_up False\n",
      "activation_69 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_13 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_13 False\n",
      "activation_70 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_71 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_72 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_14 False\n",
      "reshape_14 False\n",
      "conv5_2_1x1_down False\n",
      "activation_73 False\n",
      "conv5_2_1x1_up False\n",
      "activation_74 False\n",
      "multiply_14 False\n",
      "add_14 False\n",
      "activation_75 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_76 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_77 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_15 False\n",
      "reshape_15 False\n",
      "conv5_3_1x1_down False\n",
      "activation_78 False\n",
      "conv5_3_1x1_up False\n",
      "activation_79 False\n",
      "multiply_15 False\n",
      "add_15 False\n",
      "activation_80 False\n",
      "avg_pool False\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_16 (Gl (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_17 (Gl (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 2048)         0           global_average_pooling2d_16[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 2048)         0           global_average_pooling2d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 2048)         0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 2048)         0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 2048)         0           layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 2048)         0           subtract[0][0]                   \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 2048)         0           multiply_17[0][0]                \n",
      "                                                                 multiply_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 2048)         0           layer_normalization[0][0]        \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6144)         0           multiply_16[0][0]                \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 multiply_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           196640      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          4224        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-09 18:15:01.620275: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-08-09 18:15:01.639189: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-09 18:15:06.997825: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-09 18:15:07.202013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 60s 177ms/step - loss: 0.8461 - acc: 0.5015 - val_loss: 0.6640 - val_acc: 0.6575\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65750, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 46s 152ms/step - loss: 0.6702 - acc: 0.6584 - val_loss: 0.6483 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65750 to 0.67547, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.6251 - acc: 0.6916 - val_loss: 0.6185 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67547 to 0.69344, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.5777 - acc: 0.7274 - val_loss: 0.6071 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.69344 to 0.70422, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5598 - acc: 0.7422 - val_loss: 0.5828 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70422 to 0.72438, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5095 - acc: 0.7711 - val_loss: 0.6093 - val_acc: 0.7030\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72438\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4959 - acc: 0.7825 - val_loss: 0.5831 - val_acc: 0.7242\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.72438\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.4779 - acc: 0.7936 - val_loss: 0.5850 - val_acc: 0.7291\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.72438 to 0.72906, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4627 - acc: 0.7972 - val_loss: 0.5824 - val_acc: 0.7220\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.72906\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.4559 - acc: 0.8092 - val_loss: 0.5865 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.72906\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.4319 - acc: 0.8170 - val_loss: 0.5913 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.72906\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.4201 - acc: 0.8298 - val_loss: 0.5788 - val_acc: 0.7259\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.72906\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.4031 - acc: 0.8357 - val_loss: 0.5650 - val_acc: 0.7422\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.72906 to 0.74219, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.4232 - acc: 0.8215 - val_loss: 0.5706 - val_acc: 0.7381\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.74219\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3706 - acc: 0.8503 - val_loss: 0.5753 - val_acc: 0.7337\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.74219\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3929 - acc: 0.8379 - val_loss: 0.5710 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.74219\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3802 - acc: 0.8456 - val_loss: 0.5533 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.74219 to 0.75313, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_0.h5\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3806 - acc: 0.8529 - val_loss: 0.5763 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.75313\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3536 - acc: 0.8629 - val_loss: 0.5700 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.75313\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 45s 151ms/step - loss: 0.3511 - acc: 0.8584 - val_loss: 0.5586 - val_acc: 0.7466\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.75313\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3457 - acc: 0.8627 - val_loss: 0.5682 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.75313\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3560 - acc: 0.8600 - val_loss: 0.5736 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.75313\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3432 - acc: 0.8650 - val_loss: 0.5845 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.75313\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3276 - acc: 0.8745 - val_loss: 0.5820 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.75313\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3269 - acc: 0.8807 - val_loss: 0.5836 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.75313\n",
      "##############################\n",
      "Iteration 1: Validation on F04\n",
      "##############################\n",
      "input_6 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_81 False\n",
      "max_pooling2d_1 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_82 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_83 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_18 False\n",
      "reshape_16 False\n",
      "conv2_1_1x1_down False\n",
      "activation_84 False\n",
      "conv2_1_1x1_up False\n",
      "activation_85 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_20 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_16 False\n",
      "activation_86 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_87 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_88 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_19 False\n",
      "reshape_17 False\n",
      "conv2_2_1x1_down False\n",
      "activation_89 False\n",
      "conv2_2_1x1_up False\n",
      "activation_90 False\n",
      "multiply_21 False\n",
      "add_17 False\n",
      "activation_91 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_92 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_93 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_20 False\n",
      "reshape_18 False\n",
      "conv2_3_1x1_down False\n",
      "activation_94 False\n",
      "conv2_3_1x1_up False\n",
      "activation_95 False\n",
      "multiply_22 False\n",
      "add_18 False\n",
      "activation_96 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_97 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_98 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_21 False\n",
      "reshape_19 False\n",
      "conv3_1_1x1_down False\n",
      "activation_99 False\n",
      "conv3_1_1x1_up False\n",
      "activation_100 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_23 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_19 False\n",
      "activation_101 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_102 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_103 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_22 False\n",
      "reshape_20 False\n",
      "conv3_2_1x1_down False\n",
      "activation_104 False\n",
      "conv3_2_1x1_up False\n",
      "activation_105 False\n",
      "multiply_24 False\n",
      "add_20 False\n",
      "activation_106 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_107 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_108 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_23 False\n",
      "reshape_21 False\n",
      "conv3_3_1x1_down False\n",
      "activation_109 False\n",
      "conv3_3_1x1_up False\n",
      "activation_110 False\n",
      "multiply_25 False\n",
      "add_21 False\n",
      "activation_111 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_112 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_113 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_24 False\n",
      "reshape_22 False\n",
      "conv3_4_1x1_down False\n",
      "activation_114 False\n",
      "conv3_4_1x1_up False\n",
      "activation_115 False\n",
      "multiply_26 False\n",
      "add_22 False\n",
      "activation_116 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_117 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_118 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_25 False\n",
      "reshape_23 False\n",
      "conv4_1_1x1_down False\n",
      "activation_119 False\n",
      "conv4_1_1x1_up False\n",
      "activation_120 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_27 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_23 False\n",
      "activation_121 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_122 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_123 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_26 False\n",
      "reshape_24 False\n",
      "conv4_2_1x1_down False\n",
      "activation_124 False\n",
      "conv4_2_1x1_up False\n",
      "activation_125 False\n",
      "multiply_28 False\n",
      "add_24 False\n",
      "activation_126 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_127 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_128 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_27 False\n",
      "reshape_25 False\n",
      "conv4_3_1x1_down False\n",
      "activation_129 False\n",
      "conv4_3_1x1_up False\n",
      "activation_130 False\n",
      "multiply_29 False\n",
      "add_25 False\n",
      "activation_131 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_132 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_133 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_28 False\n",
      "reshape_26 False\n",
      "conv4_4_1x1_down False\n",
      "activation_134 False\n",
      "conv4_4_1x1_up False\n",
      "activation_135 False\n",
      "multiply_30 False\n",
      "add_26 False\n",
      "activation_136 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_137 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_138 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_29 False\n",
      "reshape_27 False\n",
      "conv4_5_1x1_down False\n",
      "activation_139 False\n",
      "conv4_5_1x1_up False\n",
      "activation_140 False\n",
      "multiply_31 False\n",
      "add_27 False\n",
      "activation_141 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_142 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_143 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_30 False\n",
      "reshape_28 False\n",
      "conv4_6_1x1_down False\n",
      "activation_144 False\n",
      "conv4_6_1x1_up False\n",
      "activation_145 False\n",
      "multiply_32 False\n",
      "add_28 False\n",
      "activation_146 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_147 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_148 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_31 False\n",
      "reshape_29 False\n",
      "conv5_1_1x1_down False\n",
      "activation_149 False\n",
      "conv5_1_1x1_up False\n",
      "activation_150 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_33 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_29 False\n",
      "activation_151 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_152 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_153 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_32 False\n",
      "reshape_30 False\n",
      "conv5_2_1x1_down False\n",
      "activation_154 False\n",
      "conv5_2_1x1_up False\n",
      "activation_155 False\n",
      "multiply_34 False\n",
      "add_30 False\n",
      "activation_156 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_157 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_158 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_33 False\n",
      "reshape_31 False\n",
      "conv5_3_1x1_down False\n",
      "activation_159 False\n",
      "conv5_3_1x1_up False\n",
      "activation_160 False\n",
      "multiply_35 False\n",
      "add_31 False\n",
      "activation_161 False\n",
      "avg_pool False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_34 (Gl (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_35 (Gl (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 2048)         0           global_average_pooling2d_34[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 2048)         0           global_average_pooling2d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 2048)         0           layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_37 (Multiply)          (None, 2048)         0           layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_38 (Multiply)          (None, 2048)         0           layer_normalization_3[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_36 (Multiply)          (None, 2048)         0           subtract_2[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 2048)         0           multiply_37[0][0]                \n",
      "                                                                 multiply_38[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_39 (Multiply)          (None, 2048)         0           layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6144)         0           multiply_36[0][0]                \n",
      "                                                                 subtract_3[0][0]                 \n",
      "                                                                 multiply_39[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           196640      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          4224        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           4128        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 59s 180ms/step - loss: 0.8025 - acc: 0.4975 - val_loss: 0.8060 - val_acc: 0.5192\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.51922, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_1.h5\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.6218 - acc: 0.6869 - val_loss: 0.8399 - val_acc: 0.5023\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.51922\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.5601 - acc: 0.7269 - val_loss: 0.8546 - val_acc: 0.5067\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.51922\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5308 - acc: 0.7539 - val_loss: 0.8551 - val_acc: 0.5133\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.51922\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.5140 - acc: 0.7607 - val_loss: 0.8586 - val_acc: 0.5256\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.51922 to 0.52562, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_1.h5\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4709 - acc: 0.7898 - val_loss: 0.8211 - val_acc: 0.5466\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.52562 to 0.54656, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_1.h5\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.4325 - acc: 0.8160 - val_loss: 0.8731 - val_acc: 0.5489\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.54656 to 0.54891, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_1.h5\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4317 - acc: 0.8106 - val_loss: 0.8812 - val_acc: 0.5442\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.54891\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3926 - acc: 0.8433 - val_loss: 0.8894 - val_acc: 0.5511\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.54891 to 0.55109, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_1.h5\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.4080 - acc: 0.8314 - val_loss: 0.8848 - val_acc: 0.5495\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.55109\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.3806 - acc: 0.8396 - val_loss: 0.9252 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.55109\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3742 - acc: 0.8437 - val_loss: 0.9176 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.55109 to 0.55703, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_1.h5\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 48s 162ms/step - loss: 0.3646 - acc: 0.8492 - val_loss: 0.9301 - val_acc: 0.5502\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.55703\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3513 - acc: 0.8572 - val_loss: 0.9086 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.55703\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3447 - acc: 0.8575 - val_loss: 0.9233 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.55703 to 0.56578, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_1.h5\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3326 - acc: 0.8671 - val_loss: 0.9731 - val_acc: 0.5484\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.56578\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3304 - acc: 0.8662 - val_loss: 0.9923 - val_acc: 0.5491\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.56578\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3322 - acc: 0.8657 - val_loss: 0.9336 - val_acc: 0.5566\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.56578\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.3251 - acc: 0.8732 - val_loss: 0.9559 - val_acc: 0.5539\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.56578\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3109 - acc: 0.8796 - val_loss: 0.9497 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.56578\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3096 - acc: 0.8823 - val_loss: 0.9571 - val_acc: 0.5608\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.56578\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3182 - acc: 0.8675 - val_loss: 1.0208 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.56578\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.2951 - acc: 0.8848 - val_loss: 0.9835 - val_acc: 0.5597\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.56578\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.2969 - acc: 0.8796 - val_loss: 1.0138 - val_acc: 0.5589\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.56578\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.2776 - acc: 0.8910 - val_loss: 1.0406 - val_acc: 0.5414\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.56578\n",
      "##############################\n",
      "Iteration 2: Validation on F06\n",
      "##############################\n",
      "input_9 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_162 False\n",
      "max_pooling2d_2 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_163 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_164 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_36 False\n",
      "reshape_32 False\n",
      "conv2_1_1x1_down False\n",
      "activation_165 False\n",
      "conv2_1_1x1_up False\n",
      "activation_166 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_40 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_32 False\n",
      "activation_167 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_168 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_169 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_37 False\n",
      "reshape_33 False\n",
      "conv2_2_1x1_down False\n",
      "activation_170 False\n",
      "conv2_2_1x1_up False\n",
      "activation_171 False\n",
      "multiply_41 False\n",
      "add_33 False\n",
      "activation_172 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_173 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_174 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_38 False\n",
      "reshape_34 False\n",
      "conv2_3_1x1_down False\n",
      "activation_175 False\n",
      "conv2_3_1x1_up False\n",
      "activation_176 False\n",
      "multiply_42 False\n",
      "add_34 False\n",
      "activation_177 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_178 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_179 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_39 False\n",
      "reshape_35 False\n",
      "conv3_1_1x1_down False\n",
      "activation_180 False\n",
      "conv3_1_1x1_up False\n",
      "activation_181 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_43 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_35 False\n",
      "activation_182 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_183 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_184 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_40 False\n",
      "reshape_36 False\n",
      "conv3_2_1x1_down False\n",
      "activation_185 False\n",
      "conv3_2_1x1_up False\n",
      "activation_186 False\n",
      "multiply_44 False\n",
      "add_36 False\n",
      "activation_187 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_188 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_189 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_41 False\n",
      "reshape_37 False\n",
      "conv3_3_1x1_down False\n",
      "activation_190 False\n",
      "conv3_3_1x1_up False\n",
      "activation_191 False\n",
      "multiply_45 False\n",
      "add_37 False\n",
      "activation_192 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_193 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_194 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_42 False\n",
      "reshape_38 False\n",
      "conv3_4_1x1_down False\n",
      "activation_195 False\n",
      "conv3_4_1x1_up False\n",
      "activation_196 False\n",
      "multiply_46 False\n",
      "add_38 False\n",
      "activation_197 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_198 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_199 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_43 False\n",
      "reshape_39 False\n",
      "conv4_1_1x1_down False\n",
      "activation_200 False\n",
      "conv4_1_1x1_up False\n",
      "activation_201 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_47 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_39 False\n",
      "activation_202 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_203 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_204 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_44 False\n",
      "reshape_40 False\n",
      "conv4_2_1x1_down False\n",
      "activation_205 False\n",
      "conv4_2_1x1_up False\n",
      "activation_206 False\n",
      "multiply_48 False\n",
      "add_40 False\n",
      "activation_207 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_208 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_209 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_45 False\n",
      "reshape_41 False\n",
      "conv4_3_1x1_down False\n",
      "activation_210 False\n",
      "conv4_3_1x1_up False\n",
      "activation_211 False\n",
      "multiply_49 False\n",
      "add_41 False\n",
      "activation_212 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_213 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_214 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_46 False\n",
      "reshape_42 False\n",
      "conv4_4_1x1_down False\n",
      "activation_215 False\n",
      "conv4_4_1x1_up False\n",
      "activation_216 False\n",
      "multiply_50 False\n",
      "add_42 False\n",
      "activation_217 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_218 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_219 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_47 False\n",
      "reshape_43 False\n",
      "conv4_5_1x1_down False\n",
      "activation_220 False\n",
      "conv4_5_1x1_up False\n",
      "activation_221 False\n",
      "multiply_51 False\n",
      "add_43 False\n",
      "activation_222 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_223 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_224 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_48 False\n",
      "reshape_44 False\n",
      "conv4_6_1x1_down False\n",
      "activation_225 False\n",
      "conv4_6_1x1_up False\n",
      "activation_226 False\n",
      "multiply_52 False\n",
      "add_44 False\n",
      "activation_227 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_228 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_229 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_49 False\n",
      "reshape_45 False\n",
      "conv5_1_1x1_down False\n",
      "activation_230 False\n",
      "conv5_1_1x1_up False\n",
      "activation_231 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_53 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_45 False\n",
      "activation_232 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_233 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_234 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_50 False\n",
      "reshape_46 False\n",
      "conv5_2_1x1_down False\n",
      "activation_235 False\n",
      "conv5_2_1x1_up False\n",
      "activation_236 False\n",
      "multiply_54 False\n",
      "add_46 False\n",
      "activation_237 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_238 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_239 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_51 False\n",
      "reshape_47 False\n",
      "conv5_3_1x1_down False\n",
      "activation_240 False\n",
      "conv5_3_1x1_up False\n",
      "activation_241 False\n",
      "multiply_55 False\n",
      "add_47 False\n",
      "activation_242 False\n",
      "avg_pool False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_52 (Gl (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_53 (Gl (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 2048)         0           global_average_pooling2d_52[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 2048)         0           global_average_pooling2d_53[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 2048)         0           layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_57 (Multiply)          (None, 2048)         0           layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_58 (Multiply)          (None, 2048)         0           layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_56 (Multiply)          (None, 2048)         0           subtract_4[0][0]                 \n",
      "                                                                 subtract_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 2048)         0           multiply_57[0][0]                \n",
      "                                                                 multiply_58[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_59 (Multiply)          (None, 2048)         0           layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 6144)         0           multiply_56[0][0]                \n",
      "                                                                 subtract_5[0][0]                 \n",
      "                                                                 multiply_59[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           196640      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          4224        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           4128        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            33          dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 58s 176ms/step - loss: 0.7898 - acc: 0.5037 - val_loss: 0.5960 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71000, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_2.h5\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 49s 164ms/step - loss: 0.6324 - acc: 0.6779 - val_loss: 0.5694 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.71000 to 0.74672, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_2.h5\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 49s 162ms/step - loss: 0.6039 - acc: 0.6899 - val_loss: 0.5809 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.74672\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.5620 - acc: 0.7285 - val_loss: 0.5893 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.74672\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.5243 - acc: 0.7544 - val_loss: 0.5430 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.74672 to 0.75063, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_2.h5\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.5159 - acc: 0.7586 - val_loss: 0.5673 - val_acc: 0.7447\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.75063\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 50s 167ms/step - loss: 0.4859 - acc: 0.7838 - val_loss: 0.5476 - val_acc: 0.7559\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.75063 to 0.75594, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_2.h5\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 50s 165ms/step - loss: 0.4596 - acc: 0.8028 - val_loss: 0.5823 - val_acc: 0.7337\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.75594\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.4397 - acc: 0.8104 - val_loss: 0.5407 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.75594 to 0.76000, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_2.h5\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.4372 - acc: 0.8100 - val_loss: 0.5451 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.76000 to 0.77188, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_2.h5\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.4157 - acc: 0.8231 - val_loss: 0.5439 - val_acc: 0.7581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77188\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3935 - acc: 0.8339 - val_loss: 0.5416 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.77188\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3731 - acc: 0.8464 - val_loss: 0.5592 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.77188\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.3789 - acc: 0.8462 - val_loss: 0.5499 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.77188\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3493 - acc: 0.8592 - val_loss: 0.5593 - val_acc: 0.7614\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.77188\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3501 - acc: 0.8575 - val_loss: 0.5682 - val_acc: 0.7527\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.77188\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3667 - acc: 0.8444 - val_loss: 0.5368 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.77188\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3308 - acc: 0.8707 - val_loss: 0.6056 - val_acc: 0.7441\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.77188\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.3253 - acc: 0.8701 - val_loss: 0.5881 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.77188\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3116 - acc: 0.8819 - val_loss: 0.5749 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.77188\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3235 - acc: 0.8703 - val_loss: 0.6178 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.77188\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3336 - acc: 0.8631 - val_loss: 0.5940 - val_acc: 0.7442\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.77188\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3152 - acc: 0.8787 - val_loss: 0.6070 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.77188\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.3170 - acc: 0.8762 - val_loss: 0.5890 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.77188\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3159 - acc: 0.8700 - val_loss: 0.5765 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.77188\n",
      "##############################\n",
      "Iteration 3: Validation on F08\n",
      "##############################\n",
      "input_12 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_243 False\n",
      "max_pooling2d_3 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_244 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_245 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_54 False\n",
      "reshape_48 False\n",
      "conv2_1_1x1_down False\n",
      "activation_246 False\n",
      "conv2_1_1x1_up False\n",
      "activation_247 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_60 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_48 False\n",
      "activation_248 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_249 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_250 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_55 False\n",
      "reshape_49 False\n",
      "conv2_2_1x1_down False\n",
      "activation_251 False\n",
      "conv2_2_1x1_up False\n",
      "activation_252 False\n",
      "multiply_61 False\n",
      "add_49 False\n",
      "activation_253 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_254 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_255 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_56 False\n",
      "reshape_50 False\n",
      "conv2_3_1x1_down False\n",
      "activation_256 False\n",
      "conv2_3_1x1_up False\n",
      "activation_257 False\n",
      "multiply_62 False\n",
      "add_50 False\n",
      "activation_258 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_259 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_260 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_57 False\n",
      "reshape_51 False\n",
      "conv3_1_1x1_down False\n",
      "activation_261 False\n",
      "conv3_1_1x1_up False\n",
      "activation_262 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_63 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_51 False\n",
      "activation_263 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_264 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_265 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_58 False\n",
      "reshape_52 False\n",
      "conv3_2_1x1_down False\n",
      "activation_266 False\n",
      "conv3_2_1x1_up False\n",
      "activation_267 False\n",
      "multiply_64 False\n",
      "add_52 False\n",
      "activation_268 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_269 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_270 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_59 False\n",
      "reshape_53 False\n",
      "conv3_3_1x1_down False\n",
      "activation_271 False\n",
      "conv3_3_1x1_up False\n",
      "activation_272 False\n",
      "multiply_65 False\n",
      "add_53 False\n",
      "activation_273 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_274 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_275 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_60 False\n",
      "reshape_54 False\n",
      "conv3_4_1x1_down False\n",
      "activation_276 False\n",
      "conv3_4_1x1_up False\n",
      "activation_277 False\n",
      "multiply_66 False\n",
      "add_54 False\n",
      "activation_278 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_279 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_280 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_61 False\n",
      "reshape_55 False\n",
      "conv4_1_1x1_down False\n",
      "activation_281 False\n",
      "conv4_1_1x1_up False\n",
      "activation_282 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_67 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_55 False\n",
      "activation_283 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_284 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_285 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_62 False\n",
      "reshape_56 False\n",
      "conv4_2_1x1_down False\n",
      "activation_286 False\n",
      "conv4_2_1x1_up False\n",
      "activation_287 False\n",
      "multiply_68 False\n",
      "add_56 False\n",
      "activation_288 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_289 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_290 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_63 False\n",
      "reshape_57 False\n",
      "conv4_3_1x1_down False\n",
      "activation_291 False\n",
      "conv4_3_1x1_up False\n",
      "activation_292 False\n",
      "multiply_69 False\n",
      "add_57 False\n",
      "activation_293 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_294 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_295 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_64 False\n",
      "reshape_58 False\n",
      "conv4_4_1x1_down False\n",
      "activation_296 False\n",
      "conv4_4_1x1_up False\n",
      "activation_297 False\n",
      "multiply_70 False\n",
      "add_58 False\n",
      "activation_298 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_299 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_300 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_65 False\n",
      "reshape_59 False\n",
      "conv4_5_1x1_down False\n",
      "activation_301 False\n",
      "conv4_5_1x1_up False\n",
      "activation_302 False\n",
      "multiply_71 False\n",
      "add_59 False\n",
      "activation_303 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_304 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_305 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_66 False\n",
      "reshape_60 False\n",
      "conv4_6_1x1_down False\n",
      "activation_306 False\n",
      "conv4_6_1x1_up False\n",
      "activation_307 False\n",
      "multiply_72 False\n",
      "add_60 False\n",
      "activation_308 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_309 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_310 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_67 False\n",
      "reshape_61 False\n",
      "conv5_1_1x1_down False\n",
      "activation_311 False\n",
      "conv5_1_1x1_up False\n",
      "activation_312 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_73 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_61 False\n",
      "activation_313 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_314 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_315 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_68 False\n",
      "reshape_62 False\n",
      "conv5_2_1x1_down False\n",
      "activation_316 False\n",
      "conv5_2_1x1_up False\n",
      "activation_317 False\n",
      "multiply_74 False\n",
      "add_62 False\n",
      "activation_318 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_319 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_320 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_69 False\n",
      "reshape_63 False\n",
      "conv5_3_1x1_down False\n",
      "activation_321 False\n",
      "conv5_3_1x1_up False\n",
      "activation_322 False\n",
      "multiply_75 False\n",
      "add_63 False\n",
      "activation_323 False\n",
      "avg_pool False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_70 (Gl (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_71 (Gl (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 2048)         0           global_average_pooling2d_70[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 2048)         0           global_average_pooling2d_71[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_6 (Subtract)           (None, 2048)         0           layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_77 (Multiply)          (None, 2048)         0           layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_78 (Multiply)          (None, 2048)         0           layer_normalization_7[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_76 (Multiply)          (None, 2048)         0           subtract_6[0][0]                 \n",
      "                                                                 subtract_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 2048)         0           multiply_77[0][0]                \n",
      "                                                                 multiply_78[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_79 (Multiply)          (None, 2048)         0           layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 6144)         0           multiply_76[0][0]                \n",
      "                                                                 subtract_7[0][0]                 \n",
      "                                                                 multiply_79[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 32)           196640      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32)           0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          4224        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           4128        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            33          dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 56s 169ms/step - loss: 0.7155 - acc: 0.6056 - val_loss: 0.6494 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67250, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.6486 - acc: 0.6690 - val_loss: 0.6135 - val_acc: 0.6841\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67250 to 0.68406, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.5954 - acc: 0.6974 - val_loss: 0.5998 - val_acc: 0.6969\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.68406 to 0.69687, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5655 - acc: 0.7271 - val_loss: 0.5955 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.69687 to 0.70578, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.5287 - acc: 0.7534 - val_loss: 0.5724 - val_acc: 0.7144\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70578 to 0.71438, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.5090 - acc: 0.7668 - val_loss: 0.5781 - val_acc: 0.7184\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.71438 to 0.71844, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 50s 166ms/step - loss: 0.4861 - acc: 0.7860 - val_loss: 0.5695 - val_acc: 0.7353\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.71844 to 0.73531, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.4677 - acc: 0.7903 - val_loss: 0.5417 - val_acc: 0.7497\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.73531 to 0.74969, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.4375 - acc: 0.8088 - val_loss: 0.5313 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.74969 to 0.75453, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4354 - acc: 0.8114 - val_loss: 0.5275 - val_acc: 0.7509\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.75453\n",
      "Epoch 11/25\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.4230 - acc: 0.8108 - val_loss: 0.5402 - val_acc: 0.7511\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.75453\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.4168 - acc: 0.8232 - val_loss: 0.5271 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.75453\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.3859 - acc: 0.8366 - val_loss: 0.5106 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.75453 to 0.76531, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3949 - acc: 0.8357 - val_loss: 0.5154 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.76531 to 0.76672, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3677 - acc: 0.8483 - val_loss: 0.5069 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.76672 to 0.77047, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3768 - acc: 0.8455 - val_loss: 0.5313 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.77047\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3422 - acc: 0.8631 - val_loss: 0.4866 - val_acc: 0.7792\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.77047 to 0.77922, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_3.h5\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.3495 - acc: 0.8615 - val_loss: 0.5097 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.77922\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.3638 - acc: 0.8456 - val_loss: 0.4996 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.77922\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3497 - acc: 0.8554 - val_loss: 0.5111 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.77922\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3275 - acc: 0.8709 - val_loss: 0.5180 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.77922\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3389 - acc: 0.8661 - val_loss: 0.5010 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.77922\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.3358 - acc: 0.8640 - val_loss: 0.4991 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.77922\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3107 - acc: 0.8745 - val_loss: 0.5065 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.77922\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3034 - acc: 0.8821 - val_loss: 0.5046 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.77922\n",
      "##############################\n",
      "Iteration 4: Validation on F09\n",
      "##############################\n",
      "input_15 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_324 False\n",
      "max_pooling2d_4 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_325 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_326 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_72 False\n",
      "reshape_64 False\n",
      "conv2_1_1x1_down False\n",
      "activation_327 False\n",
      "conv2_1_1x1_up False\n",
      "activation_328 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_80 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_64 False\n",
      "activation_329 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_330 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_331 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_73 False\n",
      "reshape_65 False\n",
      "conv2_2_1x1_down False\n",
      "activation_332 False\n",
      "conv2_2_1x1_up False\n",
      "activation_333 False\n",
      "multiply_81 False\n",
      "add_65 False\n",
      "activation_334 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_335 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_336 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_74 False\n",
      "reshape_66 False\n",
      "conv2_3_1x1_down False\n",
      "activation_337 False\n",
      "conv2_3_1x1_up False\n",
      "activation_338 False\n",
      "multiply_82 False\n",
      "add_66 False\n",
      "activation_339 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_340 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_341 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_75 False\n",
      "reshape_67 False\n",
      "conv3_1_1x1_down False\n",
      "activation_342 False\n",
      "conv3_1_1x1_up False\n",
      "activation_343 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_83 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_67 False\n",
      "activation_344 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_345 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_346 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_76 False\n",
      "reshape_68 False\n",
      "conv3_2_1x1_down False\n",
      "activation_347 False\n",
      "conv3_2_1x1_up False\n",
      "activation_348 False\n",
      "multiply_84 False\n",
      "add_68 False\n",
      "activation_349 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_350 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_351 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_77 False\n",
      "reshape_69 False\n",
      "conv3_3_1x1_down False\n",
      "activation_352 False\n",
      "conv3_3_1x1_up False\n",
      "activation_353 False\n",
      "multiply_85 False\n",
      "add_69 False\n",
      "activation_354 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_355 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_356 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_78 False\n",
      "reshape_70 False\n",
      "conv3_4_1x1_down False\n",
      "activation_357 False\n",
      "conv3_4_1x1_up False\n",
      "activation_358 False\n",
      "multiply_86 False\n",
      "add_70 False\n",
      "activation_359 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_360 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_361 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_79 False\n",
      "reshape_71 False\n",
      "conv4_1_1x1_down False\n",
      "activation_362 False\n",
      "conv4_1_1x1_up False\n",
      "activation_363 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_87 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_71 False\n",
      "activation_364 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_365 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_366 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_80 False\n",
      "reshape_72 False\n",
      "conv4_2_1x1_down False\n",
      "activation_367 False\n",
      "conv4_2_1x1_up False\n",
      "activation_368 False\n",
      "multiply_88 False\n",
      "add_72 False\n",
      "activation_369 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_370 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_371 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_81 False\n",
      "reshape_73 False\n",
      "conv4_3_1x1_down False\n",
      "activation_372 False\n",
      "conv4_3_1x1_up False\n",
      "activation_373 False\n",
      "multiply_89 False\n",
      "add_73 False\n",
      "activation_374 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_375 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_376 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_82 False\n",
      "reshape_74 False\n",
      "conv4_4_1x1_down False\n",
      "activation_377 False\n",
      "conv4_4_1x1_up False\n",
      "activation_378 False\n",
      "multiply_90 False\n",
      "add_74 False\n",
      "activation_379 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_380 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_381 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_83 False\n",
      "reshape_75 False\n",
      "conv4_5_1x1_down False\n",
      "activation_382 False\n",
      "conv4_5_1x1_up False\n",
      "activation_383 False\n",
      "multiply_91 False\n",
      "add_75 False\n",
      "activation_384 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_385 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_386 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_84 False\n",
      "reshape_76 False\n",
      "conv4_6_1x1_down False\n",
      "activation_387 False\n",
      "conv4_6_1x1_up False\n",
      "activation_388 False\n",
      "multiply_92 False\n",
      "add_76 False\n",
      "activation_389 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_390 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_391 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_85 False\n",
      "reshape_77 False\n",
      "conv5_1_1x1_down False\n",
      "activation_392 False\n",
      "conv5_1_1x1_up False\n",
      "activation_393 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_93 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_77 False\n",
      "activation_394 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_395 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_396 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_86 False\n",
      "reshape_78 False\n",
      "conv5_2_1x1_down False\n",
      "activation_397 False\n",
      "conv5_2_1x1_up False\n",
      "activation_398 False\n",
      "multiply_94 False\n",
      "add_78 False\n",
      "activation_399 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_400 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_401 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_87 False\n",
      "reshape_79 False\n",
      "conv5_3_1x1_down False\n",
      "activation_402 False\n",
      "conv5_3_1x1_up False\n",
      "activation_403 False\n",
      "multiply_95 False\n",
      "add_79 False\n",
      "activation_404 False\n",
      "avg_pool False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_88 (Gl (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_89 (Gl (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 2048)         0           global_average_pooling2d_88[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 2048)         0           global_average_pooling2d_89[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "subtract_8 (Subtract)           (None, 2048)         0           layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_97 (Multiply)          (None, 2048)         0           layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_98 (Multiply)          (None, 2048)         0           layer_normalization_9[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_96 (Multiply)          (None, 2048)         0           subtract_8[0][0]                 \n",
      "                                                                 subtract_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_9 (Subtract)           (None, 2048)         0           multiply_97[0][0]                \n",
      "                                                                 multiply_98[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_99 (Multiply)          (None, 2048)         0           layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 6144)         0           multiply_96[0][0]                \n",
      "                                                                 subtract_9[0][0]                 \n",
      "                                                                 multiply_99[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 32)           196640      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          4224        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 32)           4128        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 32)           0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            33          dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "300/300 [==============================] - 57s 173ms/step - loss: 0.6809 - acc: 0.6516 - val_loss: 0.7675 - val_acc: 0.5566\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55656, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.6268 - acc: 0.6847 - val_loss: 0.7526 - val_acc: 0.5673\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.55656 to 0.56734, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5968 - acc: 0.6932 - val_loss: 0.7290 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.56734 to 0.58328, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5460 - acc: 0.7291 - val_loss: 0.7094 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.58328 to 0.61500, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5067 - acc: 0.7650 - val_loss: 0.6884 - val_acc: 0.6311\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61500 to 0.63109, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.4815 - acc: 0.7827 - val_loss: 0.7004 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.63109\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.4494 - acc: 0.8000 - val_loss: 0.6776 - val_acc: 0.6420\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.63109 to 0.64203, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.4468 - acc: 0.8016 - val_loss: 0.6614 - val_acc: 0.6647\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.64203 to 0.66469, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.4193 - acc: 0.8164 - val_loss: 0.6712 - val_acc: 0.6642\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.66469\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.4103 - acc: 0.8206 - val_loss: 0.6546 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.66469 to 0.67656, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3942 - acc: 0.8300 - val_loss: 0.6724 - val_acc: 0.6716\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.67656\n",
      "Epoch 12/25\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3788 - acc: 0.8425 - val_loss: 0.6687 - val_acc: 0.6697\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.67656\n",
      "Epoch 13/25\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.3664 - acc: 0.8454 - val_loss: 0.6767 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.67656 to 0.67750, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 14/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.3442 - acc: 0.8576 - val_loss: 0.6724 - val_acc: 0.6794\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.67750 to 0.67937, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 15/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3552 - acc: 0.8553 - val_loss: 0.6617 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.67937 to 0.68547, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 16/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3434 - acc: 0.8573 - val_loss: 0.6748 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.68547\n",
      "Epoch 17/25\n",
      "300/300 [==============================] - 46s 154ms/step - loss: 0.3382 - acc: 0.8603 - val_loss: 0.6971 - val_acc: 0.6723\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.68547\n",
      "Epoch 18/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3365 - acc: 0.8638 - val_loss: 0.6992 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.68547\n",
      "Epoch 19/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3307 - acc: 0.8565 - val_loss: 0.7080 - val_acc: 0.6680\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.68547\n",
      "Epoch 20/25\n",
      "300/300 [==============================] - 46s 155ms/step - loss: 0.3073 - acc: 0.8768 - val_loss: 0.7227 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.68547\n",
      "Epoch 21/25\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.3286 - acc: 0.8607 - val_loss: 0.6899 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.68547\n",
      "Epoch 22/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3111 - acc: 0.8735 - val_loss: 0.6546 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.68547\n",
      "Epoch 23/25\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.3010 - acc: 0.8832 - val_loss: 0.7101 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.68547\n",
      "Epoch 24/25\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.3058 - acc: 0.8768 - val_loss: 0.6794 - val_acc: 0.6913\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.68547 to 0.69125, saving model to /root/KinshipRecognition/log/model/ensemble_vggface_senet50_notune_dense32-128-32_drop05_4.h5\n",
      "Epoch 25/25\n",
      "300/300 [==============================] - 49s 163ms/step - loss: 0.3003 - acc: 0.8775 - val_loss: 0.7224 - val_acc: 0.6741\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.69125\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "\n",
    "    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_families_list[i])\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.3, patience=30, verbose=1)\n",
    "    callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    \n",
    "    history = model.fit(gen(train, train_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        validation_data=gen(val, val_person_to_images_map, INPUT_SHAPE, batch_size=16), \n",
    "                        epochs=25, steps_per_epoch=300, validation_steps=200,\n",
    "                        verbose=1, callbacks=callbacks_list, \n",
    "                        use_multiprocessing=False, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 0: Validation on F02\n",
      "##############################\n",
      "input_18 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_405 False\n",
      "max_pooling2d_5 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_406 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_407 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_90 False\n",
      "reshape_80 False\n",
      "conv2_1_1x1_down False\n",
      "activation_408 False\n",
      "conv2_1_1x1_up False\n",
      "activation_409 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_100 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_80 False\n",
      "activation_410 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_411 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_412 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_91 False\n",
      "reshape_81 False\n",
      "conv2_2_1x1_down False\n",
      "activation_413 False\n",
      "conv2_2_1x1_up False\n",
      "activation_414 False\n",
      "multiply_101 False\n",
      "add_81 False\n",
      "activation_415 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_416 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_417 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_92 False\n",
      "reshape_82 False\n",
      "conv2_3_1x1_down False\n",
      "activation_418 False\n",
      "conv2_3_1x1_up False\n",
      "activation_419 False\n",
      "multiply_102 False\n",
      "add_82 False\n",
      "activation_420 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_421 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_422 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_93 False\n",
      "reshape_83 False\n",
      "conv3_1_1x1_down False\n",
      "activation_423 False\n",
      "conv3_1_1x1_up False\n",
      "activation_424 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_103 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_83 False\n",
      "activation_425 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_426 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_427 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_94 False\n",
      "reshape_84 False\n",
      "conv3_2_1x1_down False\n",
      "activation_428 False\n",
      "conv3_2_1x1_up False\n",
      "activation_429 False\n",
      "multiply_104 False\n",
      "add_84 False\n",
      "activation_430 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_431 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_432 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_95 False\n",
      "reshape_85 False\n",
      "conv3_3_1x1_down False\n",
      "activation_433 False\n",
      "conv3_3_1x1_up False\n",
      "activation_434 False\n",
      "multiply_105 False\n",
      "add_85 False\n",
      "activation_435 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_436 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_437 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_96 False\n",
      "reshape_86 False\n",
      "conv3_4_1x1_down False\n",
      "activation_438 False\n",
      "conv3_4_1x1_up False\n",
      "activation_439 False\n",
      "multiply_106 False\n",
      "add_86 False\n",
      "activation_440 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_441 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_442 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_97 False\n",
      "reshape_87 False\n",
      "conv4_1_1x1_down False\n",
      "activation_443 False\n",
      "conv4_1_1x1_up False\n",
      "activation_444 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_107 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_87 False\n",
      "activation_445 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_446 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_447 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_98 False\n",
      "reshape_88 False\n",
      "conv4_2_1x1_down False\n",
      "activation_448 False\n",
      "conv4_2_1x1_up False\n",
      "activation_449 False\n",
      "multiply_108 False\n",
      "add_88 False\n",
      "activation_450 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_451 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_452 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_99 False\n",
      "reshape_89 False\n",
      "conv4_3_1x1_down False\n",
      "activation_453 False\n",
      "conv4_3_1x1_up False\n",
      "activation_454 False\n",
      "multiply_109 False\n",
      "add_89 False\n",
      "activation_455 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_456 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_457 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_100 False\n",
      "reshape_90 False\n",
      "conv4_4_1x1_down False\n",
      "activation_458 False\n",
      "conv4_4_1x1_up False\n",
      "activation_459 False\n",
      "multiply_110 False\n",
      "add_90 False\n",
      "activation_460 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_461 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_462 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_101 False\n",
      "reshape_91 False\n",
      "conv4_5_1x1_down False\n",
      "activation_463 False\n",
      "conv4_5_1x1_up False\n",
      "activation_464 False\n",
      "multiply_111 False\n",
      "add_91 False\n",
      "activation_465 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_466 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_467 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_102 False\n",
      "reshape_92 False\n",
      "conv4_6_1x1_down False\n",
      "activation_468 False\n",
      "conv4_6_1x1_up False\n",
      "activation_469 False\n",
      "multiply_112 False\n",
      "add_92 False\n",
      "activation_470 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_471 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_472 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_103 False\n",
      "reshape_93 False\n",
      "conv5_1_1x1_down False\n",
      "activation_473 False\n",
      "conv5_1_1x1_up False\n",
      "activation_474 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_113 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_93 False\n",
      "activation_475 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_476 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_477 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_104 False\n",
      "reshape_94 False\n",
      "conv5_2_1x1_down False\n",
      "activation_478 False\n",
      "conv5_2_1x1_up False\n",
      "activation_479 False\n",
      "multiply_114 False\n",
      "add_94 False\n",
      "activation_480 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_481 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_482 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_105 False\n",
      "reshape_95 False\n",
      "conv5_3_1x1_down False\n",
      "activation_483 False\n",
      "conv5_3_1x1_up False\n",
      "activation_484 False\n",
      "multiply_115 False\n",
      "add_95 False\n",
      "activation_485 False\n",
      "avg_pool False\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_106 (G (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_107 (G (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 2048)         0           global_average_pooling2d_106[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 2048)         0           global_average_pooling2d_107[0][0\n",
      "__________________________________________________________________________________________________\n",
      "subtract_10 (Subtract)          (None, 2048)         0           layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_117 (Multiply)         (None, 2048)         0           layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_118 (Multiply)         (None, 2048)         0           layer_normalization_11[0][0]     \n",
      "                                                                 layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_116 (Multiply)         (None, 2048)         0           subtract_10[0][0]                \n",
      "                                                                 subtract_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_11 (Subtract)          (None, 2048)         0           multiply_117[0][0]               \n",
      "                                                                 multiply_118[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_119 (Multiply)         (None, 2048)         0           layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 6144)         0           multiply_116[0][0]               \n",
      "                                                                 subtract_11[0][0]                \n",
      "                                                                 multiply_119[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 32)           196640      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 32)           0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          4224        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           4128        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 32)           0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            33          dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 1: Validation on F04\n",
      "##############################\n",
      "input_21 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_486 False\n",
      "max_pooling2d_6 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_487 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_488 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_108 False\n",
      "reshape_96 False\n",
      "conv2_1_1x1_down False\n",
      "activation_489 False\n",
      "conv2_1_1x1_up False\n",
      "activation_490 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_120 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_96 False\n",
      "activation_491 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_492 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_493 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_109 False\n",
      "reshape_97 False\n",
      "conv2_2_1x1_down False\n",
      "activation_494 False\n",
      "conv2_2_1x1_up False\n",
      "activation_495 False\n",
      "multiply_121 False\n",
      "add_97 False\n",
      "activation_496 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_497 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_498 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_110 False\n",
      "reshape_98 False\n",
      "conv2_3_1x1_down False\n",
      "activation_499 False\n",
      "conv2_3_1x1_up False\n",
      "activation_500 False\n",
      "multiply_122 False\n",
      "add_98 False\n",
      "activation_501 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_502 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_503 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_111 False\n",
      "reshape_99 False\n",
      "conv3_1_1x1_down False\n",
      "activation_504 False\n",
      "conv3_1_1x1_up False\n",
      "activation_505 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_123 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_99 False\n",
      "activation_506 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_507 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_508 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_112 False\n",
      "reshape_100 False\n",
      "conv3_2_1x1_down False\n",
      "activation_509 False\n",
      "conv3_2_1x1_up False\n",
      "activation_510 False\n",
      "multiply_124 False\n",
      "add_100 False\n",
      "activation_511 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_512 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_513 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_113 False\n",
      "reshape_101 False\n",
      "conv3_3_1x1_down False\n",
      "activation_514 False\n",
      "conv3_3_1x1_up False\n",
      "activation_515 False\n",
      "multiply_125 False\n",
      "add_101 False\n",
      "activation_516 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_517 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_518 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_114 False\n",
      "reshape_102 False\n",
      "conv3_4_1x1_down False\n",
      "activation_519 False\n",
      "conv3_4_1x1_up False\n",
      "activation_520 False\n",
      "multiply_126 False\n",
      "add_102 False\n",
      "activation_521 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_522 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_523 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_115 False\n",
      "reshape_103 False\n",
      "conv4_1_1x1_down False\n",
      "activation_524 False\n",
      "conv4_1_1x1_up False\n",
      "activation_525 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_127 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_103 False\n",
      "activation_526 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_527 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_528 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_116 False\n",
      "reshape_104 False\n",
      "conv4_2_1x1_down False\n",
      "activation_529 False\n",
      "conv4_2_1x1_up False\n",
      "activation_530 False\n",
      "multiply_128 False\n",
      "add_104 False\n",
      "activation_531 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_532 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_533 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_117 False\n",
      "reshape_105 False\n",
      "conv4_3_1x1_down False\n",
      "activation_534 False\n",
      "conv4_3_1x1_up False\n",
      "activation_535 False\n",
      "multiply_129 False\n",
      "add_105 False\n",
      "activation_536 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_537 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_538 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_118 False\n",
      "reshape_106 False\n",
      "conv4_4_1x1_down False\n",
      "activation_539 False\n",
      "conv4_4_1x1_up False\n",
      "activation_540 False\n",
      "multiply_130 False\n",
      "add_106 False\n",
      "activation_541 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_542 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_543 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_119 False\n",
      "reshape_107 False\n",
      "conv4_5_1x1_down False\n",
      "activation_544 False\n",
      "conv4_5_1x1_up False\n",
      "activation_545 False\n",
      "multiply_131 False\n",
      "add_107 False\n",
      "activation_546 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_547 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_548 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_120 False\n",
      "reshape_108 False\n",
      "conv4_6_1x1_down False\n",
      "activation_549 False\n",
      "conv4_6_1x1_up False\n",
      "activation_550 False\n",
      "multiply_132 False\n",
      "add_108 False\n",
      "activation_551 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_552 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_553 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_121 False\n",
      "reshape_109 False\n",
      "conv5_1_1x1_down False\n",
      "activation_554 False\n",
      "conv5_1_1x1_up False\n",
      "activation_555 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_133 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_109 False\n",
      "activation_556 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_557 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_558 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_122 False\n",
      "reshape_110 False\n",
      "conv5_2_1x1_down False\n",
      "activation_559 False\n",
      "conv5_2_1x1_up False\n",
      "activation_560 False\n",
      "multiply_134 False\n",
      "add_110 False\n",
      "activation_561 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_562 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_563 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_123 False\n",
      "reshape_111 False\n",
      "conv5_3_1x1_down False\n",
      "activation_564 False\n",
      "conv5_3_1x1_up False\n",
      "activation_565 False\n",
      "multiply_135 False\n",
      "add_111 False\n",
      "activation_566 False\n",
      "avg_pool False\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_124 (G (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_125 (G (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 2048)         0           global_average_pooling2d_124[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 2048)         0           global_average_pooling2d_125[0][0\n",
      "__________________________________________________________________________________________________\n",
      "subtract_12 (Subtract)          (None, 2048)         0           layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_137 (Multiply)         (None, 2048)         0           layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_138 (Multiply)         (None, 2048)         0           layer_normalization_13[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_136 (Multiply)         (None, 2048)         0           subtract_12[0][0]                \n",
      "                                                                 subtract_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_13 (Subtract)          (None, 2048)         0           multiply_137[0][0]               \n",
      "                                                                 multiply_138[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_139 (Multiply)         (None, 2048)         0           layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 6144)         0           multiply_136[0][0]               \n",
      "                                                                 subtract_13[0][0]                \n",
      "                                                                 multiply_139[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 32)           196640      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 32)           0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128)          4224        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 128)          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 32)           4128        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            33          dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 2: Validation on F06\n",
      "##############################\n",
      "input_24 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_567 False\n",
      "max_pooling2d_7 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_568 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_569 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_126 False\n",
      "reshape_112 False\n",
      "conv2_1_1x1_down False\n",
      "activation_570 False\n",
      "conv2_1_1x1_up False\n",
      "activation_571 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_140 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_112 False\n",
      "activation_572 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_573 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_574 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_127 False\n",
      "reshape_113 False\n",
      "conv2_2_1x1_down False\n",
      "activation_575 False\n",
      "conv2_2_1x1_up False\n",
      "activation_576 False\n",
      "multiply_141 False\n",
      "add_113 False\n",
      "activation_577 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_578 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_579 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_128 False\n",
      "reshape_114 False\n",
      "conv2_3_1x1_down False\n",
      "activation_580 False\n",
      "conv2_3_1x1_up False\n",
      "activation_581 False\n",
      "multiply_142 False\n",
      "add_114 False\n",
      "activation_582 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_583 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_584 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_129 False\n",
      "reshape_115 False\n",
      "conv3_1_1x1_down False\n",
      "activation_585 False\n",
      "conv3_1_1x1_up False\n",
      "activation_586 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_143 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_115 False\n",
      "activation_587 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_588 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_589 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_130 False\n",
      "reshape_116 False\n",
      "conv3_2_1x1_down False\n",
      "activation_590 False\n",
      "conv3_2_1x1_up False\n",
      "activation_591 False\n",
      "multiply_144 False\n",
      "add_116 False\n",
      "activation_592 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_593 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_594 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_131 False\n",
      "reshape_117 False\n",
      "conv3_3_1x1_down False\n",
      "activation_595 False\n",
      "conv3_3_1x1_up False\n",
      "activation_596 False\n",
      "multiply_145 False\n",
      "add_117 False\n",
      "activation_597 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_598 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_599 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_132 False\n",
      "reshape_118 False\n",
      "conv3_4_1x1_down False\n",
      "activation_600 False\n",
      "conv3_4_1x1_up False\n",
      "activation_601 False\n",
      "multiply_146 False\n",
      "add_118 False\n",
      "activation_602 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_603 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_604 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_133 False\n",
      "reshape_119 False\n",
      "conv4_1_1x1_down False\n",
      "activation_605 False\n",
      "conv4_1_1x1_up False\n",
      "activation_606 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_147 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_119 False\n",
      "activation_607 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_608 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_609 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_134 False\n",
      "reshape_120 False\n",
      "conv4_2_1x1_down False\n",
      "activation_610 False\n",
      "conv4_2_1x1_up False\n",
      "activation_611 False\n",
      "multiply_148 False\n",
      "add_120 False\n",
      "activation_612 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_613 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_614 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_135 False\n",
      "reshape_121 False\n",
      "conv4_3_1x1_down False\n",
      "activation_615 False\n",
      "conv4_3_1x1_up False\n",
      "activation_616 False\n",
      "multiply_149 False\n",
      "add_121 False\n",
      "activation_617 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_618 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_619 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_136 False\n",
      "reshape_122 False\n",
      "conv4_4_1x1_down False\n",
      "activation_620 False\n",
      "conv4_4_1x1_up False\n",
      "activation_621 False\n",
      "multiply_150 False\n",
      "add_122 False\n",
      "activation_622 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_623 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_624 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_137 False\n",
      "reshape_123 False\n",
      "conv4_5_1x1_down False\n",
      "activation_625 False\n",
      "conv4_5_1x1_up False\n",
      "activation_626 False\n",
      "multiply_151 False\n",
      "add_123 False\n",
      "activation_627 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_628 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_629 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_138 False\n",
      "reshape_124 False\n",
      "conv4_6_1x1_down False\n",
      "activation_630 False\n",
      "conv4_6_1x1_up False\n",
      "activation_631 False\n",
      "multiply_152 False\n",
      "add_124 False\n",
      "activation_632 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_633 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_634 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_139 False\n",
      "reshape_125 False\n",
      "conv5_1_1x1_down False\n",
      "activation_635 False\n",
      "conv5_1_1x1_up False\n",
      "activation_636 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_153 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_125 False\n",
      "activation_637 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_638 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_639 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_140 False\n",
      "reshape_126 False\n",
      "conv5_2_1x1_down False\n",
      "activation_640 False\n",
      "conv5_2_1x1_up False\n",
      "activation_641 False\n",
      "multiply_154 False\n",
      "add_126 False\n",
      "activation_642 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_643 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_644 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_141 False\n",
      "reshape_127 False\n",
      "conv5_3_1x1_down False\n",
      "activation_645 False\n",
      "conv5_3_1x1_up False\n",
      "activation_646 False\n",
      "multiply_155 False\n",
      "add_127 False\n",
      "activation_647 False\n",
      "avg_pool False\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_142 (G (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_143 (G (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 2048)         0           global_average_pooling2d_142[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 2048)         0           global_average_pooling2d_143[0][0\n",
      "__________________________________________________________________________________________________\n",
      "subtract_14 (Subtract)          (None, 2048)         0           layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_157 (Multiply)         (None, 2048)         0           layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_158 (Multiply)         (None, 2048)         0           layer_normalization_15[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_156 (Multiply)         (None, 2048)         0           subtract_14[0][0]                \n",
      "                                                                 subtract_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_15 (Subtract)          (None, 2048)         0           multiply_157[0][0]               \n",
      "                                                                 multiply_158[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_159 (Multiply)         (None, 2048)         0           layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 6144)         0           multiply_156[0][0]               \n",
      "                                                                 subtract_15[0][0]                \n",
      "                                                                 multiply_159[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 32)           196640      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          4224        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 128)          0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 32)           4128        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32)           0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1)            33          dropout_23[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 3: Validation on F08\n",
      "##############################\n",
      "input_27 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_648 False\n",
      "max_pooling2d_8 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_649 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_650 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_144 False\n",
      "reshape_128 False\n",
      "conv2_1_1x1_down False\n",
      "activation_651 False\n",
      "conv2_1_1x1_up False\n",
      "activation_652 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_160 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_128 False\n",
      "activation_653 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_654 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_655 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_145 False\n",
      "reshape_129 False\n",
      "conv2_2_1x1_down False\n",
      "activation_656 False\n",
      "conv2_2_1x1_up False\n",
      "activation_657 False\n",
      "multiply_161 False\n",
      "add_129 False\n",
      "activation_658 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_659 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_660 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_146 False\n",
      "reshape_130 False\n",
      "conv2_3_1x1_down False\n",
      "activation_661 False\n",
      "conv2_3_1x1_up False\n",
      "activation_662 False\n",
      "multiply_162 False\n",
      "add_130 False\n",
      "activation_663 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_664 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_665 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_147 False\n",
      "reshape_131 False\n",
      "conv3_1_1x1_down False\n",
      "activation_666 False\n",
      "conv3_1_1x1_up False\n",
      "activation_667 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_163 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_131 False\n",
      "activation_668 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_669 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_670 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_148 False\n",
      "reshape_132 False\n",
      "conv3_2_1x1_down False\n",
      "activation_671 False\n",
      "conv3_2_1x1_up False\n",
      "activation_672 False\n",
      "multiply_164 False\n",
      "add_132 False\n",
      "activation_673 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_674 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_675 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_149 False\n",
      "reshape_133 False\n",
      "conv3_3_1x1_down False\n",
      "activation_676 False\n",
      "conv3_3_1x1_up False\n",
      "activation_677 False\n",
      "multiply_165 False\n",
      "add_133 False\n",
      "activation_678 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_679 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_680 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_150 False\n",
      "reshape_134 False\n",
      "conv3_4_1x1_down False\n",
      "activation_681 False\n",
      "conv3_4_1x1_up False\n",
      "activation_682 False\n",
      "multiply_166 False\n",
      "add_134 False\n",
      "activation_683 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_684 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_685 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_151 False\n",
      "reshape_135 False\n",
      "conv4_1_1x1_down False\n",
      "activation_686 False\n",
      "conv4_1_1x1_up False\n",
      "activation_687 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_167 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_135 False\n",
      "activation_688 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_689 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_690 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_152 False\n",
      "reshape_136 False\n",
      "conv4_2_1x1_down False\n",
      "activation_691 False\n",
      "conv4_2_1x1_up False\n",
      "activation_692 False\n",
      "multiply_168 False\n",
      "add_136 False\n",
      "activation_693 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_694 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_695 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_153 False\n",
      "reshape_137 False\n",
      "conv4_3_1x1_down False\n",
      "activation_696 False\n",
      "conv4_3_1x1_up False\n",
      "activation_697 False\n",
      "multiply_169 False\n",
      "add_137 False\n",
      "activation_698 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_699 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_700 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_154 False\n",
      "reshape_138 False\n",
      "conv4_4_1x1_down False\n",
      "activation_701 False\n",
      "conv4_4_1x1_up False\n",
      "activation_702 False\n",
      "multiply_170 False\n",
      "add_138 False\n",
      "activation_703 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_704 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_705 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_155 False\n",
      "reshape_139 False\n",
      "conv4_5_1x1_down False\n",
      "activation_706 False\n",
      "conv4_5_1x1_up False\n",
      "activation_707 False\n",
      "multiply_171 False\n",
      "add_139 False\n",
      "activation_708 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_709 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_710 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_156 False\n",
      "reshape_140 False\n",
      "conv4_6_1x1_down False\n",
      "activation_711 False\n",
      "conv4_6_1x1_up False\n",
      "activation_712 False\n",
      "multiply_172 False\n",
      "add_140 False\n",
      "activation_713 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_714 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_715 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_157 False\n",
      "reshape_141 False\n",
      "conv5_1_1x1_down False\n",
      "activation_716 False\n",
      "conv5_1_1x1_up False\n",
      "activation_717 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_173 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_141 False\n",
      "activation_718 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_719 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_720 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_158 False\n",
      "reshape_142 False\n",
      "conv5_2_1x1_down False\n",
      "activation_721 False\n",
      "conv5_2_1x1_up False\n",
      "activation_722 False\n",
      "multiply_174 False\n",
      "add_142 False\n",
      "activation_723 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_724 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_725 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_159 False\n",
      "reshape_143 False\n",
      "conv5_3_1x1_down False\n",
      "activation_726 False\n",
      "conv5_3_1x1_up False\n",
      "activation_727 False\n",
      "multiply_175 False\n",
      "add_143 False\n",
      "activation_728 False\n",
      "avg_pool False\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_25[0][0]                   \n",
      "                                                                 input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_160 (G (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_161 (G (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 2048)         0           global_average_pooling2d_160[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 2048)         0           global_average_pooling2d_161[0][0\n",
      "__________________________________________________________________________________________________\n",
      "subtract_16 (Subtract)          (None, 2048)         0           layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_177 (Multiply)         (None, 2048)         0           layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_178 (Multiply)         (None, 2048)         0           layer_normalization_17[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_176 (Multiply)         (None, 2048)         0           subtract_16[0][0]                \n",
      "                                                                 subtract_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_17 (Subtract)          (None, 2048)         0           multiply_177[0][0]               \n",
      "                                                                 multiply_178[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_179 (Multiply)         (None, 2048)         0           layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 6144)         0           multiply_176[0][0]               \n",
      "                                                                 subtract_17[0][0]                \n",
      "                                                                 multiply_179[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 32)           196640      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32)           0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          4224        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 128)          0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 32)           4128        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 32)           0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1)            33          dropout_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Iteration 4: Validation on F09\n",
      "##############################\n",
      "input_30 False\n",
      "conv1/7x7_s2 False\n",
      "conv1/7x7_s2/bn False\n",
      "activation_729 False\n",
      "max_pooling2d_9 False\n",
      "conv2_1_1x1_reduce False\n",
      "conv2_1_1x1_reduce/bn False\n",
      "activation_730 False\n",
      "conv2_1_3x3 False\n",
      "conv2_1_3x3/bn False\n",
      "activation_731 False\n",
      "conv2_1_1x1_increase False\n",
      "conv2_1_1x1_increase/bn False\n",
      "global_average_pooling2d_162 False\n",
      "reshape_144 False\n",
      "conv2_1_1x1_down False\n",
      "activation_732 False\n",
      "conv2_1_1x1_up False\n",
      "activation_733 False\n",
      "conv2_1_1x1_proj False\n",
      "multiply_180 False\n",
      "conv2_1_1x1_proj/bn False\n",
      "add_144 False\n",
      "activation_734 False\n",
      "conv2_2_1x1_reduce False\n",
      "conv2_2_1x1_reduce/bn False\n",
      "activation_735 False\n",
      "conv2_2_3x3 False\n",
      "conv2_2_3x3/bn False\n",
      "activation_736 False\n",
      "conv2_2_1x1_increase False\n",
      "conv2_2_1x1_increase/bn False\n",
      "global_average_pooling2d_163 False\n",
      "reshape_145 False\n",
      "conv2_2_1x1_down False\n",
      "activation_737 False\n",
      "conv2_2_1x1_up False\n",
      "activation_738 False\n",
      "multiply_181 False\n",
      "add_145 False\n",
      "activation_739 False\n",
      "conv2_3_1x1_reduce False\n",
      "conv2_3_1x1_reduce/bn False\n",
      "activation_740 False\n",
      "conv2_3_3x3 False\n",
      "conv2_3_3x3/bn False\n",
      "activation_741 False\n",
      "conv2_3_1x1_increase False\n",
      "conv2_3_1x1_increase/bn False\n",
      "global_average_pooling2d_164 False\n",
      "reshape_146 False\n",
      "conv2_3_1x1_down False\n",
      "activation_742 False\n",
      "conv2_3_1x1_up False\n",
      "activation_743 False\n",
      "multiply_182 False\n",
      "add_146 False\n",
      "activation_744 False\n",
      "conv3_1_1x1_reduce False\n",
      "conv3_1_1x1_reduce/bn False\n",
      "activation_745 False\n",
      "conv3_1_3x3 False\n",
      "conv3_1_3x3/bn False\n",
      "activation_746 False\n",
      "conv3_1_1x1_increase False\n",
      "conv3_1_1x1_increase/bn False\n",
      "global_average_pooling2d_165 False\n",
      "reshape_147 False\n",
      "conv3_1_1x1_down False\n",
      "activation_747 False\n",
      "conv3_1_1x1_up False\n",
      "activation_748 False\n",
      "conv3_1_1x1_proj False\n",
      "multiply_183 False\n",
      "conv3_1_1x1_proj/bn False\n",
      "add_147 False\n",
      "activation_749 False\n",
      "conv3_2_1x1_reduce False\n",
      "conv3_2_1x1_reduce/bn False\n",
      "activation_750 False\n",
      "conv3_2_3x3 False\n",
      "conv3_2_3x3/bn False\n",
      "activation_751 False\n",
      "conv3_2_1x1_increase False\n",
      "conv3_2_1x1_increase/bn False\n",
      "global_average_pooling2d_166 False\n",
      "reshape_148 False\n",
      "conv3_2_1x1_down False\n",
      "activation_752 False\n",
      "conv3_2_1x1_up False\n",
      "activation_753 False\n",
      "multiply_184 False\n",
      "add_148 False\n",
      "activation_754 False\n",
      "conv3_3_1x1_reduce False\n",
      "conv3_3_1x1_reduce/bn False\n",
      "activation_755 False\n",
      "conv3_3_3x3 False\n",
      "conv3_3_3x3/bn False\n",
      "activation_756 False\n",
      "conv3_3_1x1_increase False\n",
      "conv3_3_1x1_increase/bn False\n",
      "global_average_pooling2d_167 False\n",
      "reshape_149 False\n",
      "conv3_3_1x1_down False\n",
      "activation_757 False\n",
      "conv3_3_1x1_up False\n",
      "activation_758 False\n",
      "multiply_185 False\n",
      "add_149 False\n",
      "activation_759 False\n",
      "conv3_4_1x1_reduce False\n",
      "conv3_4_1x1_reduce/bn False\n",
      "activation_760 False\n",
      "conv3_4_3x3 False\n",
      "conv3_4_3x3/bn False\n",
      "activation_761 False\n",
      "conv3_4_1x1_increase False\n",
      "conv3_4_1x1_increase/bn False\n",
      "global_average_pooling2d_168 False\n",
      "reshape_150 False\n",
      "conv3_4_1x1_down False\n",
      "activation_762 False\n",
      "conv3_4_1x1_up False\n",
      "activation_763 False\n",
      "multiply_186 False\n",
      "add_150 False\n",
      "activation_764 False\n",
      "conv4_1_1x1_reduce False\n",
      "conv4_1_1x1_reduce/bn False\n",
      "activation_765 False\n",
      "conv4_1_3x3 False\n",
      "conv4_1_3x3/bn False\n",
      "activation_766 False\n",
      "conv4_1_1x1_increase False\n",
      "conv4_1_1x1_increase/bn False\n",
      "global_average_pooling2d_169 False\n",
      "reshape_151 False\n",
      "conv4_1_1x1_down False\n",
      "activation_767 False\n",
      "conv4_1_1x1_up False\n",
      "activation_768 False\n",
      "conv4_1_1x1_proj False\n",
      "multiply_187 False\n",
      "conv4_1_1x1_proj/bn False\n",
      "add_151 False\n",
      "activation_769 False\n",
      "conv4_2_1x1_reduce False\n",
      "conv4_2_1x1_reduce/bn False\n",
      "activation_770 False\n",
      "conv4_2_3x3 False\n",
      "conv4_2_3x3/bn False\n",
      "activation_771 False\n",
      "conv4_2_1x1_increase False\n",
      "conv4_2_1x1_increase/bn False\n",
      "global_average_pooling2d_170 False\n",
      "reshape_152 False\n",
      "conv4_2_1x1_down False\n",
      "activation_772 False\n",
      "conv4_2_1x1_up False\n",
      "activation_773 False\n",
      "multiply_188 False\n",
      "add_152 False\n",
      "activation_774 False\n",
      "conv4_3_1x1_reduce False\n",
      "conv4_3_1x1_reduce/bn False\n",
      "activation_775 False\n",
      "conv4_3_3x3 False\n",
      "conv4_3_3x3/bn False\n",
      "activation_776 False\n",
      "conv4_3_1x1_increase False\n",
      "conv4_3_1x1_increase/bn False\n",
      "global_average_pooling2d_171 False\n",
      "reshape_153 False\n",
      "conv4_3_1x1_down False\n",
      "activation_777 False\n",
      "conv4_3_1x1_up False\n",
      "activation_778 False\n",
      "multiply_189 False\n",
      "add_153 False\n",
      "activation_779 False\n",
      "conv4_4_1x1_reduce False\n",
      "conv4_4_1x1_reduce/bn False\n",
      "activation_780 False\n",
      "conv4_4_3x3 False\n",
      "conv4_4_3x3/bn False\n",
      "activation_781 False\n",
      "conv4_4_1x1_increase False\n",
      "conv4_4_1x1_increase/bn False\n",
      "global_average_pooling2d_172 False\n",
      "reshape_154 False\n",
      "conv4_4_1x1_down False\n",
      "activation_782 False\n",
      "conv4_4_1x1_up False\n",
      "activation_783 False\n",
      "multiply_190 False\n",
      "add_154 False\n",
      "activation_784 False\n",
      "conv4_5_1x1_reduce False\n",
      "conv4_5_1x1_reduce/bn False\n",
      "activation_785 False\n",
      "conv4_5_3x3 False\n",
      "conv4_5_3x3/bn False\n",
      "activation_786 False\n",
      "conv4_5_1x1_increase False\n",
      "conv4_5_1x1_increase/bn False\n",
      "global_average_pooling2d_173 False\n",
      "reshape_155 False\n",
      "conv4_5_1x1_down False\n",
      "activation_787 False\n",
      "conv4_5_1x1_up False\n",
      "activation_788 False\n",
      "multiply_191 False\n",
      "add_155 False\n",
      "activation_789 False\n",
      "conv4_6_1x1_reduce False\n",
      "conv4_6_1x1_reduce/bn False\n",
      "activation_790 False\n",
      "conv4_6_3x3 False\n",
      "conv4_6_3x3/bn False\n",
      "activation_791 False\n",
      "conv4_6_1x1_increase False\n",
      "conv4_6_1x1_increase/bn False\n",
      "global_average_pooling2d_174 False\n",
      "reshape_156 False\n",
      "conv4_6_1x1_down False\n",
      "activation_792 False\n",
      "conv4_6_1x1_up False\n",
      "activation_793 False\n",
      "multiply_192 False\n",
      "add_156 False\n",
      "activation_794 False\n",
      "conv5_1_1x1_reduce False\n",
      "conv5_1_1x1_reduce/bn False\n",
      "activation_795 False\n",
      "conv5_1_3x3 False\n",
      "conv5_1_3x3/bn False\n",
      "activation_796 False\n",
      "conv5_1_1x1_increase False\n",
      "conv5_1_1x1_increase/bn False\n",
      "global_average_pooling2d_175 False\n",
      "reshape_157 False\n",
      "conv5_1_1x1_down False\n",
      "activation_797 False\n",
      "conv5_1_1x1_up False\n",
      "activation_798 False\n",
      "conv5_1_1x1_proj False\n",
      "multiply_193 False\n",
      "conv5_1_1x1_proj/bn False\n",
      "add_157 False\n",
      "activation_799 False\n",
      "conv5_2_1x1_reduce False\n",
      "conv5_2_1x1_reduce/bn False\n",
      "activation_800 False\n",
      "conv5_2_3x3 False\n",
      "conv5_2_3x3/bn False\n",
      "activation_801 False\n",
      "conv5_2_1x1_increase False\n",
      "conv5_2_1x1_increase/bn False\n",
      "global_average_pooling2d_176 False\n",
      "reshape_158 False\n",
      "conv5_2_1x1_down False\n",
      "activation_802 False\n",
      "conv5_2_1x1_up False\n",
      "activation_803 False\n",
      "multiply_194 False\n",
      "add_158 False\n",
      "activation_804 False\n",
      "conv5_3_1x1_reduce False\n",
      "conv5_3_1x1_reduce/bn False\n",
      "activation_805 False\n",
      "conv5_3_3x3 False\n",
      "conv5_3_3x3/bn False\n",
      "activation_806 False\n",
      "conv5_3_1x1_increase False\n",
      "conv5_3_1x1_increase/bn False\n",
      "global_average_pooling2d_177 False\n",
      "reshape_159 False\n",
      "conv5_3_1x1_down False\n",
      "activation_807 False\n",
      "conv5_3_1x1_up False\n",
      "activation_808 False\n",
      "multiply_195 False\n",
      "add_159 False\n",
      "activation_809 False\n",
      "avg_pool False\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_28 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_178 (G (None, 2048)         0           vggface_senet50[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_179 (G (None, 2048)         0           vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 2048)         0           global_average_pooling2d_178[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 2048)         0           global_average_pooling2d_179[0][0\n",
      "__________________________________________________________________________________________________\n",
      "subtract_18 (Subtract)          (None, 2048)         0           layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_197 (Multiply)         (None, 2048)         0           layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_198 (Multiply)         (None, 2048)         0           layer_normalization_19[0][0]     \n",
      "                                                                 layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_196 (Multiply)         (None, 2048)         0           subtract_18[0][0]                \n",
      "                                                                 subtract_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_19 (Subtract)          (None, 2048)         0           multiply_197[0][0]               \n",
      "                                                                 multiply_198[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_199 (Multiply)         (None, 2048)         0           layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 6144)         0           multiply_196[0][0]               \n",
      "                                                                 subtract_19[0][0]                \n",
      "                                                                 multiply_199[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 32)           196640      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 32)           0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 128)          4224        dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 128)          0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 32)           4128        dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 32)           0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1)            33          dropout_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,297,169\n",
      "Trainable params: 205,025\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_path = f\"{BASE_PATH}/data/test/\"\n",
    "submission = pd.read_csv(f'{BASE_PATH}/data/test_ds.csv')\n",
    "preds_for_sub = np.zeros(submission.shape[0])\n",
    "all_preds = list()\n",
    "for i in range(len(val_families_list)):\n",
    "\n",
    "    print('##############################')\n",
    "    print(f'Iteration {i}: Validation on {val_families_list[i]}')\n",
    "    print('##############################')\n",
    "    \n",
    "    model = baseline_model(BASE_MODEL, fine_tune=FINE_TUNE)\n",
    "    file_path = f\"{BASE_PATH}/log/model/{MODEL_NAME}_{i}.h5\"\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = []\n",
    "    for j in range(0, len(submission.p1.values), 32):\n",
    "        X1 = submission.p1.values[j:j+32]\n",
    "        X1 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X1])\n",
    "\n",
    "        X2 = submission.p2.values[j:j+32]\n",
    "        X2 = np.array([read_img(test_path + x, INPUT_SHAPE) for x in X2])\n",
    "\n",
    "        pred = model.predict([X1, X2]).ravel().tolist()\n",
    "        predictions += pred    \n",
    "    \n",
    "    all_preds.append(np.array(predictions))\n",
    "    preds_for_sub += np.array(predictions) / len(val_families_list)\n",
    "\n",
    "    \n",
    "all_preds = np.asarray(all_preds)\n",
    "submission['score'] = preds_for_sub\n",
    "pd.DataFrame(all_preds).to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}_allpreds.csv\", index=False)\n",
    "submission.to_csv(f\"{BASE_PATH}/log/results/{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2102\n",
      "3000 \n",
      "\n",
      "0.1405569225549698\n",
      "0.19679031372070313\n",
      "0.16570896208286287\n",
      "0.6744260787963867\n",
      "0.12587559446692465\n",
      "0.7159622609615326\n",
      "0.26796026304364207\n",
      "0.04645086880773306\n",
      "0.6873768150806427\n",
      "0.16564698293805122\n",
      "0.05537792593240738\n",
      "0.5209225058555603\n",
      "0.07266951277852059\n",
      "0.7899823665618897\n",
      "0.07442988157272339\n",
      "0.8907375574111939\n",
      "0.3614255875349045\n",
      "0.5143779695034028\n",
      "0.5198260933160782\n",
      "0.08709712624549866\n",
      "0.7790850162506103\n",
      "0.04848393853753806\n",
      "0.4948931962251663\n",
      "0.12991649545729161\n",
      "0.5226746797561646\n",
      "0.11144632752984762\n",
      "0.11533686965703964\n",
      "0.22458265423774718\n",
      "0.07578394748270512\n",
      "0.24775723218917844\n",
      "0.3215015590190887\n",
      "0.23117848113179207\n",
      "0.5242587119340897\n",
      "0.2194188579916954\n",
      "0.30670291781425474\n",
      "0.14785592854022978\n",
      "0.16970993727445605\n",
      "0.7758516192436218\n",
      "0.4229318410158157\n",
      "0.10538024418056012\n",
      "0.0606456022709608\n",
      "0.12241012752056121\n",
      "0.3772853091359139\n",
      "0.15519112423062326\n",
      "0.5996911108493804\n",
      "0.09204949587583541\n",
      "0.3739614367485047\n",
      "0.03901573587208986\n",
      "0.5858844161033631\n",
      "0.04598416462540627\n",
      "0.1653174336999655\n",
      "0.34000007808208466\n",
      "0.5851688355207444\n",
      "0.8846886277198791\n",
      "0.37729477882385254\n",
      "0.7172907650470733\n",
      "0.16388942152261732\n",
      "0.44561362862586973\n",
      "0.4980732470750809\n",
      "0.05050531327724457\n",
      "0.20270388424396515\n",
      "0.042586929909884934\n",
      "0.3544860124588013\n",
      "0.7976195931434631\n",
      "0.04026066139340401\n",
      "0.18505306243896485\n",
      "0.09966491814702749\n",
      "0.26477767378091815\n",
      "0.109157245606184\n",
      "0.9070411086082459\n",
      "0.04249628596007824\n",
      "0.2350505165755749\n",
      "0.5509905815124512\n",
      "0.3153938606381417\n",
      "0.3109876334667206\n",
      "0.7613610029220581\n",
      "0.435295432806015\n",
      "0.03669142760336399\n",
      "0.19830420166254042\n",
      "0.2697747446596622\n",
      "0.4287167221307755\n",
      "0.35384802073240285\n",
      "0.11549984067678451\n",
      "0.3317315876483918\n",
      "0.14480494409799577\n",
      "0.44157059937715537\n",
      "0.29622395932674406\n",
      "0.06427853722125292\n",
      "0.09295485615730287\n",
      "0.17891994789242746\n",
      "0.14526919648051262\n",
      "0.5534812688827515\n",
      "0.39538757503032684\n",
      "0.23065027147531508\n",
      "0.5050356566905975\n",
      "0.14805067032575608\n",
      "0.7826146245002747\n",
      "0.06153486147522926\n",
      "0.5075785040855407\n",
      "0.37283809483051294\n",
      "0.367042002081871\n",
      "0.10142115317285061\n",
      "0.3850099638104439\n",
      "0.15145815163850782\n",
      "0.5344569742679596\n",
      "0.1923841029405594\n",
      "0.1454150579869747\n",
      "0.26886303909122944\n",
      "0.11978467255830764\n",
      "0.5068825721740723\n",
      "0.5333956420421601\n",
      "0.4510463923215866\n",
      "0.8309258580207826\n",
      "0.06353936158120632\n",
      "0.4141769692301751\n",
      "0.18568347990512848\n",
      "0.24045591726899146\n",
      "0.3541863530874252\n",
      "0.20783880203962327\n",
      "0.2286947190761566\n",
      "0.022084813099354502\n",
      "0.2001803621649742\n",
      "0.48968694508075705\n",
      "0.8543773412704467\n",
      "0.5769248679280281\n",
      "0.3806572794914246\n",
      "0.03406975977122784\n",
      "0.19910602867603303\n",
      "0.41659659147262573\n",
      "0.31763574481010437\n",
      "0.06147815063595771\n",
      "0.3906337350606918\n",
      "0.06764775142073631\n",
      "0.270050585269928\n",
      "0.5178708493709564\n",
      "0.1799938578158617\n",
      "0.39411851167678835\n",
      "0.38663215488195424\n",
      "0.4200214445590973\n",
      "0.6713922381401062\n",
      "0.17524051181972028\n",
      "0.5274278551340102\n",
      "0.7610739469528198\n",
      "0.9185250043869019\n",
      "0.05214690305292606\n",
      "0.05425577759742737\n",
      "0.21418337523937225\n",
      "0.10878815650939942\n",
      "0.7866977095603943\n",
      "0.2218616914004087\n",
      "0.3368453323841095\n",
      "0.4070750296115875\n",
      "0.05733006075024604\n",
      "0.03956725038588047\n",
      "0.7543481349945068\n",
      "0.12008634731173516\n",
      "0.28651323467493056\n",
      "0.11652226932346822\n",
      "0.09899456016719341\n",
      "0.6332304537296296\n",
      "0.2761945813894272\n",
      "0.14685997664928435\n",
      "0.2720779225230217\n",
      "0.5781198799610139\n",
      "0.5261992305517196\n",
      "0.26399168372154236\n",
      "0.16479914635419846\n",
      "0.027155796624720095\n",
      "0.5321416556835175\n",
      "0.4786724239587784\n",
      "0.410217159986496\n",
      "0.3613686159253121\n",
      "0.19631076008081433\n",
      "0.10544973462820054\n",
      "0.3693875104188919\n",
      "0.12769688442349433\n",
      "0.21562667340040206\n",
      "0.09696589708328246\n",
      "0.47970788776874546\n",
      "0.14401626884937285\n",
      "0.1587034583091736\n",
      "0.2659820377826691\n",
      "0.5103201746940613\n",
      "0.29531801193952556\n",
      "0.03720650263130665\n",
      "0.5015768557786942\n",
      "0.05519511513411998\n",
      "0.5726022705435753\n",
      "0.9413639903068541\n",
      "0.27129604220390324\n",
      "0.03131309486925602\n",
      "0.41171055436134335\n",
      "0.4643227934837342\n",
      "0.7891318082809449\n",
      "0.4656182244420052\n",
      "0.11843165010213852\n",
      "0.06457382384687661\n",
      "0.8845803976058961\n",
      "0.8933754563331604\n",
      "0.15163392722606658\n",
      "0.7923769235610962\n",
      "0.16977629475295541\n",
      "0.5669524639844894\n",
      "0.12064286172389985\n",
      "0.3518066823482513\n",
      "0.5191007502377033\n",
      "0.6105545967817306\n",
      "0.6097085684537888\n",
      "0.17386624757200478\n",
      "0.599897825717926\n",
      "0.30071946531534194\n",
      "0.04727853536605835\n",
      "0.2632910400629044\n",
      "0.6958931684494017\n",
      "0.387211349606514\n",
      "0.2508359644562006\n",
      "0.021558889746665956\n",
      "0.22557323426008224\n",
      "0.9142509341239928\n",
      "0.052490347810089585\n",
      "0.8484053134918212\n",
      "0.16683205291628836\n",
      "0.17278998270630835\n",
      "0.8946734070777893\n",
      "0.09070917274802924\n",
      "0.6531391143798828\n",
      "0.1592487543821335\n",
      "0.026215268485248085\n",
      "0.03703589346259833\n",
      "0.024655661545693878\n",
      "0.5654655665159225\n",
      "0.1398212917149067\n",
      "0.2877478331327438\n",
      "0.10426682531833649\n",
      "0.40283089578151704\n",
      "0.34023040235042573\n",
      "0.09829880259931087\n",
      "0.34709579944610597\n",
      "0.9074943780899047\n",
      "0.6604864180088043\n",
      "0.3851584315299988\n",
      "0.8705569982528686\n",
      "0.5866650998592378\n",
      "0.8532815337181092\n",
      "0.15218881368637086\n",
      "0.4188295304775238\n",
      "0.09685884080827235\n",
      "0.23988216817378996\n",
      "0.32920418679714203\n",
      "0.06210295148193837\n",
      "0.6565517663955689\n",
      "0.8269997835159301\n",
      "0.6447796553373337\n",
      "0.5924152940511704\n",
      "0.3839845553040505\n",
      "0.0652757465839386\n",
      "0.34165214896202084\n",
      "0.16834154725074768\n",
      "0.4974807739257812\n",
      "0.20441132336854936\n",
      "0.5825650960206986\n",
      "0.02335486691445112\n",
      "0.12260048426687718\n",
      "0.739505660533905\n",
      "0.30989519953727723\n",
      "0.6875653386116027\n",
      "0.23570900559425353\n",
      "0.26849998235702516\n",
      "0.07798547185957432\n",
      "0.48762037158012383\n",
      "0.23762181103229524\n",
      "0.04488949775695801\n",
      "0.32642599791288374\n",
      "0.05000862739980221\n",
      "0.1128416746854782\n",
      "0.8317895531654357\n",
      "0.03778953868895769\n",
      "0.5803279742598534\n",
      "0.23258285075426105\n",
      "0.8846041798591613\n",
      "0.4025611937046051\n",
      "0.6005580514669419\n",
      "0.9487883687019347\n",
      "0.30146200284361835\n",
      "0.6168889939785003\n",
      "0.2302975796163082\n",
      "0.3519659206271172\n",
      "0.29521516859531405\n",
      "0.8601487159729004\n",
      "0.5690193712711334\n",
      "0.7832442760467528\n",
      "0.028429863415658476\n",
      "0.3058378905057907\n",
      "0.6242611289024353\n",
      "0.715878462791443\n",
      "0.12952966317534445\n",
      "0.04964144751429558\n",
      "0.8924071311950683\n",
      "0.8593032121658324\n",
      "0.07272079214453697\n",
      "0.15174127817153932\n",
      "0.15395129024982454\n",
      "0.08253248166292906\n",
      "0.67008495926857\n",
      "0.5504423350095748\n",
      "0.09797540232539177\n",
      "0.09797124899923801\n",
      "0.6108934983611107\n",
      "0.8972322702407837\n",
      "0.07295017763972282\n",
      "0.030563915893435475\n",
      "0.248653282225132\n",
      "0.8940133213996887\n",
      "0.19430087134242058\n",
      "0.6904390335083008\n",
      "0.03200932275503873\n",
      "0.8477760314941406\n",
      "0.4986444234848022\n",
      "0.1658018920570612\n",
      "0.8826088428497314\n",
      "0.39384881854057313\n",
      "0.7315288186073303\n",
      "0.9084909200668334\n",
      "0.09470976814627648\n",
      "0.04916039351373911\n",
      "0.21576096788048746\n",
      "0.8386115193367005\n",
      "0.36389707326889037\n",
      "0.3221868842840195\n",
      "0.40766332745552064\n",
      "0.40960230529308317\n",
      "0.343137127161026\n",
      "0.09135808404535055\n",
      "0.8615589499473572\n",
      "0.6654986441135405\n",
      "0.665507936477661\n",
      "0.44319067299365994\n",
      "0.6592667579650878\n",
      "0.3979812070727349\n",
      "0.08167803138494492\n",
      "0.2431525260210037\n",
      "0.05478762462735176\n",
      "0.0433492798358202\n",
      "0.37384412288665775\n",
      "0.922572135925293\n",
      "0.10092525184154509\n",
      "0.026192003488540647\n",
      "0.19738668948411942\n",
      "0.4986245691776276\n",
      "0.12168812602758408\n",
      "0.09387018568813801\n",
      "0.20660406053066252\n",
      "0.2708656042814255\n",
      "0.2823890432715416\n",
      "0.31373392790555954\n",
      "0.8355247378349304\n",
      "0.05799539107829333\n",
      "0.12131849080324172\n",
      "0.022866720240563156\n",
      "0.08321237191557884\n",
      "0.3318761885166168\n",
      "0.6340633749961853\n",
      "0.7419253587722778\n",
      "0.43059659004211426\n",
      "0.30684342980384827\n",
      "0.7969378352165223\n",
      "0.756780904531479\n",
      "0.2022752493619919\n",
      "0.12693139016628266\n",
      "0.11363127306103704\n",
      "0.05559975281357765\n",
      "0.1633212499320507\n",
      "0.406691238284111\n",
      "0.5188675343990325\n",
      "0.10752932578325271\n",
      "0.12641171142458915\n",
      "0.28187950104475024\n",
      "0.29419891238212587\n",
      "0.3124265879392624\n",
      "0.022806823439896106\n",
      "0.8285482406616211\n",
      "0.11150888651609421\n",
      "0.045378191769123076\n",
      "0.6149327218532562\n",
      "0.8147681117057801\n",
      "0.04184942077845335\n",
      "0.8826612353324891\n",
      "0.03570803105831146\n",
      "0.6166819930076599\n",
      "0.5611488223075867\n",
      "0.06927609220147134\n",
      "0.07317552343010902\n",
      "0.04277291148900985\n",
      "0.03460597693920135\n",
      "0.8491266965866089\n",
      "0.4501057416200638\n",
      "0.3200938731431961\n",
      "0.1536301225423813\n",
      "0.7290256410837174\n",
      "0.016641049273312093\n",
      "0.33727202042937277\n",
      "0.34206941090524196\n",
      "0.2763164699077606\n",
      "0.311836376786232\n",
      "0.16275978684425355\n",
      "0.8158994317054749\n",
      "0.49514609575271606\n",
      "0.6528934061527252\n",
      "0.06853896807879209\n",
      "0.47460664957761767\n",
      "0.3459194600582123\n",
      "0.7369396924972534\n",
      "0.33122643530368806\n",
      "0.4186496138572693\n",
      "0.4854302704334259\n",
      "0.3101466476917267\n",
      "0.030027392692863943\n",
      "0.06338275223970413\n",
      "0.04085910730063915\n",
      "0.27796549797058107\n",
      "0.3657708376646042\n",
      "0.4104690492153168\n",
      "0.1845989167690277\n",
      "0.2511622056365013\n",
      "0.30053622722625734\n",
      "0.8250804543495178\n",
      "0.10837684795260429\n",
      "0.7767005324363709\n",
      "0.621426236629486\n",
      "0.2653434559702873\n",
      "0.7116449117660523\n",
      "0.8169433116912841\n",
      "0.104910509288311\n",
      "0.5025680780410766\n",
      "0.08180478885769843\n",
      "0.3248492255806923\n",
      "0.1396098792552948\n",
      "0.3561567530035973\n",
      "0.09256256259977816\n",
      "0.32294065654277804\n",
      "0.4977095365524292\n",
      "0.8886363148689271\n",
      "0.8224554419517517\n",
      "0.2402635306119919\n",
      "0.2073860689997673\n",
      "0.04458070956170558\n",
      "0.7257078886032104\n",
      "0.22315784692764282\n",
      "0.37665152251720424\n",
      "0.036586690321564676\n",
      "0.3350666508078575\n",
      "0.4078384831547737\n",
      "0.08313658889383078\n",
      "0.8132916390895844\n",
      "0.5498985469341278\n",
      "0.7767403244972229\n",
      "0.09540289640426636\n",
      "0.037605790048837656\n",
      "0.4232173383235931\n",
      "0.47731091827154165\n",
      "0.791352927684784\n",
      "0.5091283559799195\n",
      "0.052172420732676986\n",
      "0.6385806024074554\n",
      "0.16982114613056185\n",
      "0.45656533241271974\n",
      "0.03605527020990849\n",
      "0.27708441615104673\n",
      "0.7026751220226288\n",
      "0.7651488184928893\n",
      "0.5847326040267945\n",
      "0.4637020915746689\n",
      "0.16948499083518984\n",
      "0.5867486029863358\n",
      "0.3094073891639709\n",
      "0.07066148333251476\n",
      "0.2315498344600201\n",
      "0.46467945277690886\n",
      "0.05578744560480117\n",
      "0.5379833757877349\n",
      "0.19866323471069336\n",
      "0.34703715294599535\n",
      "0.33930106461048126\n",
      "0.7200366675853729\n",
      "0.3033312037587165\n",
      "0.07670963518321514\n",
      "0.04359774366021157\n",
      "0.47550992518663404\n",
      "0.8034264683723449\n",
      "0.7919677138328552\n",
      "0.14805086702108383\n",
      "0.12802320197224618\n",
      "0.058757925778627394\n",
      "0.16602569818496704\n",
      "0.043076501041650776\n",
      "0.10917474590241909\n",
      "0.7753573656082153\n",
      "0.4667695999145508\n",
      "0.5517088413238526\n",
      "0.08243898525834084\n",
      "0.09302549548447131\n",
      "0.05438455380499363\n",
      "0.4150775820016861\n",
      "0.5044234335422516\n",
      "0.6256242513656617\n",
      "0.23628873899579048\n",
      "0.28297418132424357\n",
      "0.024322110787034033\n",
      "0.09866233095526696\n",
      "0.20965765714645385\n",
      "0.18324200361967086\n",
      "0.2322688862681389\n",
      "0.19759253188967707\n",
      "0.2794057607650757\n",
      "0.07797856517136098\n",
      "0.07195713371038437\n",
      "0.5846662223339081\n",
      "0.7624608278274537\n",
      "0.3161175698041916\n",
      "0.4140887454152107\n",
      "0.6517990291118622\n",
      "0.841383981704712\n",
      "0.1299700975418091\n",
      "0.85514897108078\n",
      "0.22321878075599672\n",
      "0.5915074825286866\n",
      "0.7750248372554779\n",
      "0.33813062012195594\n",
      "0.06539830900728702\n",
      "0.8725329160690307\n",
      "0.37218547165393834\n",
      "0.1684910073876381\n",
      "0.9423853278160095\n",
      "0.06177358329296112\n",
      "0.8718555688858033\n",
      "0.02112964550033212\n",
      "0.1637806635349989\n",
      "0.2646103382110596\n",
      "0.03726748563349247\n",
      "0.5094071686267853\n",
      "0.0399213645607233\n",
      "0.11780368089675904\n",
      "0.04556420259177685\n",
      "0.6363783240318298\n",
      "0.062086246162652965\n",
      "0.2643130376935005\n",
      "0.3695246115326881\n",
      "0.8366605043411255\n",
      "0.47471658885478973\n",
      "0.2252655729651451\n",
      "0.8863424658775328\n",
      "0.1400341931730509\n",
      "0.09821690991520882\n",
      "0.6729838192462921\n",
      "0.19550627171993257\n",
      "0.799028480052948\n",
      "0.20634143352508544\n",
      "0.08504506796598435\n",
      "0.038576512970030306\n",
      "0.04376130104064942\n",
      "0.46735821962356566\n",
      "0.16146301329135895\n",
      "0.4146694868803024\n",
      "0.7062143445014952\n",
      "0.2629716508090496\n",
      "0.3744161128997802\n",
      "0.17106179222464563\n",
      "0.918999409675598\n",
      "0.22796225845813753\n",
      "0.3103442907333374\n",
      "0.08831639923155309\n",
      "0.6247495353221892\n",
      "0.29348215609788897\n",
      "0.4484028607606888\n",
      "0.2632327266037464\n",
      "0.17174165658652785\n",
      "0.35602836608886723\n",
      "0.026158915460109712\n",
      "0.047723338752985\n",
      "0.6661786556243896\n",
      "0.22370849475264548\n",
      "0.25465053468942644\n",
      "0.4222384482622146\n",
      "0.06793871074914933\n",
      "0.5794956058263778\n",
      "0.2890533566474915\n",
      "0.0836918368935585\n",
      "0.05526023022830487\n",
      "0.22883855402469633\n",
      "0.05705436896532775\n",
      "0.834125280380249\n",
      "0.7408783435821533\n",
      "0.14006542712450026\n",
      "0.9101520776748658\n",
      "0.39674184620380404\n",
      "0.0696899339556694\n",
      "0.1345797173678875\n",
      "0.5229157149791718\n",
      "0.06970041543245314\n",
      "0.33352643549442296\n",
      "0.6075489342212677\n",
      "0.12819398641586305\n",
      "0.12254857234656812\n",
      "0.058732880279421806\n",
      "0.15495029129087926\n",
      "0.602185708284378\n",
      "0.19530444219708443\n",
      "0.08707913756370544\n",
      "0.6420438230037689\n",
      "0.06920530945062638\n",
      "0.4917035937309264\n",
      "0.1932737279683351\n",
      "0.30861745327711104\n",
      "0.2924920491874218\n",
      "0.0464788280427456\n",
      "0.09000704400241376\n",
      "0.5716343939304351\n",
      "0.06407495737075805\n",
      "0.3528789013624191\n",
      "0.03061525635421276\n",
      "0.0492382800206542\n",
      "0.7183931767940521\n",
      "0.4016759008169174\n",
      "0.13956590667366983\n",
      "0.6452837526798247\n",
      "0.57330122590065\n",
      "0.5234860062599183\n",
      "0.48065082132816317\n",
      "0.8627179265022278\n",
      "0.6320546269416809\n",
      "0.09862311631441116\n",
      "0.4820689678192138\n",
      "0.05728129334747792\n",
      "0.9418521285057067\n",
      "0.3404126647859812\n",
      "0.7939449071884156\n",
      "0.3562419705092907\n",
      "0.7522148370742798\n",
      "0.02049897238612175\n",
      "0.024799928069114685\n",
      "0.33274226337671287\n",
      "0.19505628645420073\n",
      "0.1391100324690342\n",
      "0.05024956800043583\n",
      "0.1414628066122532\n",
      "0.1925996109843254\n",
      "0.03564031906425953\n",
      "0.2807598769664764\n",
      "0.1406304180622101\n",
      "0.04189421609044075\n",
      "0.48595156073570256\n",
      "0.4043611377477646\n",
      "0.44527476429939267\n",
      "0.11456659138202668\n",
      "0.157040149345994\n",
      "0.6019265711307525\n",
      "0.3036852493882179\n",
      "0.21259473860263825\n",
      "0.06842959821224212\n",
      "0.061871004290878766\n",
      "0.7059447288513183\n",
      "0.08086860813200474\n",
      "0.3306027084589005\n",
      "0.41876217424869533\n",
      "0.28130910694599154\n",
      "0.7137898921966553\n",
      "0.2972893953323364\n",
      "0.09116301611065863\n",
      "0.5102433562278748\n",
      "0.6309328496456146\n",
      "0.11137382090091706\n",
      "0.3723311573266983\n",
      "0.48834587931633\n",
      "0.11262012273073196\n",
      "0.1083810020238161\n",
      "0.12420704811811448\n",
      "0.6154038846492769\n",
      "0.03941834717988968\n",
      "0.42009600549936293\n",
      "0.06166111417114734\n",
      "0.04769007600843906\n",
      "0.7085313260555266\n",
      "0.8054858803749085\n",
      "0.9384691119194031\n",
      "0.2904614940285683\n",
      "0.03199171498417854\n",
      "0.5994063675403596\n",
      "0.6390040695667267\n",
      "0.5410510063171386\n",
      "0.6166551113128662\n",
      "0.7614192366600037\n",
      "0.21604944989085198\n",
      "0.8204599738121033\n",
      "0.8363812327384949\n",
      "0.3565285086631775\n",
      "0.794228482246399\n",
      "0.21972784548997878\n",
      "0.8563378334045411\n",
      "0.2441750779747963\n",
      "0.22103299275040628\n",
      "0.045303251221776\n",
      "0.09110068678855897\n",
      "0.07107153572142125\n",
      "0.7360954463481904\n",
      "0.2264642760157585\n",
      "0.04599894974380732\n",
      "0.03599788397550583\n",
      "0.45110052078962326\n",
      "0.13232060596346853\n",
      "0.5439502596855164\n",
      "0.3027006447315216\n",
      "0.4826472043991089\n",
      "0.940182089805603\n",
      "0.17932881861925126\n",
      "0.051947338134050364\n",
      "0.12376014254987239\n",
      "0.5791385293006897\n",
      "0.12296362072229386\n",
      "0.7844241738319396\n",
      "0.11201910786330699\n",
      "0.5327589273452759\n",
      "0.32376537322998045\n",
      "0.19101316928863524\n",
      "0.3057372033596039\n",
      "0.5933317840099335\n",
      "0.1975610062479973\n",
      "0.32712316066026687\n",
      "0.3379170000553131\n",
      "0.06197944581508637\n",
      "0.3123570948839188\n",
      "0.04716789722442626\n",
      "0.6548616766929627\n",
      "0.1233197920024395\n",
      "0.5889188408851623\n",
      "0.2857294648885727\n",
      "0.6452915847301484\n",
      "0.441766744852066\n",
      "0.4824408978223801\n",
      "0.222541856020689\n",
      "0.06515468917787075\n",
      "0.038551433384418486\n",
      "0.520387578010559\n",
      "0.06404389217495918\n",
      "0.04553095009177924\n",
      "0.4126592665910721\n",
      "0.019315671920776368\n",
      "0.259286293387413\n",
      "0.6982822895050049\n",
      "0.09615978300571441\n",
      "0.17821401804685594\n",
      "0.479284730553627\n",
      "0.6973817348480225\n",
      "0.8637389302253723\n",
      "0.3633551925420761\n",
      "0.12424907088279724\n",
      "0.26180188059806825\n",
      "0.35995872765779496\n",
      "0.9716160297393799\n",
      "0.35765536725521085\n",
      "0.09771846737712621\n",
      "0.8971250295639038\n",
      "0.36470887064933777\n",
      "0.29676816463470457\n",
      "0.08468919117003679\n",
      "0.6999776124954225\n",
      "0.02725648656487465\n",
      "0.12317331060767174\n",
      "0.11744410768151284\n",
      "0.17776314914226532\n",
      "0.11435778290033341\n",
      "0.2949808720499277\n",
      "0.29504262953996657\n",
      "0.17688502073287962\n",
      "0.14221283569931986\n",
      "0.15683343037962913\n",
      "0.55598886013031\n",
      "0.44504263848066333\n",
      "0.4691128969192505\n",
      "0.3274964272975922\n",
      "0.33021352142095567\n",
      "0.03931111302226782\n",
      "0.6338510513305664\n",
      "0.1025331899523735\n",
      "0.19746797606348993\n",
      "0.19440455734729764\n",
      "0.5089102506637573\n",
      "0.16953675597906112\n",
      "0.10413254499435425\n",
      "0.42142673432826994\n",
      "0.3639396488666534\n",
      "0.5774441778659821\n",
      "0.06151687279343605\n",
      "0.11453938651829958\n",
      "0.10626184158027172\n",
      "0.279269540309906\n",
      "0.3598558560013771\n",
      "0.5939039587974548\n",
      "0.5829576581716537\n",
      "0.3372194141149521\n",
      "0.5034513354301453\n",
      "0.28159262239933014\n",
      "0.0735490582883358\n",
      "0.2492087408900261\n",
      "0.7390210986137391\n",
      "0.05181896090507508\n",
      "0.24665221869945528\n",
      "0.35719343423843386\n",
      "0.43827562034130096\n",
      "0.2745029985904694\n",
      "0.4607993602752686\n",
      "0.6927414536476135\n",
      "0.2835421159863472\n",
      "0.38612914830446243\n",
      "0.03359122388064861\n",
      "0.46292319893836975\n",
      "0.033223452605307104\n",
      "0.34469514787197114\n",
      "0.85680992603302\n",
      "0.30127269178628924\n",
      "0.04082217253744602\n",
      "0.7347274243831634\n",
      "0.02726643197238445\n",
      "0.8949446678161621\n",
      "0.07983246743679046\n",
      "0.06854407414793968\n",
      "0.6683828175067902\n",
      "0.3219368666410446\n",
      "0.10922371968626975\n",
      "0.7397969245910645\n",
      "0.8976270198822021\n",
      "0.01687392145395279\n",
      "0.7652522206306458\n",
      "0.8210373520851135\n",
      "0.027056283690035342\n",
      "0.030218814685940742\n",
      "0.07932765632867814\n",
      "0.1458721026778221\n",
      "0.2604886502027512\n",
      "0.04912025220692157\n",
      "0.20301865339279176\n",
      "0.387785392254591\n",
      "0.7338367283344269\n",
      "0.1376732550561428\n",
      "0.5986349165439606\n",
      "0.08810550570487977\n",
      "0.4158197343349457\n",
      "0.0884074978530407\n",
      "0.051406983286142356\n",
      "0.5284859776496887\n",
      "0.06346994787454605\n",
      "0.17601097151637077\n",
      "0.579339736700058\n",
      "0.5779116481542587\n",
      "0.22439152076840402\n",
      "0.21010878682136536\n",
      "0.3048705220222473\n",
      "0.051518787443637845\n",
      "0.19835319966077802\n",
      "0.4766319751739502\n",
      "0.034702783450484276\n",
      "0.306075793504715\n",
      "0.2674341775476932\n",
      "0.42844273447990416\n",
      "0.04103257693350315\n",
      "0.017156653106212616\n",
      "0.06983564551919698\n",
      "0.04594637714326382\n",
      "0.03169139325618744\n",
      "0.07158944495022297\n",
      "0.08328633978962897\n",
      "0.5447517037391661\n",
      "0.06404531635344028\n",
      "0.05364564415067435\n",
      "0.5190634906291962\n",
      "0.6549184083938598\n",
      "0.10575401857495309\n",
      "0.2687538743019104\n",
      "0.06460556611418725\n",
      "0.32342745661735534\n",
      "0.605594864487648\n",
      "0.07393121495842933\n",
      "0.0822672076523304\n",
      "0.2201088644564152\n",
      "0.4849199056625367\n",
      "0.404652202129364\n",
      "0.537208366394043\n",
      "0.7146114349365235\n",
      "0.24466382190585137\n",
      "0.6148021638393402\n",
      "0.16690336912870407\n",
      "0.4048005312681198\n",
      "0.8687194705009461\n",
      "0.39347372353076937\n",
      "0.0935188703238964\n",
      "0.3294812381267547\n",
      "0.3392282724380493\n",
      "0.08537953216582536\n",
      "0.18707249388098715\n",
      "0.6986243784427643\n",
      "0.36145299077033993\n",
      "0.42832399606704713\n",
      "0.11165020242333414\n",
      "0.09202674850821495\n",
      "0.16373676508665086\n",
      "0.2752710491418839\n",
      "0.92601420879364\n",
      "0.15454283505678176\n",
      "0.14777849316596986\n",
      "0.2939832732081413\n",
      "0.3769891351461411\n",
      "0.029161974787712094\n",
      "0.648643147945404\n",
      "0.561304759979248\n",
      "0.4810917854309082\n",
      "0.4135415613651276\n",
      "0.8923469185829163\n",
      "0.7804591417312622\n",
      "0.11570989117026328\n",
      "0.8542070269584656\n",
      "0.4851774096488952\n",
      "0.23641710579395292\n",
      "0.8819487690925598\n",
      "0.02065927926450968\n",
      "0.23696162253618241\n",
      "0.015705825574696065\n",
      "0.5637067258358002\n",
      "0.22799329161643983\n",
      "0.8265404105186462\n",
      "0.20360456854104994\n",
      "0.3224225752055645\n",
      "0.11331821754574775\n",
      "0.9141750335693359\n",
      "0.12093071490526199\n",
      "0.028340199217200282\n",
      "0.5735469341278076\n",
      "0.3489260137081146\n",
      "0.5367739990353584\n",
      "0.04887306373566389\n",
      "0.6230502009391785\n",
      "0.38118776232004165\n",
      "0.06421183031052351\n",
      "0.12351408749818801\n",
      "0.24627412259578707\n",
      "0.09708114843815566\n",
      "0.6669964075088501\n",
      "0.06397012099623679\n",
      "0.08862585052847863\n",
      "0.4238644391298294\n",
      "0.03628949578851461\n",
      "0.27120816707611084\n",
      "0.9042972683906556\n",
      "0.2745825782418251\n",
      "0.2669640779495239\n",
      "0.20212860703468322\n",
      "0.21618606746196747\n",
      "0.49627030193805693\n",
      "0.7399405330419541\n",
      "0.16887671202421187\n",
      "0.11639988124370575\n",
      "0.12012944072484971\n",
      "0.05779926590621471\n",
      "0.42540996372699735\n",
      "0.4398299098014831\n",
      "0.10486463457345964\n",
      "0.33321078717708585\n",
      "0.6435617744922638\n",
      "0.5857404172420502\n",
      "0.03615179024636746\n",
      "0.3101881638169289\n",
      "0.35904999524354936\n",
      "0.8839385509490967\n",
      "0.4525682687759399\n",
      "0.21223335359245538\n",
      "0.26649626567959783\n",
      "0.5108581364154816\n",
      "0.4512507304549217\n",
      "0.28794075548648834\n",
      "0.060114852525293826\n",
      "0.6817005813121796\n",
      "0.4862054824829102\n",
      "0.5175553441047669\n",
      "0.04183213338255882\n",
      "0.4596573352813721\n",
      "0.8547544717788695\n",
      "0.12319527566432953\n",
      "0.076384437084198\n",
      "0.9370270848274231\n",
      "0.505726158618927\n",
      "0.09135229177772998\n",
      "0.09368065819144249\n",
      "0.8259971499443054\n",
      "0.19576016142964364\n",
      "0.17076862454414365\n",
      "0.4568967431783676\n",
      "0.10322007834911347\n",
      "0.25009288713335986\n",
      "0.7710543155670166\n",
      "0.26221295595169064\n",
      "0.03755548223853111\n",
      "0.5826619803905487\n",
      "0.1640896052122116\n",
      "0.1198488213121891\n",
      "0.21395384520292282\n",
      "0.5620522618293763\n",
      "0.3010164752602577\n",
      "0.022218609042465684\n",
      "0.659305238723755\n",
      "0.33321060985326767\n",
      "0.07733375020325185\n",
      "0.3209054097533226\n",
      "0.14267363697290422\n",
      "0.7750739693641662\n",
      "0.30688161253929136\n",
      "0.047790722176432605\n",
      "0.2580113068223\n",
      "0.6003169894218445\n",
      "0.13664998039603235\n",
      "0.8058165907859802\n",
      "0.582698631286621\n",
      "0.20414914041757584\n",
      "0.300267818570137\n",
      "0.3190586924552917\n",
      "0.6252223253250122\n",
      "0.10251438431441784\n",
      "0.5606131434440613\n",
      "0.6287698328495026\n",
      "0.41516422033309935\n",
      "0.07345586195588112\n",
      "0.7733972787857056\n",
      "0.5560053884983063\n",
      "0.096432301402092\n",
      "0.5143917083740234\n",
      "0.529559999704361\n",
      "0.13353432193398476\n",
      "0.6464963853359222\n",
      "0.7247487664222717\n",
      "0.8082163929939271\n",
      "0.30484642013907437\n",
      "0.07483672238886356\n",
      "0.15210718587040903\n",
      "0.30358590036630634\n",
      "0.06305252909660339\n",
      "0.16447815895080564\n",
      "0.796085524559021\n",
      "0.38739302158355715\n",
      "0.035965198650956154\n",
      "0.17404006123542784\n",
      "0.3296116068959236\n",
      "0.028046748414635655\n",
      "0.12536282315850256\n",
      "0.14703530035912993\n",
      "0.4276173889636993\n",
      "0.03506456073373556\n",
      "0.06945008374750615\n",
      "0.2718603670597077\n",
      "0.5433424040675163\n",
      "0.6433208584785461\n",
      "0.12469246685504913\n",
      "0.7104404866695403\n",
      "0.35838902294635777\n",
      "0.01298324316740036\n",
      "0.12922153286635876\n",
      "0.6868339061737061\n",
      "0.23810099810361862\n",
      "0.17334809489548209\n",
      "0.03777678348124028\n",
      "0.773223078250885\n",
      "0.1847574099898338\n",
      "0.02894108779728412\n",
      "0.092933451756835\n",
      "0.39242258369922645\n",
      "0.42568116188049315\n",
      "0.2384340360760689\n",
      "0.32537014186382296\n",
      "0.2618553563952446\n",
      "0.8070432424545289\n",
      "0.18537402302026748\n",
      "0.16031691543757914\n",
      "0.10781295970082283\n",
      "0.16271997913718225\n",
      "0.9646819233894348\n",
      "0.03298880495131016\n",
      "0.25875467509031297\n",
      "0.620468032360077\n",
      "0.08887762203812599\n",
      "0.6961276352405548\n",
      "0.2732189819216728\n",
      "0.26902536079287526\n",
      "0.11444196477532387\n",
      "0.2827566251158714\n",
      "0.2543444722890854\n",
      "0.6290368854999542\n",
      "0.20497217457741498\n",
      "0.04492380172014236\n",
      "0.34126978814601894\n",
      "0.6727473974227904\n",
      "0.7999155461788178\n",
      "0.037608491070568566\n",
      "0.18454569578170776\n",
      "0.5871488332748414\n",
      "0.6996692657470703\n",
      "0.7657373905181885\n",
      "0.6292025864124299\n",
      "0.1387024536728859\n",
      "0.334262315928936\n",
      "0.473165325820446\n",
      "0.15078501254320145\n",
      "0.4091287553310394\n",
      "0.2207589497789741\n",
      "0.17109544575214386\n",
      "0.147403696924448\n",
      "0.22759681940078735\n",
      "0.7568820953369141\n",
      "0.42826414108276367\n",
      "0.6279902338981628\n",
      "0.3830792486667633\n",
      "0.09779375046491623\n",
      "0.6832194209098815\n",
      "0.9457154154777526\n",
      "0.06175750531256199\n",
      "0.08687572777271271\n",
      "0.2807130694389343\n",
      "0.2277911089360714\n",
      "0.14614204689860344\n",
      "0.09772874563932418\n",
      "0.17430527061223983\n",
      "0.1687545195221901\n",
      "0.8636656761169432\n",
      "0.5797755360603333\n",
      "0.18382477760314941\n",
      "0.39010496288537977\n",
      "0.30966119319200514\n",
      "0.09318196102976799\n",
      "0.2917296186089516\n",
      "0.21682499423623086\n",
      "0.1176720567047596\n",
      "0.045228318125009534\n",
      "0.20307163298130035\n",
      "0.22008356451988217\n",
      "0.02620485033839941\n",
      "0.45287272334098816\n",
      "0.2441198840737343\n",
      "0.2766412168741226\n",
      "0.7207891583442688\n",
      "0.21047333553433417\n",
      "0.30517673417925834\n",
      "0.449521067738533\n",
      "0.20529476627707482\n",
      "0.0549096617847681\n",
      "0.468848392367363\n",
      "0.20293843895196917\n",
      "0.19506358802318574\n",
      "0.37381695508956914\n",
      "0.7281829953193664\n",
      "0.05121788904070854\n",
      "0.874214482307434\n",
      "0.3838051587343216\n",
      "0.5765631020069122\n",
      "0.46641732156276705\n",
      "0.6154897809028625\n",
      "0.5070031464099884\n",
      "0.40399412661790846\n",
      "0.17754070349037648\n",
      "0.50568727850914\n",
      "0.5283014893531799\n",
      "0.06420939229428768\n",
      "0.6799724221229553\n",
      "0.5649744927883149\n",
      "0.19083280190825463\n",
      "0.04514130111783743\n",
      "0.33100213706493375\n",
      "0.12343538068234922\n",
      "0.6147213816642761\n",
      "0.7911184191703796\n",
      "0.1540824845433235\n",
      "0.7241073071956634\n",
      "0.13637165240943433\n",
      "0.4754563540220261\n",
      "0.1854745104908943\n",
      "0.5599433422088623\n",
      "0.3092873752117157\n",
      "0.22142840102314948\n",
      "0.08638682439923287\n",
      "0.09835264608263969\n",
      "0.2628180988132954\n",
      "0.5317388832569123\n",
      "0.04251007549464702\n",
      "0.4498034417629242\n",
      "0.5582012474536896\n",
      "0.6346440047025681\n",
      "0.22669750824570656\n",
      "0.381250137090683\n",
      "0.6695358574390411\n",
      "0.029277672059834003\n",
      "0.5209958583116532\n",
      "0.23543701916933057\n",
      "0.361254446208477\n",
      "0.7224119961261749\n",
      "0.8832065701484679\n",
      "0.17498727440834044\n",
      "0.9166002988815306\n",
      "0.3113948181271553\n",
      "0.06465613394975661\n",
      "0.05154682695865631\n",
      "0.5604914546012878\n",
      "0.942426323890686\n",
      "0.13690533265471458\n",
      "0.05786602906882763\n",
      "0.026027161441743378\n",
      "0.7493950843811036\n",
      "0.721983790397644\n",
      "0.10717740803956985\n",
      "0.5036064922809601\n",
      "0.05043608695268631\n",
      "0.15517300367355344\n",
      "0.9720946788787841\n",
      "0.41089215129613876\n",
      "0.511268100142479\n",
      "0.7781151890754701\n",
      "0.49140705466270446\n",
      "0.0735016331076622\n",
      "0.032826585695147514\n",
      "0.02710143066942692\n",
      "0.2880959570407867\n",
      "0.5107125997543335\n",
      "0.4779595032334328\n",
      "0.08343151621520518\n",
      "0.10028492659330368\n",
      "0.17062979452311994\n",
      "0.16962829679250715\n",
      "0.6023047804832459\n",
      "0.8624530792236328\n",
      "0.05962272118777037\n",
      "0.724148190021515\n",
      "0.29785524457693097\n",
      "0.2614081121981144\n",
      "0.040320059284567826\n",
      "0.9517274141311645\n",
      "0.07998798191547395\n",
      "0.1375907704234123\n",
      "0.3928774535655975\n",
      "0.1233885882422328\n",
      "0.34857114106416703\n",
      "0.4423146158456802\n",
      "0.6716829776763915\n",
      "0.46226221323013306\n",
      "0.6201427519321441\n",
      "0.647745668888092\n",
      "0.06186390742659569\n",
      "0.766059398651123\n",
      "0.027980783209204673\n",
      "0.2404206797480583\n",
      "0.08272800743579865\n",
      "0.9175573110580445\n",
      "0.045959116518497475\n",
      "0.2656582534313202\n",
      "0.14049452543258667\n",
      "0.04148811977356673\n",
      "0.08689194694161415\n",
      "0.1394763670861721\n",
      "0.7306585371494294\n",
      "0.03216834664344788\n",
      "0.2316143289208412\n",
      "0.03963543865829706\n",
      "0.2093467548489571\n",
      "0.06272282525897027\n",
      "0.12963353842496872\n",
      "0.8108685493469239\n",
      "0.31551773250102993\n",
      "0.7494554758071899\n",
      "0.027743318863213063\n",
      "0.6930652737617492\n",
      "0.825754451751709\n",
      "0.6846444070339204\n",
      "0.7995521068572998\n",
      "0.34553917348384855\n",
      "0.03266169410198927\n",
      "0.5356878876686096\n",
      "0.4030457943677902\n",
      "0.14902786687016487\n",
      "0.26901127994060514\n",
      "0.2748789519071579\n",
      "0.37437992691993716\n",
      "0.10670326724648475\n",
      "0.0663600094616413\n",
      "0.12324571311473848\n",
      "0.12988955676555633\n",
      "0.3068468406796455\n",
      "0.25666856318712233\n",
      "0.3146873824298382\n",
      "0.26083620488643644\n",
      "0.5429658889770508\n",
      "0.4721813321113586\n",
      "0.04064758121967316\n",
      "0.09047555141150952\n",
      "0.6213046193122863\n",
      "0.6689401507377625\n",
      "0.09133001491427421\n",
      "0.6028774082660675\n",
      "0.021946755424141883\n",
      "0.02269466668367386\n",
      "0.1266434147953987\n",
      "0.3549223348498344\n",
      "0.5096257925033569\n",
      "0.294459930062294\n",
      "0.03446843270212412\n",
      "0.16153412014245988\n",
      "0.479149666428566\n",
      "0.07157475911080838\n",
      "0.7609613001346589\n",
      "0.4622058629989624\n",
      "0.27500308454036715\n",
      "0.33579570055007935\n",
      "0.515357357263565\n",
      "0.4520624876022339\n",
      "0.21206881590187548\n",
      "0.7057514786720276\n",
      "0.01834204960614443\n",
      "0.5696974813938142\n",
      "0.678744637966156\n",
      "0.8210801362991333\n",
      "0.4477396577596665\n",
      "0.03510002885013819\n",
      "0.8829365491867065\n",
      "0.31714649833738806\n",
      "0.17971998266875747\n",
      "0.3983204066753387\n",
      "0.09116789698600769\n",
      "0.28384020924568176\n",
      "0.5594736278057099\n",
      "0.27592333108186723\n",
      "0.9424580335617065\n",
      "0.8592528223991395\n",
      "0.32644554600119585\n",
      "0.23511564433574678\n",
      "0.34602248668670654\n",
      "0.02380513343960047\n",
      "0.4219706952571869\n",
      "0.10418133586645126\n",
      "0.11328953132033348\n",
      "0.28765418380498886\n",
      "0.26811743527650833\n",
      "0.5313325315713883\n",
      "0.04644229896366596\n",
      "0.6020406305789947\n",
      "0.6894285678863525\n",
      "0.7669196844100952\n",
      "0.9259014844894409\n",
      "0.9220351934432984\n",
      "0.17476189732551572\n",
      "0.07945655211806298\n",
      "0.04946445226669312\n",
      "0.12046370729804039\n",
      "0.696720963716507\n",
      "0.04448652062565088\n",
      "0.3274427365511656\n",
      "0.03383322488516569\n",
      "0.11644793078303337\n",
      "0.515247306227684\n",
      "0.5846986413002014\n",
      "0.3115972489118576\n",
      "0.5095665514469147\n",
      "0.07479707375168801\n",
      "0.4715246334671974\n",
      "0.8889300346374511\n",
      "0.48730309009552\n",
      "0.4879860520362854\n",
      "0.6668997764587402\n",
      "0.20293418690562248\n",
      "0.05053056888282299\n",
      "0.051490541175007815\n",
      "0.02877016942948103\n",
      "0.6878941476345062\n",
      "0.29193902164697644\n",
      "0.19382478445768356\n",
      "0.0775267206132412\n",
      "0.44371709525585173\n",
      "0.10175700932741165\n",
      "0.3799020126461983\n",
      "0.8003741502761842\n",
      "0.5300180733203887\n",
      "0.5331191539764404\n",
      "0.4369423359632492\n",
      "0.05703630708158017\n",
      "0.6833778142929078\n",
      "0.8706146955490113\n",
      "0.6212738931179047\n",
      "0.7104103803634643\n",
      "0.07383432500064373\n",
      "0.14661028906702994\n",
      "0.18344522416591644\n",
      "0.5254787340760231\n",
      "0.34458118230104445\n",
      "0.25489935278892517\n",
      "0.5793482631444931\n",
      "0.15856222137808798\n",
      "0.5290620714426041\n",
      "0.7089201986789704\n",
      "0.05739577189087868\n",
      "0.652835863828659\n",
      "0.6650729298591613\n",
      "0.22531528472900392\n",
      "0.7187602877616881\n",
      "0.01993082333356142\n",
      "0.5837887644767761\n",
      "0.6342704951763153\n",
      "0.10530054718255996\n",
      "0.12371872812509538\n",
      "0.02434939416125417\n",
      "0.04371909946203231\n",
      "0.2838391065597534\n",
      "0.2832957312464714\n",
      "0.730107879638672\n",
      "0.14648571684956552\n",
      "0.10706181153655052\n",
      "0.6325835466384887\n",
      "0.7686094462871551\n",
      "0.09188238456845282\n",
      "0.1954304777085781\n",
      "0.3925809353590011\n",
      "0.9097319483757018\n",
      "0.04927569888532162\n",
      "0.509285283088684\n",
      "0.2804879151284695\n",
      "0.3427013397216797\n",
      "0.036053466983139514\n",
      "0.1368340939283371\n",
      "0.18569195568561556\n",
      "0.34749917238950734\n",
      "0.2655433103442192\n",
      "0.5812954008579254\n",
      "0.43408955037593844\n",
      "0.5761336863040925\n",
      "0.028516676276922226\n",
      "0.4157529532909394\n",
      "0.07611137181520461\n",
      "0.7595322728157043\n",
      "0.9386948943138123\n",
      "0.6377540290355682\n",
      "0.7529178857803345\n",
      "0.5511507809162141\n",
      "0.31275508999824525\n",
      "0.33106598258018494\n",
      "0.28015948086977005\n",
      "0.5800056278705596\n",
      "0.28698877394199374\n",
      "0.42388431727886206\n",
      "0.343205714225769\n",
      "0.29162557423114777\n",
      "0.08510250225663185\n",
      "0.2756490260362625\n",
      "0.7348025679588318\n",
      "0.6038459837436676\n",
      "0.3043952643871307\n",
      "0.9115927696228027\n",
      "0.18707144260406494\n",
      "0.3553535707294941\n",
      "0.20623201951384543\n",
      "0.19105450212955477\n",
      "0.9413243651390075\n",
      "0.6418122589588166\n",
      "0.7625984430313111\n",
      "0.3621207654476166\n",
      "0.25492558628320694\n",
      "0.19930557236075402\n",
      "0.2166952982544899\n",
      "0.027306373044848446\n",
      "0.12376551181077956\n",
      "0.03154204860329628\n",
      "0.4170765787363052\n",
      "0.1167292430996895\n",
      "0.20504263192415237\n",
      "0.07253095507621765\n",
      "0.03364795297384262\n",
      "0.31699194088578225\n",
      "0.556039571762085\n",
      "0.030929260142147544\n",
      "0.7083753824234009\n",
      "0.05727772898972035\n",
      "0.05166628025472164\n",
      "0.19665001481771469\n",
      "0.14774092733860017\n",
      "0.1081166423857212\n",
      "0.22378407716751098\n",
      "0.8238661050796509\n",
      "0.05299511402845383\n",
      "0.7582360863685608\n",
      "0.526648685336113\n",
      "0.3896728605031967\n",
      "0.06702198125422\n",
      "0.27897056490182875\n",
      "0.11416833251714706\n",
      "0.7605544269084931\n",
      "0.5867405176162719\n",
      "0.3232590541243553\n",
      "0.8459564328193665\n",
      "0.4315007597208023\n",
      "0.14410713016986848\n",
      "0.4392042428255082\n",
      "0.3384284026920795\n",
      "0.26274334937334054\n",
      "0.41636202931404115\n",
      "0.0795555666089058\n",
      "0.4043159902095795\n",
      "0.742293381690979\n",
      "0.5122767627239228\n",
      "0.9155601859092714\n",
      "0.08147142454981804\n",
      "0.5668566286563873\n",
      "0.07824388295412063\n",
      "0.44303620457649234\n",
      "0.12167857438325881\n",
      "0.6839469075202942\n",
      "0.9107568383216857\n",
      "0.10206806771457196\n",
      "0.01990601159632206\n",
      "0.2182570680975914\n",
      "0.09098798781633377\n",
      "0.029640416614711285\n",
      "0.4946075171232224\n",
      "0.027240249142050744\n",
      "0.04411552399396896\n",
      "0.33305944800376897\n",
      "0.681384140253067\n",
      "0.5269845008850098\n",
      "0.3013996288180351\n",
      "0.7637513518333435\n",
      "0.06096721831709147\n",
      "0.305092765390873\n",
      "0.6406376183032989\n",
      "0.7899526119232178\n",
      "0.7284245848655702\n",
      "0.09125192165374754\n",
      "0.5472250699996948\n",
      "0.04945456553250552\n",
      "0.2903489485383034\n",
      "0.44870952963829036\n",
      "0.0870627485215664\n",
      "0.026968455687165264\n",
      "0.5160232841968536\n",
      "0.056474035046994685\n",
      "0.4953018188476563\n",
      "0.1829275857657194\n",
      "0.025044838897883892\n",
      "0.3531670659780502\n",
      "0.027918101474642756\n",
      "0.029397538304328917\n",
      "0.1430766552686691\n",
      "0.4762309789657593\n",
      "0.08008614853024482\n",
      "0.07761831954121591\n",
      "0.08848268650472164\n",
      "0.21745375841856004\n",
      "0.22262278199195862\n",
      "0.5538512706756592\n",
      "0.4636810839176178\n",
      "0.20723527520895005\n",
      "0.09122100099921227\n",
      "0.14177365899086\n",
      "0.06531246453523636\n",
      "0.08540081903338433\n",
      "0.6621996164321899\n",
      "0.5384764313697814\n",
      "0.31215245425701144\n",
      "0.48184093832969666\n",
      "0.4530426114797592\n",
      "0.2457110270857811\n",
      "0.023188374284654856\n",
      "0.15228097662329676\n",
      "0.0790501743555069\n",
      "0.14758094549179077\n",
      "0.33663979470729827\n",
      "0.1249489612877369\n",
      "0.6042547702789307\n",
      "0.6409103274345398\n",
      "0.608500349521637\n",
      "0.09288161545991898\n",
      "0.11600147858262062\n",
      "0.34285476207733157\n",
      "0.7202615380287171\n",
      "0.15170765593647956\n",
      "0.6429920762777328\n",
      "0.19720894247293475\n",
      "0.08415605835616588\n",
      "0.11591821908950806\n",
      "0.6776975929737091\n",
      "0.6617528975009919\n",
      "0.11409904286265374\n",
      "0.33357625603675845\n",
      "0.4928340256214142\n",
      "0.446287290751934\n",
      "0.28013913333415985\n",
      "0.2132375679910183\n",
      "0.15119350403547285\n",
      "0.6306859254837036\n",
      "0.06890308242291213\n",
      "0.07033696062862872\n",
      "0.057288508862257004\n",
      "0.6196915149688721\n",
      "0.25027435570955275\n",
      "0.6486185669898986\n",
      "0.044618326053023336\n",
      "0.2950880780816078\n",
      "0.458816847205162\n",
      "0.30698370933532715\n",
      "0.047241716831922534\n",
      "0.07462804391980171\n",
      "0.04141543582081795\n",
      "0.7658294081687927\n",
      "0.29188350290060044\n",
      "0.21762248799204825\n",
      "0.4826177805662155\n",
      "0.5755993664264679\n",
      "0.5669060811400414\n",
      "0.8791232824325561\n",
      "0.07654481604695319\n",
      "0.6432523846626281\n",
      "0.31971379220485685\n",
      "0.08597264252603054\n",
      "0.5986412763595581\n",
      "0.8373326301574707\n",
      "0.3373650934547186\n",
      "0.19154103994369506\n",
      "0.306222876906395\n",
      "0.486142784357071\n",
      "0.33166065663099287\n",
      "0.05092121958732605\n",
      "0.6541283786296844\n",
      "0.0896723885089159\n",
      "0.11036978028714657\n",
      "0.431365692615509\n",
      "0.05478581711649895\n",
      "0.2627362996339798\n",
      "0.500192666053772\n",
      "0.3770639479160309\n",
      "0.5734644591808319\n",
      "0.3506703361868858\n",
      "0.8398773193359375\n",
      "0.12049402967095377\n",
      "0.02965312423184514\n",
      "0.6050681591033936\n",
      "0.2666004702448845\n",
      "0.07998459935188293\n",
      "0.4591493487358093\n",
      "0.6707309901714325\n",
      "0.07831485271453857\n",
      "0.08443386442959308\n",
      "0.12183710336685183\n",
      "0.16845398396253586\n",
      "0.4852736234664917\n",
      "0.36737130284309394\n",
      "0.7063287615776063\n",
      "0.5617224216461182\n",
      "0.09681494161486626\n",
      "0.12616335079073906\n",
      "0.1348706867545843\n",
      "0.6317935049533844\n",
      "0.7711182355880738\n",
      "0.38441224694252013\n",
      "0.4615143060684204\n",
      "0.3641094520688057\n",
      "0.023139492236077787\n",
      "0.18714949339628217\n",
      "0.40217918455600743\n",
      "0.8266006231307983\n",
      "0.23160762786865233\n",
      "0.28332274705171584\n",
      "0.5972364008426666\n",
      "0.13417195305228233\n",
      "0.1675573132932186\n",
      "0.33085544779896736\n",
      "0.3135012544691563\n",
      "0.27573751509189603\n",
      "0.4784043729305268\n",
      "0.2776820853352546\n",
      "0.1558428667485714\n",
      "0.020974505227059126\n",
      "0.08493348304182291\n",
      "0.34796434044837954\n",
      "0.16305348947644235\n",
      "0.40648532509803775\n",
      "0.17937792018055915\n",
      "0.0781428799033165\n",
      "0.5315487772226333\n",
      "0.10784232169389725\n",
      "0.8005590915679932\n",
      "0.11983359418809414\n",
      "0.4020280003547668\n",
      "0.4760149657726288\n",
      "0.30796388834714894\n",
      "0.46253239512443545\n",
      "0.4902224808931351\n",
      "0.7120839715003967\n",
      "0.7182765066623689\n",
      "0.8163554549217225\n",
      "0.7769668877124787\n",
      "0.19191965460777283\n",
      "0.8145431518554689\n",
      "0.18682383000850677\n",
      "0.8647509694099427\n",
      "0.2046253174543381\n",
      "0.2702506959438324\n",
      "0.2569634020328522\n",
      "0.12070851810276509\n",
      "0.3298309095203876\n",
      "0.3483277589082718\n",
      "0.056435133516788485\n",
      "0.9196572422981262\n",
      "0.219894514977932\n",
      "0.13632861897349358\n",
      "0.5030280172824859\n",
      "0.37750792354345325\n",
      "0.17899967208504677\n",
      "0.09071297347545623\n",
      "0.15986595377326013\n",
      "0.9119561553001405\n",
      "0.8814077258110047\n",
      "0.6332150280475616\n",
      "0.4932708442211151\n",
      "0.5737488448619843\n",
      "0.9393207550048829\n",
      "0.4227371037006378\n",
      "0.4644213855266571\n",
      "0.9302641510963439\n",
      "0.1055575206875801\n",
      "0.8473864674568177\n",
      "0.11815200448036194\n",
      "0.10686750262975693\n",
      "0.3037320762872696\n",
      "0.49976641535758975\n",
      "0.7857451558113099\n",
      "0.9035479545593261\n",
      "0.19841477274894714\n",
      "0.2632496073842049\n",
      "0.07888112328946591\n",
      "0.7151680350303651\n",
      "0.038905325904488565\n",
      "0.08202131316065789\n",
      "0.5221382439136505\n",
      "0.11293118372559548\n",
      "0.26467811614274983\n",
      "0.8353103876113892\n",
      "0.6448195695877075\n",
      "0.698820561170578\n",
      "0.023308958858251568\n",
      "0.17738943919539452\n",
      "0.6832750678062439\n",
      "0.38520388007163997\n",
      "0.9282047033309935\n",
      "0.4687356144189835\n",
      "0.15700697749853135\n",
      "0.26167465150356295\n",
      "0.36891693845391277\n",
      "0.6949869573116303\n",
      "0.9439187288284302\n",
      "0.18478991687297822\n",
      "0.02923108339309692\n",
      "0.1626346692442894\n",
      "0.5857568204402923\n",
      "0.351264625787735\n",
      "0.08856289703398944\n",
      "0.4081212759017945\n",
      "0.026979126408696177\n",
      "0.05386727396398783\n",
      "0.15242271274328234\n",
      "0.6656457602977752\n",
      "0.2924396373331547\n",
      "0.6391738712787628\n",
      "0.14732721894979478\n",
      "0.40298129320144654\n",
      "0.8988942861557007\n",
      "0.04364765603095293\n",
      "0.6103027105331421\n",
      "0.8082977294921875\n",
      "0.06622291468083859\n",
      "0.46898772716522213\n",
      "0.17504931092262266\n",
      "0.6022705256938934\n",
      "0.42038227915763854\n",
      "0.807730221748352\n",
      "0.04809867851436138\n",
      "0.6000708937644957\n",
      "0.434581957757473\n",
      "0.04615840446203946\n",
      "0.32014347538352017\n",
      "0.10828269869089127\n",
      "0.8844588041305542\n",
      "0.34302180558443074\n",
      "0.1226175408810377\n",
      "0.3247244626283646\n",
      "0.5387827575206756\n",
      "0.2726753354072571\n",
      "0.2784501075744629\n",
      "0.7337013244628906\n",
      "0.42449329644441613\n",
      "0.03836379200220108\n",
      "0.5453977435827255\n",
      "0.1847276046872139\n",
      "0.3583170726895333\n",
      "0.04366705156862735\n",
      "0.13566302508115768\n",
      "0.3890704929828644\n",
      "0.11917326748371124\n",
      "0.5003695011138916\n",
      "0.23381525725126265\n",
      "0.9629536867141724\n",
      "0.34249315559864046\n",
      "0.33629608750343326\n",
      "0.7345171034336091\n",
      "0.31542296111583706\n",
      "0.7955915689468385\n",
      "0.2185371220111847\n",
      "0.47736227512359625\n",
      "0.5853367745876312\n",
      "0.103808381408453\n",
      "0.23316701799631118\n",
      "0.30585188642144207\n",
      "0.26604838520288465\n",
      "0.7122848749160766\n",
      "0.41809442341327674\n",
      "0.5051951587200165\n",
      "0.6298617541790008\n",
      "0.16900313049554824\n",
      "0.4527933835983276\n",
      "0.04797583781182766\n",
      "0.6590368568897247\n",
      "0.09011559635400772\n",
      "0.14390664249658583\n",
      "0.35520370304584503\n",
      "0.19498716965317725\n",
      "0.9051673293113709\n",
      "0.15792169943451884\n",
      "0.3288773730397224\n",
      "0.12019724696874619\n",
      "0.42562131881713866\n",
      "0.1812827982008457\n",
      "0.23275788873434067\n",
      "0.6382349193096162\n",
      "0.8840519905090334\n",
      "0.05580242425203324\n",
      "0.7287375032901765\n",
      "0.17438398897647858\n",
      "0.4548092156648636\n",
      "0.13229233473539354\n",
      "0.029812585189938547\n",
      "0.02320042233914137\n",
      "0.438159990310669\n",
      "0.3569734096527099\n",
      "0.046736331284046174\n",
      "0.3984500125050545\n",
      "0.298898708075285\n",
      "0.14211463704705238\n",
      "0.6596941232681275\n",
      "0.7678300023078918\n",
      "0.45930426716804507\n",
      "0.4766521170735359\n",
      "0.12820392660796642\n",
      "0.0837635599076748\n",
      "0.061995596438646314\n",
      "0.27627189233899113\n",
      "0.7199222266674041\n",
      "0.23525943756103518\n",
      "0.10980045534670353\n",
      "0.6374234050512314\n",
      "0.6020669341087341\n",
      "0.09663014337420463\n",
      "0.5853197067975998\n",
      "0.5044386565685273\n",
      "0.5654662013053895\n",
      "0.35148703604936604\n",
      "0.2832976095378399\n",
      "0.9414622664451598\n",
      "0.09032062515616417\n",
      "0.7694958806037903\n",
      "0.7460926234722137\n",
      "0.18986827656626698\n",
      "0.5295604377985\n",
      "0.7284865617752075\n",
      "0.20507476925849916\n",
      "0.8026743650436402\n",
      "0.30013742595911025\n",
      "0.2335234835743904\n",
      "0.14230536408722402\n",
      "0.5040559589862824\n",
      "0.06122966632246971\n",
      "0.5676741957664491\n",
      "0.10938885435461998\n",
      "0.6709353148937226\n",
      "0.07601484134793282\n",
      "0.1132957234978676\n",
      "0.5028524443507194\n",
      "0.7317690432071686\n",
      "0.12666444778442384\n",
      "0.17191208451986312\n",
      "0.5105684876441956\n",
      "0.17671931870281696\n",
      "0.5740008622407913\n",
      "0.2920068621635437\n",
      "0.046838462725281714\n",
      "0.1957654044032097\n",
      "0.7937183201313018\n",
      "0.7695857286453246\n",
      "0.8482622623443603\n",
      "0.11415720023214818\n",
      "0.12162034809589387\n",
      "0.6515588223934173\n",
      "0.38062851428985595\n",
      "0.5635367602109909\n",
      "0.5368165165185929\n",
      "0.026828495785593983\n",
      "0.14177756421267987\n",
      "0.3759282469749451\n",
      "0.6537219941616058\n",
      "0.2800583094358444\n",
      "0.8316732287406922\n",
      "0.04494763724505901\n",
      "0.6800336897373198\n",
      "0.16873847991228103\n",
      "0.7894376873970033\n",
      "0.4656061917543411\n",
      "0.312990415096283\n",
      "0.3616324126720428\n",
      "0.400931790471077\n",
      "0.7553998470306398\n",
      "0.2847562551498413\n",
      "0.30393046587705613\n",
      "0.3152460604906082\n",
      "0.08157875761389732\n",
      "0.04342675283551216\n",
      "0.10291008539497852\n",
      "0.22948645651340485\n",
      "0.7231753110885621\n",
      "0.3014431744813919\n",
      "0.5658856928348541\n",
      "0.09432727023959162\n",
      "0.06734934933483601\n",
      "0.7179529249668122\n",
      "0.46042295694351193\n",
      "0.14938063472509383\n",
      "0.7576875329017639\n",
      "0.4112081855535507\n",
      "0.09987864643335342\n",
      "0.21690306365489959\n",
      "0.0639662530273199\n",
      "0.665761125087738\n",
      "0.2567384004592896\n",
      "0.059033777378499505\n",
      "0.14896321967244147\n",
      "0.5903081297874451\n",
      "0.5485546588897705\n",
      "0.06731347516179086\n",
      "0.22563350498676304\n",
      "0.906421160697937\n",
      "0.9101226687431335\n",
      "0.34617296010255816\n",
      "0.05897757597267628\n",
      "0.062066343426704404\n",
      "0.9155501246452331\n",
      "0.858104419708252\n",
      "0.5034763872623443\n",
      "0.17683512568473814\n",
      "0.7045184731483459\n",
      "0.41138490736484523\n",
      "0.8623004674911499\n",
      "0.3201799809932709\n",
      "0.7232932329177857\n",
      "0.6320987790822983\n",
      "0.5544865429401398\n",
      "0.08972980603575706\n",
      "0.04998925924301147\n",
      "0.3692437797784805\n",
      "0.4695355564355851\n",
      "0.028721617162227632\n",
      "0.36457752138376237\n",
      "0.057682618498802185\n",
      "0.030864192917943\n",
      "0.35480370819568635\n",
      "0.2584053158760071\n",
      "0.7433299779891966\n",
      "0.4353391528129577\n",
      "0.1923496149480343\n",
      "0.2753168322145939\n",
      "0.031710900366306305\n",
      "0.1138727754354477\n",
      "0.6695660710334776\n",
      "0.35172697305679324\n",
      "0.20564400106668473\n",
      "0.12575292810797692\n",
      "0.05996410250663757\n",
      "0.5192134499549865\n",
      "0.31710365116596223\n",
      "0.09644215032458305\n",
      "0.18017217963933943\n",
      "0.0300848837941885\n",
      "0.5571072787046432\n",
      "0.6955852925777435\n",
      "0.03830590657889843\n",
      "0.17522879652678966\n",
      "0.6991075277328491\n",
      "0.5295867323875427\n",
      "0.6404339671134949\n",
      "0.6579022645950318\n",
      "0.3363404393196106\n",
      "0.19054701030254362\n",
      "0.80553537607193\n",
      "0.13345471099019052\n",
      "0.052794314362108706\n",
      "0.6812917649745941\n",
      "0.6088964700698853\n",
      "0.7972670793533325\n",
      "0.921125590801239\n",
      "0.03615780267864466\n",
      "0.333869680762291\n",
      "0.8571023821830749\n",
      "0.3338590621948242\n",
      "0.03570445608347654\n",
      "0.21865347772836682\n",
      "0.7006748199462891\n",
      "0.06885317638516426\n",
      "0.8064688563346862\n",
      "0.09005269482731819\n",
      "0.09433277249336243\n",
      "0.16426031477749345\n",
      "0.5360592931509018\n",
      "0.9340776562690736\n",
      "0.6690860331058502\n",
      "0.02628374863415956\n",
      "0.11407944094389676\n",
      "0.203240866959095\n",
      "0.4870675265789032\n",
      "0.17810091972351078\n",
      "0.4136818528175354\n",
      "0.5566820859909059\n",
      "0.9181530475616455\n",
      "0.04405129402875901\n",
      "0.23610810786485675\n",
      "0.2648539125919342\n",
      "0.5752728283405304\n",
      "0.8826918244361878\n",
      "0.7987830638885497\n",
      "0.2016711637377739\n",
      "0.4763887882232666\n",
      "0.6075341522693634\n",
      "0.4160679996013642\n",
      "0.6562816739082337\n",
      "0.16821337714791296\n",
      "0.2029906675219536\n",
      "0.1789903499186039\n",
      "0.28991080522537227\n",
      "0.27440854683518406\n",
      "0.3834849864244461\n",
      "0.6179582118988036\n",
      "0.45398799479007723\n",
      "0.1241260603070259\n",
      "0.5742635488510132\n",
      "0.4062055498361587\n",
      "0.8632427453994751\n",
      "0.26844199970364574\n",
      "0.4095068424940109\n",
      "0.8681682825088501\n",
      "0.26467620581388474\n",
      "0.5389201447367667\n",
      "0.900968837738037\n",
      "0.23812680393457414\n",
      "0.2066226251423359\n",
      "0.47591321170330053\n",
      "0.548040223121643\n",
      "0.19572003036737443\n",
      "0.29087617695331575\n",
      "0.6554727673530578\n",
      "0.12521491199731827\n",
      "0.5164857566356659\n",
      "0.5034296900033951\n",
      "0.6059162050485611\n",
      "0.05242405012249946\n",
      "0.24881366156041618\n",
      "0.649626475572586\n",
      "0.1216608427464962\n",
      "0.058242814615368836\n",
      "0.5527117192745209\n",
      "0.2749969378113747\n",
      "0.10500918570905925\n",
      "0.37199384421110154\n",
      "0.7579906702041627\n",
      "0.7137979984283447\n",
      "0.36695011854171755\n",
      "0.03610492087900639\n",
      "0.06295924969017505\n",
      "0.6567145705223084\n",
      "0.6654209703207017\n",
      "0.12081058025360109\n",
      "0.12319690585136414\n",
      "0.7594739556312561\n",
      "0.057731734961271285\n",
      "0.05871567837893962\n",
      "0.7628386497497558\n",
      "0.7249337077140807\n",
      "0.04447396025061607\n",
      "0.11027101278305054\n",
      "0.0932704158127308\n",
      "0.5346667289733886\n",
      "0.19527632743120193\n",
      "0.7439649701118469\n",
      "0.33029298186302186\n",
      "0.1506228819489479\n",
      "0.5081702798604965\n",
      "0.4337268680334091\n",
      "0.11405543386936187\n",
      "0.22265840172767637\n",
      "0.022011002339422703\n",
      "0.15880860537290573\n",
      "0.6868756353855133\n",
      "0.6573476135730745\n",
      "0.23106683939695358\n",
      "0.829895806312561\n",
      "0.15217828750610352\n",
      "0.04064548555761576\n",
      "0.17599868476390837\n",
      "0.24559244215488435\n",
      "0.13217494077980518\n",
      "0.1856194943189621\n",
      "0.06877123415470124\n",
      "0.09949453957378865\n",
      "0.4916070520877838\n",
      "0.7147639274597168\n",
      "0.8868298649787903\n",
      "0.24445268958806993\n",
      "0.1289642494171858\n",
      "0.057522781193256385\n",
      "0.06416812017560006\n",
      "0.4979683265089989\n",
      "0.13566066920757294\n",
      "0.12473300173878671\n",
      "0.4138874888420105\n",
      "0.2217058062553406\n",
      "0.3497828930616379\n",
      "0.9572322845458984\n",
      "0.12676754221320152\n",
      "0.5715959012508391\n",
      "0.028998794965446\n",
      "0.311913649737835\n",
      "0.05604050271213054\n",
      "0.15574593842029572\n",
      "0.03642743304371834\n",
      "0.0258259130641818\n",
      "0.20789977312088015\n",
      "0.5273963168263435\n",
      "0.3486747562885285\n",
      "0.7943781614303589\n",
      "0.4958494901657105\n",
      "0.08014430366456508\n",
      "0.226936873793602\n",
      "0.054249681532382965\n",
      "0.5943596601486205\n",
      "0.5183731019496918\n",
      "0.5515961468219758\n",
      "0.8535070419311523\n",
      "0.42757546305656435\n",
      "0.24393052607774734\n",
      "0.48691727221012115\n",
      "0.08873903900384902\n",
      "0.037099753599613906\n",
      "0.3094182424247265\n",
      "0.7640486836433411\n",
      "0.6229194670915603\n",
      "0.6164062708616256\n",
      "0.8689009189605712\n",
      "0.1864149235188961\n",
      "0.4239316962659359\n",
      "0.21883201003074648\n",
      "0.1718329682946205\n",
      "0.5686182320117951\n",
      "0.47518284916877745\n",
      "0.8428268194198608\n",
      "0.5854878425598145\n",
      "0.8808151245117187\n",
      "0.42941958606243136\n",
      "0.08521315921097994\n",
      "0.8927748084068297\n",
      "0.10429470390081406\n",
      "0.7706413447856904\n",
      "0.8055680274963378\n",
      "0.6313043177127838\n",
      "0.7524335384368896\n",
      "0.22475950717926024\n",
      "0.6529970407485962\n",
      "0.5599459916353225\n",
      "0.04389321804046631\n",
      "0.5915064454078675\n",
      "0.5121582329273224\n",
      "0.5266274780035018\n",
      "0.1304621197283268\n",
      "0.5315890699625015\n",
      "0.03792079910635948\n",
      "0.03110270602628589\n",
      "0.557603394985199\n",
      "0.34952316284179685\n",
      "0.04026067499071359\n",
      "0.49926052093505857\n",
      "0.557293039560318\n",
      "0.9415715098381041\n",
      "0.40231069922447205\n",
      "0.3813898630440235\n",
      "0.9098420858383178\n",
      "0.5651532411575317\n",
      "0.2524569883942604\n",
      "0.8335256695747376\n",
      "0.10564682185649872\n",
      "0.49756403565406804\n",
      "0.056170604750514025\n",
      "0.040484162606298925\n",
      "0.06366081442683935\n",
      "0.9012882709503174\n",
      "0.029853399842977524\n",
      "0.04689579736441374\n",
      "0.07398086562752725\n",
      "0.01691861981526017\n",
      "0.1613479681313038\n",
      "0.741713935136795\n",
      "0.02678849697113037\n",
      "0.9225506782531739\n",
      "0.8474800586700441\n",
      "0.5058162212371826\n",
      "0.16123363226652146\n",
      "0.3773994475603104\n",
      "0.6555205464363097\n",
      "0.2658521898090839\n",
      "0.05892629772424698\n",
      "0.2798763334751129\n",
      "0.6133097350597381\n",
      "0.42441487312316895\n",
      "0.10882908683270216\n",
      "0.3667193666100502\n",
      "0.09186180792748928\n",
      "0.2285394433885813\n",
      "0.05827909409999847\n",
      "0.3571922361850738\n",
      "0.8665662050247193\n",
      "0.4202152818441391\n",
      "0.15241439603269102\n",
      "0.4306121408939362\n",
      "0.3412650354206562\n",
      "0.660566759109497\n",
      "0.20278020799160004\n",
      "0.11493086591362954\n",
      "0.06315496107563376\n",
      "0.03412857912480831\n",
      "0.6757315188646317\n",
      "0.06915405616164208\n",
      "0.9207988142967223\n",
      "0.8172536492347717\n",
      "0.17723308838903903\n",
      "0.20072935447096824\n",
      "0.5844719231128692\n",
      "0.39459966421127324\n",
      "0.025863440707325935\n",
      "0.05256757121533155\n",
      "0.20994012504816054\n",
      "0.07880860865116118\n",
      "0.7473948121070861\n",
      "0.9195600509643556\n",
      "0.7435962915420533\n",
      "0.9288382172584534\n",
      "0.1133291982114315\n",
      "0.23445813059806825\n",
      "0.214477401971817\n",
      "0.2977282047271729\n",
      "0.06674474030733109\n",
      "0.7933852791786193\n",
      "0.09386201035231352\n",
      "0.688303142786026\n",
      "0.36909602731466296\n",
      "0.03841177076101303\n",
      "0.42197391390800476\n",
      "0.6861180663108826\n",
      "0.21934986114501953\n",
      "0.3232176288962364\n",
      "0.40481479167938234\n",
      "0.08803751803934574\n",
      "0.09052969515323639\n",
      "0.5348128139972688\n",
      "0.21699445620179178\n",
      "0.3307143747806549\n",
      "0.6264463543891906\n",
      "0.6643219590187073\n",
      "0.10567257404327393\n",
      "0.7099658250808716\n",
      "0.031216990947723386\n",
      "0.36358286440372467\n",
      "0.7191258192062377\n",
      "0.07162230312824248\n",
      "0.29014110714197155\n",
      "0.1532481238245964\n",
      "0.04848024621605873\n",
      "0.20296920835971835\n",
      "0.21760248094797136\n",
      "0.31345500499010087\n",
      "0.49410277009010317\n",
      "0.2749669007956982\n",
      "0.6345847904682159\n",
      "0.4947238638997078\n",
      "0.4925501883029937\n",
      "0.32263158261775976\n",
      "0.37741562724113464\n",
      "0.029228328540921212\n",
      "0.489335697889328\n",
      "0.1648386172950268\n",
      "0.15439507514238357\n",
      "0.40741921663284303\n",
      "0.28116828799247745\n",
      "0.19380572885274885\n",
      "0.06445367634296417\n",
      "0.47230498194694526\n",
      "0.06740773320198058\n",
      "0.3407796621322632\n",
      "0.04526934139430523\n",
      "0.22154165357351302\n",
      "0.3442950814962387\n",
      "0.37072818279266356\n",
      "0.09000681042671205\n",
      "0.771510761976242\n",
      "0.7559985280036927\n",
      "0.6817918241024017\n",
      "0.09761145748198032\n",
      "0.6043749988079071\n",
      "0.07263302467763424\n",
      "0.1573590464890003\n",
      "0.43616894185543065\n",
      "0.5944088160991667\n",
      "0.1388205960392952\n",
      "0.07862291410565377\n",
      "0.06291590128093957\n",
      "0.32382552921772\n",
      "0.8051717877388\n",
      "0.046300490945577616\n",
      "0.1717078574001789\n",
      "0.2782830700278282\n",
      "0.3793872475624085\n",
      "0.5921865493059159\n",
      "0.5652713596820831\n",
      "0.3340811163187027\n",
      "0.1465945914387703\n",
      "0.2539276823401451\n",
      "0.06509135439991952\n",
      "0.17808665335178375\n",
      "0.18287190571427347\n",
      "0.7690676093101502\n",
      "0.19167008101940156\n",
      "0.059186140447855\n",
      "0.7904323816299439\n",
      "0.9296639442443848\n",
      "0.07723495066165925\n",
      "0.16834306493401527\n",
      "0.31231842637062074\n",
      "0.14312663823366165\n",
      "0.03249816745519638\n",
      "0.5374719038605691\n",
      "0.5899680078029633\n",
      "0.5740747034549714\n",
      "0.7526344895362854\n",
      "0.38035700917243953\n",
      "0.7542411684989929\n",
      "0.4587665319442749\n",
      "0.6510364890098572\n",
      "0.04215534143149852\n",
      "0.11510382071137429\n",
      "0.27630298435688017\n",
      "0.38927040100097654\n",
      "0.7213182151317596\n",
      "0.508620184659958\n",
      "0.4856636732816696\n",
      "0.5653630167245864\n",
      "0.13967422395944595\n",
      "0.6051758468151093\n",
      "0.8214796185493469\n",
      "0.9164048790931701\n",
      "0.5393445700407028\n",
      "0.12187189608812332\n",
      "0.8910795569419862\n",
      "0.024141948111355307\n",
      "0.12970311492681505\n",
      "0.7534177333116532\n",
      "0.11021953970193864\n",
      "0.022828364931046962\n",
      "0.5873488485813141\n",
      "0.13591540902853014\n",
      "0.07793103232979773\n",
      "0.9211224079132081\n",
      "0.28344480991363524\n",
      "0.45908900201320646\n",
      "0.0490917406976223\n",
      "0.3834755629301071\n",
      "0.36736157536506653\n",
      "0.19005146771669387\n",
      "0.45851590633392336\n",
      "0.05774384699761868\n",
      "0.03137561101466418\n",
      "0.25099331364035604\n",
      "0.19531268477439878\n",
      "0.07590836435556411\n",
      "0.13027682676911354\n",
      "0.12492183297872543\n",
      "0.46369924247264865\n",
      "0.18642761185765266\n",
      "0.33474187552928925\n",
      "0.6514509856700897\n",
      "0.15874224305152893\n",
      "0.35998075008392333\n",
      "0.8003768086433412\n",
      "0.8714156389236449\n",
      "0.1637472592294216\n",
      "0.20526127517223358\n",
      "0.20513257384300232\n",
      "0.1017308671027422\n",
      "0.6797130465507508\n",
      "0.8453209280967712\n",
      "0.7069262087345123\n",
      "0.29834794849157337\n",
      "0.6640232920646667\n",
      "0.6712378442287444\n",
      "0.20041354149579999\n",
      "0.36120973005890844\n",
      "0.11457853950560093\n",
      "0.48562731146812443\n",
      "0.08641489371657371\n",
      "0.1380759984254837\n",
      "0.42537259459495547\n",
      "0.6945694327354431\n",
      "0.32754253000020983\n",
      "0.14656675904989241\n",
      "0.1623789593577385\n",
      "0.5456387996673584\n",
      "0.08629553206264971\n",
      "0.20957805216312408\n",
      "0.5994007408618927\n",
      "0.4285188734531402\n",
      "0.061613445356488224\n",
      "0.7197563767433166\n",
      "0.23869289308786393\n",
      "0.09231139570474624\n",
      "0.3696956753730774\n",
      "0.07516577895730735\n",
      "0.44445029050111773\n",
      "0.5173289805650712\n",
      "0.21812770813703536\n",
      "0.3202411592006683\n",
      "0.6125244557857513\n",
      "0.29890226423740385\n",
      "0.19017994590103626\n",
      "0.15931392610073092\n",
      "0.3043859496712685\n",
      "0.26989390254020695\n",
      "0.3444301515817642\n",
      "0.14811775088310242\n",
      "0.5309031009674072\n",
      "0.044564501754939555\n",
      "0.17083768695592882\n",
      "0.17062995880842208\n",
      "0.21926069259643555\n",
      "0.6590167164802552\n",
      "0.5092894159257412\n",
      "0.03301287591457367\n",
      "0.7959749698638916\n",
      "0.6002566486597061\n",
      "0.020885992050170898\n",
      "0.3204484775662422\n",
      "0.33093640655279155\n",
      "0.7460941493511201\n",
      "0.08525053709745409\n",
      "0.6826895952224732\n",
      "0.1473267987370491\n",
      "0.04101208299398422\n",
      "0.5956847846508025\n",
      "0.6863584280014038\n",
      "0.7287662625312805\n",
      "0.34977536201477055\n",
      "0.3019318461418152\n",
      "0.1049569621682167\n",
      "0.14859187752008438\n",
      "0.5364558041095734\n",
      "0.04471007771790028\n",
      "0.6656413078308105\n",
      "0.020582312159240244\n",
      "0.3920831359922886\n",
      "0.4223985582590103\n",
      "0.1399580854922533\n",
      "0.021487794630229477\n",
      "0.07579332813620568\n",
      "0.022423522081226107\n",
      "0.06066707959398627\n",
      "0.6460883736610412\n",
      "0.8331501603126525\n",
      "0.7989323437213897\n",
      "0.12481938153505326\n",
      "0.2596076563000679\n",
      "0.10633359402418137\n",
      "0.0738345168530941\n",
      "0.7177489995956421\n",
      "0.5223145455121994\n",
      "0.11101579256355762\n",
      "0.4958319395780564\n",
      "0.7757838249206542\n",
      "0.5655359864234925\n",
      "0.0392884936183691\n",
      "0.12099842708557845\n",
      "0.12029010653495789\n",
      "0.7595785140991211\n",
      "0.042778129503130924\n",
      "0.2974890470504761\n",
      "0.22853597328066827\n",
      "0.2889207571744919\n",
      "0.049941854923963545\n",
      "0.2668807446956635\n",
      "0.1171506367623806\n",
      "0.4453208386898041\n",
      "0.1281396597623825\n",
      "0.27378744184970855\n",
      "0.6129635095596313\n",
      "0.7289008140563964\n",
      "0.0929991913959384\n",
      "0.13282816857099533\n",
      "0.019924398325383664\n",
      "0.40746891498565674\n",
      "0.7093542814254761\n",
      "0.6177833318710327\n",
      "0.05349136330187321\n",
      "0.4398443609476089\n",
      "0.5013369083404541\n",
      "0.10076106488704681\n",
      "0.5899848110973835\n",
      "0.03566412944346667\n",
      "0.2021705165505409\n",
      "0.11397209763526917\n",
      "0.3362161174416542\n",
      "0.6456461906433105\n",
      "0.1356590449810028\n",
      "0.7880765914916992\n",
      "0.09525810033082963\n",
      "0.16782740205526353\n",
      "0.022352678142488005\n",
      "0.8836483597755431\n",
      "0.1648487627506256\n",
      "0.028893804363906383\n",
      "0.22166655138134955\n",
      "0.07128437533974646\n",
      "0.48549119085073467\n",
      "0.20806698575615884\n",
      "0.546220451593399\n",
      "0.8419734001159669\n",
      "0.9465583801269531\n",
      "0.4416513204574585\n",
      "0.6444270819425583\n",
      "0.07827627547085286\n",
      "0.5740164756774903\n",
      "0.7200495839118958\n",
      "0.0843174882233143\n",
      "0.505928811430931\n",
      "0.4327061980962753\n",
      "0.04207723755389452\n",
      "0.2227809563279152\n",
      "0.4256770461797714\n",
      "0.36465855538845066\n",
      "0.12602297589182854\n",
      "0.8387486219406127\n",
      "0.7979108333587646\n",
      "0.844831430912018\n",
      "0.20156252086162565\n",
      "0.21511649191379545\n",
      "0.10266183391213417\n",
      "0.2094126492738724\n",
      "0.08540466316044332\n",
      "0.036746766045689584\n",
      "0.7924192667007446\n",
      "0.661091560125351\n",
      "0.6252640902996063\n",
      "0.11334489285945891\n",
      "0.05983772370964288\n",
      "0.12481324523687362\n",
      "0.6852423191070556\n",
      "0.390868516266346\n",
      "0.01698372680693865\n",
      "0.5161785989999771\n",
      "0.18175365924835207\n",
      "0.12467098943889142\n",
      "0.7603483676910401\n",
      "0.4763625204563141\n",
      "0.10767803527414799\n",
      "0.11026785448193549\n",
      "0.3213046707212925\n",
      "0.3823938429355621\n",
      "0.3857138112187385\n",
      "0.3403246015310287\n",
      "0.11007024832069874\n",
      "0.3776726454496384\n",
      "0.5281302973628044\n",
      "0.5160338699817658\n",
      "0.11305546313524246\n",
      "0.3637694731354713\n",
      "0.4502674520015717\n",
      "0.5333873093128205\n",
      "0.11894680038094521\n",
      "0.556897670030594\n",
      "0.3824944972991943\n",
      "0.8285062670707704\n",
      "0.42386582493782043\n",
      "0.18637265861034394\n",
      "0.045699138194322586\n",
      "0.9146472692489624\n",
      "0.07827658150345088\n",
      "0.8573160409927367\n",
      "0.14590913206338885\n",
      "0.03133243303745985\n",
      "0.36509460359811785\n",
      "0.5124688535928726\n",
      "0.12678160667419433\n",
      "0.4300581231713295\n",
      "0.38436794131994245\n",
      "0.5160256803035737\n",
      "0.2869552493095398\n",
      "0.65898597240448\n",
      "0.42866567075252526\n",
      "0.13971011936664582\n",
      "0.031559163611382245\n",
      "0.04312164261937141\n",
      "0.050871855393052105\n",
      "0.09120871387422086\n",
      "0.685019052028656\n",
      "0.16718651428818704\n",
      "0.34673087000846864\n",
      "0.050582884997129436\n",
      "0.3692476719617843\n",
      "0.0822015218436718\n",
      "0.08448974862694741\n",
      "0.8747558832168579\n",
      "0.31046760976314547\n",
      "0.0826442375779152\n",
      "0.18459353744983675\n",
      "0.43113176673650744\n",
      "0.23453439250588418\n",
      "0.18890957571566108\n",
      "0.24830043017864226\n",
      "0.5802828669548035\n",
      "0.09167095795273782\n",
      "0.12581419572234154\n",
      "0.13690196201205254\n",
      "0.23236223086714747\n",
      "0.1394827537238598\n",
      "0.30384726375341414\n",
      "0.5361113190650939\n",
      "0.5310551762580872\n",
      "0.1452843353152275\n",
      "0.7659574627876281\n",
      "0.04524336941540241\n",
      "0.0516013190150261\n",
      "0.17486235201358794\n",
      "0.4820968091487885\n",
      "0.10565629303455354\n",
      "0.710660469532013\n",
      "0.3022358775138855\n",
      "0.7869325160980225\n",
      "0.352317600697279\n",
      "0.3420834183692932\n",
      "0.14659021645784379\n",
      "0.05150816347450018\n",
      "0.8225992321968079\n",
      "0.09334784522652625\n",
      "0.1938799634575844\n",
      "0.03373713437467813\n",
      "0.04487211853265762\n",
      "0.12337925285100937\n",
      "0.6219004809856414\n",
      "0.4659857779741287\n",
      "0.8126128792762757\n",
      "0.6458358466625214\n",
      "0.2806560277938843\n",
      "0.05942513607442379\n",
      "0.1455678399652243\n",
      "0.036106967367231846\n",
      "0.4619709804654122\n",
      "0.6191548526287078\n",
      "0.6549209117889404\n",
      "0.19352085292339324\n",
      "0.22033508867025375\n",
      "0.2610668808221817\n",
      "0.05225403308868408\n",
      "0.05095585584640503\n",
      "0.7014506697654724\n",
      "0.42012309134006504\n",
      "0.08525699861347674\n",
      "0.18676068484783173\n",
      "0.08656496778130532\n",
      "0.4069497257471084\n",
      "0.0910914834588766\n",
      "0.9334713220596312\n",
      "0.7552324175834655\n",
      "0.2332103535532951\n",
      "0.7234181642532349\n",
      "0.27654325813055036\n",
      "0.514361971616745\n",
      "0.22201786413788793\n",
      "0.8555578827857971\n",
      "0.08751650303602218\n",
      "0.4905657857656479\n",
      "0.12991135194897652\n",
      "0.3946450501680374\n",
      "0.12800728306174278\n",
      "0.38207728266716\n",
      "0.282663930952549\n",
      "0.2685085147619247\n",
      "0.5300827547907829\n",
      "0.05215310193598271\n",
      "0.05015104562044144\n",
      "0.27164000980556013\n",
      "0.7886027693748474\n",
      "0.2073518432676792\n",
      "0.22066579014062881\n",
      "0.22397457733750345\n",
      "0.3048667475581169\n",
      "0.04246004540473223\n",
      "0.06881901398301124\n",
      "0.06101820915937423\n",
      "0.7864142179489135\n",
      "0.06849145516753197\n",
      "0.7310642719268798\n",
      "0.2224257171154022\n",
      "0.15201934278011323\n",
      "0.09343742299824953\n",
      "0.5497942209243775\n",
      "0.2193944573402405\n",
      "0.8940473794937134\n",
      "0.8277933597564697\n",
      "0.18722135238349438\n",
      "0.2814086973667145\n",
      "0.5606905519962311\n",
      "0.2782694548368454\n",
      "0.35383386015892027\n",
      "0.030181175097823146\n",
      "0.6288906693458557\n",
      "0.4078502774238586\n",
      "0.35130063891410823\n",
      "0.4147074729204178\n",
      "0.06960329879075289\n",
      "0.6969442009925844\n",
      "0.0767903182655573\n",
      "0.2718921281397343\n",
      "0.4485877633094788\n",
      "0.027699945122003557\n",
      "0.07274844497442245\n",
      "0.916432797908783\n",
      "0.4246987789869308\n",
      "0.7624964952468872\n",
      "0.09538242537528276\n",
      "0.3162568122148514\n",
      "0.7902187824249267\n",
      "0.8260146498680114\n",
      "0.6396411061286926\n",
      "0.6012701630592345\n",
      "0.6936229944229126\n",
      "0.848312258720398\n",
      "0.2262053146958351\n",
      "0.5335676848888398\n",
      "0.09485103487968445\n",
      "0.4242311894893646\n",
      "0.06252179592847824\n",
      "0.33272272944450376\n",
      "0.02857835255563259\n",
      "0.18521225489675996\n",
      "0.35320324301719663\n",
      "0.03180494345724583\n",
      "0.15424427688121795\n",
      "0.34475115239620213\n",
      "0.6847922921180725\n",
      "0.05353965722024441\n",
      "0.1057432271540165\n",
      "0.5765231251716614\n",
      "0.6634647965431213\n",
      "0.10789114162325858\n",
      "0.1729888040572405\n",
      "0.044423897936940196\n",
      "0.4582531273365021\n",
      "0.20150804966688157\n",
      "0.45884373486042024\n",
      "0.06259088069200516\n",
      "0.5613362073898315\n",
      "0.5378754734992981\n",
      "0.39543160498142244\n",
      "0.40095853507518775\n",
      "0.09657083302736283\n",
      "0.461261759698391\n",
      "0.5571746706962586\n",
      "0.11769592128694058\n",
      "0.07714591324329376\n",
      "0.1812166094779968\n",
      "0.16862173900008198\n",
      "0.6468819200992585\n",
      "0.12808144688606263\n",
      "0.07257731184363365\n",
      "0.05564956665039063\n",
      "0.32477598190307616\n",
      "0.39869981408119204\n",
      "0.20065848976373674\n",
      "0.048996972665190694\n",
      "0.28895399495959284\n",
      "0.593682837486267\n",
      "0.742436408996582\n",
      "0.07205167710781098\n",
      "0.12760675922036172\n",
      "0.09202612657099964\n",
      "0.1589759364724159\n",
      "0.15934661030769345\n",
      "0.44824072718620306\n",
      "0.0745659528300166\n",
      "0.2648795001208782\n",
      "0.15997782424092294\n",
      "0.417649894952774\n",
      "0.3930392697453499\n",
      "0.5793374061584473\n",
      "0.35301697701215745\n",
      "0.1733573719859123\n",
      "0.11716545224189759\n",
      "0.4280686497688294\n",
      "0.06314982883632184\n",
      "0.8276255011558533\n",
      "0.32534117102622984\n",
      "0.8048189759254456\n",
      "0.7712971568107604\n",
      "0.06827556937932969\n",
      "0.06035393252968789\n",
      "0.05026043653488159\n",
      "0.05462881438434124\n",
      "0.5825780391693115\n",
      "0.4659993231296539\n",
      "0.06883385740220546\n",
      "0.20499104168266055\n",
      "0.23315644562244414\n",
      "0.4070409029722214\n",
      "0.3778953790664673\n",
      "0.5506767779588699\n",
      "0.1951881319284439\n",
      "0.07173068225383758\n",
      "0.2655356675386429\n",
      "0.515042407810688\n",
      "0.035600738972425466\n",
      "0.09239680729806422\n",
      "0.3618492022156715\n",
      "0.07445276975631715\n",
      "0.5193816035985946\n",
      "0.15648425370454788\n",
      "0.3438755407929421\n",
      "0.21780156791210173\n",
      "0.2136440388858318\n",
      "0.2432092800736427\n",
      "0.13437285907566546\n",
      "0.1280466753989458\n",
      "0.6523581445217133\n",
      "0.15434884801506996\n",
      "0.05296046249568463\n",
      "0.012982495687901975\n",
      "0.11502417773008346\n",
      "0.47820435166358943\n",
      "0.029726787097752092\n",
      "0.0696213299408555\n",
      "0.07300636935979127\n",
      "0.24130242317914966\n",
      "0.14848202168941496\n",
      "0.09478470534086228\n",
      "0.2760130152106285\n",
      "0.04454102590680123\n",
      "0.08924461975693702\n",
      "0.5072265297174454\n",
      "0.42627646028995514\n",
      "0.20198650360107423\n",
      "0.12357284352183341\n",
      "0.3929249525070191\n",
      "0.41663023233413693\n",
      "0.041367338038980954\n",
      "0.8784391999244688\n",
      "0.5540420919656753\n",
      "0.6644552573561668\n",
      "0.29575578570365907\n",
      "0.17323219329118728\n",
      "0.3519852250814438\n",
      "0.34593517482280733\n",
      "0.07828014977276325\n",
      "0.4090675555169583\n",
      "0.4494833111763001\n",
      "0.7477646768093108\n",
      "0.5903153955936432\n",
      "0.7565039157867431\n",
      "0.4114666730165482\n",
      "0.044407335668802256\n",
      "0.21608087792992592\n",
      "0.6155809968709945\n",
      "0.5106845617294312\n",
      "0.09061786085367203\n",
      "0.6639714181423186\n",
      "0.8962984442710877\n",
      "0.25261665284633633\n",
      "0.6139376640319825\n",
      "0.42420848906040187\n",
      "0.2465575821697712\n",
      "0.2105800077319145\n",
      "0.6282596617937088\n",
      "0.05420984588563442\n",
      "0.8622895717620849\n",
      "0.1001979410648346\n",
      "0.050261569768190385\n",
      "0.08928170278668403\n",
      "0.6551213830709457\n",
      "0.07130046673119068\n",
      "0.47095318436622624\n",
      "0.8643944144248962\n",
      "0.2455354392528534\n",
      "0.13065508976578713\n",
      "0.798648750782013\n",
      "0.40098024532198906\n",
      "0.3193050861358642\n",
      "0.17438632398843765\n",
      "0.6843352198600768\n",
      "0.1609824523329735\n",
      "0.8345711588859559\n",
      "0.3370420657098293\n",
      "0.10321132689714431\n",
      "0.47925104498863214\n",
      "0.28000276237726207\n",
      "0.5211258828639984\n",
      "0.749895578622818\n",
      "0.30366801321506504\n",
      "0.3343800842761993\n",
      "0.7191267430782318\n",
      "0.3297252207994461\n",
      "0.5255145072937012\n",
      "0.9252604007720948\n",
      "0.0456650085747242\n",
      "0.07530488967895509\n",
      "0.5692355215549469\n",
      "0.03694597519934177\n",
      "0.7653357505798339\n",
      "0.4693725675344467\n",
      "0.8106239080429076\n",
      "0.04890674855560065\n",
      "0.40120585560798644\n",
      "0.8488790750503539\n",
      "0.6613174676895142\n",
      "0.17003271207213402\n",
      "0.32023309767246244\n",
      "0.19509954750537872\n",
      "0.1311114251613617\n",
      "0.6188040137290954\n",
      "0.7966394662857056\n",
      "0.06701936908066274\n",
      "0.03375808112323284\n",
      "0.48821286112070084\n",
      "0.032572939060628414\n",
      "0.5287735939025878\n",
      "0.7419982552528381\n",
      "0.4137308090925217\n",
      "0.18921114802360536\n",
      "0.34153777211904524\n",
      "0.38481506258249276\n",
      "0.16003656163811683\n",
      "0.8366171836853028\n",
      "0.03692467212677002\n",
      "0.800794106721878\n",
      "0.10258059725165368\n",
      "0.06931443698704243\n",
      "0.08350433744490147\n",
      "0.29688268303871157\n",
      "0.1070054866373539\n",
      "0.08991815075278281\n",
      "0.9584927797317505\n",
      "0.21531262323260308\n",
      "0.18210852965712548\n",
      "0.06746224723756314\n",
      "0.19177970588207247\n",
      "0.04320679139345884\n",
      "0.3991730988025665\n",
      "0.49625754654407506\n",
      "0.25485743433237074\n",
      "0.1783620998263359\n",
      "0.7225699543952941\n",
      "0.2473197400569916\n",
      "0.027617262676358223\n",
      "0.5157671153545379\n",
      "0.46745037138462064\n",
      "0.5613276302814484\n",
      "0.7821906447410584\n",
      "0.5259222835302353\n",
      "0.08039740230888127\n",
      "0.14986416548490522\n",
      "0.5187165617942809\n",
      "0.26670504696667197\n",
      "0.17933431565761568\n",
      "0.6512619256973268\n",
      "0.06495602242648602\n",
      "0.6308802425861358\n",
      "0.05749522484838962\n",
      "0.29788573980331423\n",
      "0.4217308133840561\n",
      "0.8526749849319458\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(preds_for_sub <= 0.5))\n",
    "print(len(preds_for_sub), '\\n')\n",
    "for line in preds_for_sub:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
